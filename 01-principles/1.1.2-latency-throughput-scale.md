# 1.1.2 Latency, Throughput, and Scaling

## Intuitive Explanation

Imagine a highway:

1. **Latency (The Delay):** This is the time it takes for one car (a data packet/request) to travel from point A (user)
   to point B (server) and back. Low latency means a fast response time.
2. **Throughput (The Volume):** This is the total number of cars (requests) that the highway can process per unit of
   time (e.g., cars per minute). High throughput means the system can handle many concurrent users.
3. **Scaling (Adding Capacity):** This is how you increase the highway's capacity to handle more cars.

---

## In-Depth Analysis

### Latency vs. Throughput

- **Latency (Time):** Measured in milliseconds (ms). It is the delay between a request and a response. Key factors
  include network speed, I/O operations (disk reads), and computation time.
- **Throughput (Rate):** Measured in Queries Per Second (QPS) or transactions per second. It indicates the capacity of
  the system.
- **Optimization Trade-off:** Increasing throughput (e.g., processing requests in parallel) can sometimes increase
  average latency due to queueing and resource contention.

### Scaling Strategies

#### 1. Vertical Scaling (Scaling Up)

- **Definition:** Adding more resources (CPU, RAM, faster SSDs) to an existing single machine.
- **Analogy:** Upgrading your single computer to a faster one.
- **Use Case:** Often used for **Relational Databases (RDBMS)** due to their transactional consistency requirements.
- **Limitation:** Hardware eventually hits a ceiling (you can't buy infinite RAM), and it creates a **Single Point of
  Failure (SPOF).**

#### 2. Horizontal Scaling (Scaling Out)

- **Definition:** Adding more low-cost machines (nodes) to a pool and distributing the load across them.
- **Analogy:** Buying 10 regular computers instead of one supercomputer.
- **Use Case:** Ideal for stateless web servers, caching layers (Redis), and most NoSQL databases.
- **Limitation:** Increases complexity (network calls, data consistency issues).

### Key Concepts / Tradeoffs

| Concept            | Explanation                                                                                                                                                                                 | Tradeoff                                                                                                      |
|--------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------|
| Amdahl's Law       | The theoretical speedup limit of a program using parallel processing. It states that the improvement is limited by the sequential (non-parallelizable) part of the task.                    | Scaling only helps the parallelizable parts; sequential tasks like final data aggregation become bottlenecks. |
| Tail Latency       | The latency experienced by the slowest requests (e.g., 99th percentile or p99). High tail latency significantly degrades the user experience for a small, but important, fraction of users. | Optimization should target reducing p99 latency, not just the average (p50).                                  |
| Stateless Services | A service that does not store session-specific data locally, allowing any request from a user to be routed to any available server.                                                         | Requires an external, highly available store (like Redis or a database) for session management.               |

---

## üí° Real-World Use Cases

- **Content Delivery Networks (CDNs):** Used to reduce **latency** by serving static content (images, videos) from a
  server geographically closer to the user.

- **Web Tier:** The Web/API servers are almost always **horizontally scaled** and **stateless** (scaling out) to handle
  massive QPS.

- **Gaming:** Games are extremely sensitive to **latency (ping)**, prioritizing quick response times over almost all
  other factors.

---

## ‚úèÔ∏è Design Challenge

You have a service that requires a 5-second computation for every request. If your target throughput is 100 QPS, how
many server cores do you theoretically need, assuming 80% utilization?