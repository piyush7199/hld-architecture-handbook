# 1.1.6 Failure Modes and Fault Tolerance Patterns

## Intuitive Explanation

In distributed systems, failures are not edge cases‚Äîthey are the norm. Networks fail, servers crash, disks fill up, and software has bugs. **Fault Tolerance** is the art of designing systems that continue operating correctly even when individual components fail.

- **Failure Mode:** A specific way in which a system can fail (e.g., server crash, network partition, slow dependency).
- **Fault Tolerance Pattern:** A proven design strategy to handle or mitigate specific failure modes.
- **The Goal:** Build systems that degrade gracefully rather than catastrophically fail.

---

## In-Depth Analysis

### 1. Common Failure Modes in Distributed Systems

| Failure Mode | Description | Impact | Example |
|-------------|-------------|--------|---------|
| **Crash Failure** | A node stops functioning entirely | Service unavailable | Server hardware failure, OOM crash |
| **Omission Failure** | A node fails to send or receive messages | Request timeout, data loss | Network packet loss, dropped messages |
| **Timing Failure** | A node responds too slowly | Timeout, cascade failure | Overloaded database, slow API |
| **Byzantine Failure** | A node behaves arbitrarily or maliciously | Data corruption, security breach | Software bug, compromised node |
| **Network Partition** | Network splits into isolated segments | Split-brain, inconsistency | Data center connectivity loss |
| **Cascading Failure** | One failure triggers multiple downstream failures | System-wide outage | Overload spreading across services |
| **Resource Exhaustion** | Running out of memory, disk, connections | Service degradation/crash | Memory leak, disk full, connection pool exhausted |

### 2. Core Fault Tolerance Patterns

#### Pattern 1: Circuit Breaker

The Circuit Breaker pattern prevents cascading failures by detecting when a dependency is failing and temporarily blocking requests to it.

**How It Works:**

```
States:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  (failures < threshold)  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ CLOSED  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ CLOSED ‚îÇ
‚îÇ(normal) ‚îÇ                          ‚îÇ        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ
     ‚îÇ (failures ‚â• threshold)
     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  (after timeout)        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  OPEN   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ HALF-  ‚îÇ
‚îÇ(blocked)‚îÇ                          ‚îÇ OPEN   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚ñ≤                                    ‚îÇ
     ‚îÇ (probe fails)                      ‚îÇ (probe succeeds)
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**States:**
1. **CLOSED** (Normal): Requests pass through normally. Failures are counted.
2. **OPEN** (Broken): After threshold failures, circuit opens. All requests fail fast without hitting the dependency.
3. **HALF-OPEN** (Testing): After timeout, allow one test request. If it succeeds, close the circuit. If it fails, reopen.

**Configuration:**
- **Failure Threshold:** Number of failures before opening (e.g., 5 failures)
- **Timeout:** How long to wait before testing (e.g., 30 seconds)
- **Success Threshold:** Successes needed to close (e.g., 2 successful requests)

**Example:**
```python
class CircuitBreaker:
    def __init__(self, failure_threshold=5, timeout=30):
        self.state = "CLOSED"
        self.failure_count = 0
        self.failure_threshold = failure_threshold
        self.timeout = timeout
        self.last_failure_time = None
    
    def call(self, func):
        if self.state == "OPEN":
            if time.now() - self.last_failure_time > self.timeout:
                self.state = "HALF-OPEN"
            else:
                raise CircuitOpenError("Circuit is OPEN")
        
        try:
            result = func()
            self.on_success()
            return result
        except Exception as e:
            self.on_failure()
            raise e
    
    def on_success(self):
        self.failure_count = 0
        self.state = "CLOSED"
    
    def on_failure(self):
        self.failure_count += 1
        self.last_failure_time = time.now()
        if self.failure_count >= self.failure_threshold:
            self.state = "OPEN"
```

**Benefits:**
- Prevents cascading failures
- Fails fast instead of waiting for timeouts
- Gives failing service time to recover

**Trade-offs:**
- Can cause "thundering herd" when circuit closes
- May reject valid requests during recovery
- Requires careful threshold tuning

#### Pattern 2: Retry with Exponential Backoff and Jitter

Automatically retry failed requests with increasing delays to handle transient failures.

**Algorithm:**
```
wait_time = base_delay √ó 2^attempt + random_jitter

Example with base_delay = 1s:
Attempt 1: 1s + jitter(0-1s) = 1-2s
Attempt 2: 2s + jitter(0-2s) = 2-4s
Attempt 3: 4s + jitter(0-4s) = 4-8s
Attempt 4: 8s + jitter(0-8s) = 8-16s (capped at max_delay)
```

**Why Jitter?**
- Without jitter, all clients retry at exactly the same time
- Creates synchronized "thundering herd" problem
- Jitter spreads retries over time, reducing load spikes

**Example:**
```python
import random
import time

def retry_with_backoff(func, max_attempts=5, base_delay=1, max_delay=32):
    for attempt in range(max_attempts):
        try:
            return func()
        except TransientError as e:
            if attempt == max_attempts - 1:
                raise  # Last attempt, propagate error
            
            # Exponential backoff with jitter
            wait = min(base_delay * (2 ** attempt), max_delay)
            jitter = random.uniform(0, wait)
            time.sleep(wait + jitter)
```

**When to Retry:**
- Network timeouts
- Rate limit errors (429)
- Server errors (500, 502, 503, 504)
- Temporary unavailability

**When NOT to Retry:**
- Client errors (400, 401, 403, 404)
- Business logic errors
- Non-idempotent operations (without idempotency keys)

#### Pattern 3: Bulkhead Pattern

Isolate resources to prevent a failure in one area from affecting the entire system. Named after ship compartments that prevent the entire ship from sinking if one compartment floods.

**Implementation Strategies:**

**A. Thread Pool Isolation:**
```python
# Separate thread pools for different dependencies
user_service_pool = ThreadPoolExecutor(max_workers=20)
payment_service_pool = ThreadPoolExecutor(max_workers=10)
analytics_pool = ThreadPoolExecutor(max_workers=5)

# If analytics service hangs, only 5 threads are blocked
# User and payment services continue working normally
```

**B. Connection Pool Isolation:**
```python
# Separate connection pools per service
primary_db_pool = ConnectionPool(max_connections=100)
analytics_db_pool = ConnectionPool(max_connections=20)
cache_pool = ConnectionPool(max_connections=50)
```

**C. Process/Container Isolation:**
```
Each microservice runs in separate containers
If one service crashes, others continue running
```

**Benefits:**
- Limits blast radius of failures
- Prevents resource exhaustion from affecting entire system
- Easier to identify which component is causing issues

**Trade-offs:**
- More complex resource management
- May waste resources if pools are oversized
- Requires capacity planning per component

#### Pattern 4: Timeout Pattern

Set explicit time limits for operations to prevent indefinite blocking.

**Timeout Types:**

| Type | Description | Example |
|------|-------------|---------|
| **Connection Timeout** | Max time to establish connection | 2-5 seconds |
| **Request Timeout** | Max time for entire request/response | 10-30 seconds |
| **Idle Timeout** | Max time connection can be idle | 5-10 minutes |
| **Keep-Alive Timeout** | Max time to keep persistent connection | 60-120 seconds |

**Example:**
```python
import requests

response = requests.get(
    'https://api.example.com/data',
    timeout=(3, 10)  # (connect_timeout, read_timeout)
)
```

**Best Practices:**
- Always set timeouts (never use infinite timeouts)
- Set shorter timeouts for interactive user requests
- Set longer timeouts for batch/background jobs
- Use connection pooling with keep-alive for frequently accessed services

#### Pattern 5: Graceful Degradation

When a non-critical component fails, continue operating with reduced functionality.

**Strategies:**

**A. Feature Toggles:**
```python
def get_user_recommendations(user_id):
    if recommendation_service.is_available():
        return recommendation_service.get_recommendations(user_id)
    else:
        # Fallback: return popular items
        return get_popular_items()
```

**B. Cached/Stale Data:**
```python
def get_product_details(product_id):
    try:
        return database.get_product(product_id)
    except DatabaseError:
        # Serve stale data from cache
        cached = cache.get(product_id)
        if cached:
            return cached  # Mark as potentially stale
        raise
```

**C. Simplified Response:**
```python
def get_user_profile(user_id):
    profile = get_basic_profile(user_id)  # Critical
    
    try:
        profile.preferences = get_preferences(user_id)  # Optional
    except:
        profile.preferences = {}  # Degrade gracefully
    
    try:
        profile.recommendations = get_recommendations(user_id)  # Optional
    except:
        profile.recommendations = []
    
    return profile
```

**Trade-offs:**
- Better user experience than complete failure
- Requires identifying critical vs. optional features
- May hide problems (need good monitoring)

#### Pattern 6: Rate Limiting and Load Shedding

Protect services from overload by limiting incoming request rate.

**A. Rate Limiting (Proactive):**
- Limit requests per user/API key
- Prevents abuse and ensures fair usage
- See 2.5.1 for algorithms (Token Bucket, Leaky Bucket)

**B. Load Shedding (Reactive):**
- Drop low-priority requests when overloaded
- Protect critical functionality first

**Example:**
```python
def handle_request(request):
    # Check system load
    if cpu_usage > 90 or queue_depth > 10000:
        # Shed load based on priority
        if request.priority < CRITICAL:
            return Response("Service overloaded, please retry", status=503)
    
    # Process request normally
    return process(request)
```

**Priority Classes:**
```
1. CRITICAL (always serve): Health checks, admin operations
2. HIGH (serve if possible): Paid users, core features
3. MEDIUM (shed under load): Free users, analytics
4. LOW (shed first): Background jobs, prefetch
```

#### Pattern 7: Health Checks and Self-Healing

Continuously monitor service health and automatically recover from failures.

**Health Check Types:**

**A. Liveness Probe:**
```python
# Is the service running?
@app.route('/health/live')
def liveness():
    return {"status": "UP"}, 200
```

**B. Readiness Probe:**
```python
# Is the service ready to accept traffic?
@app.route('/health/ready')
def readiness():
    checks = {
        "database": check_database_connection(),
        "cache": check_cache_connection(),
        "disk_space": check_disk_space() > 10_GB
    }
    
    if all(checks.values()):
        return {"status": "READY", "checks": checks}, 200
    else:
        return {"status": "NOT_READY", "checks": checks}, 503
```

**C. Startup Probe:**
```python
# Has the service finished initialization?
@app.route('/health/startup')
def startup():
    if app.is_initialized:
        return {"status": "STARTED"}, 200
    else:
        return {"status": "STARTING"}, 503
```

**Self-Healing Mechanisms:**
- **Kubernetes:** Automatically restarts failed pods
- **Auto-scaling:** Adds instances when load increases
- **Circuit breakers:** Automatically block traffic to failing services
- **Graceful shutdown:** Clean up resources before termination

### 3. Failure Detection and Recovery

| Technique | Detection Time | Recovery Time | Use Case |
|-----------|---------------|---------------|----------|
| **Active Health Checks** | Seconds | Seconds | Load balancer removing unhealthy nodes |
| **Passive Monitoring** | Milliseconds | Varies | Circuit breakers detecting failures |
| **Heartbeats** | Seconds | Seconds | Distributed consensus (Raft, Paxos) |
| **Timeouts** | Milliseconds | Immediate | Request-level failure detection |
| **Watchdogs** | Seconds | Seconds | Process monitoring and restart |

### 4. Fault Tolerance Anti-Patterns

| Anti-Pattern | Problem | Better Approach |
|--------------|---------|-----------------|
| **Retry Everything** | Amplifies load on failing service | Use circuit breakers, only retry transient errors |
| **Infinite Timeout** | Threads blocked forever | Always set explicit timeouts |
| **Synchronous Cascades** | Failures propagate instantly | Use async communication, message queues |
| **Single Points of Failure** | No redundancy | Multi-region deployment, replication |
| **Tight Coupling** | One service failure breaks entire system | Loose coupling, graceful degradation |
| **No Monitoring** | Can't detect failures | Comprehensive observability (logs, metrics, traces) |
| **Optimistic Resource Assumptions** | System breaks under load | Capacity planning, load testing |

---

## üí° Real-World Use Cases

- **Netflix:** Uses Chaos Engineering (deliberately injecting failures) to test fault tolerance. Their Chaos Monkey randomly terminates production instances.
- **AWS:** Uses Cell-based Architecture to limit blast radius. A failure in one cell doesn't affect others.
- **Google:** Uses Circuit Breakers and Bulkheads extensively in their internal RPC framework.
- **Uber:** Implements sophisticated retry logic with exponential backoff for all inter-service communication.

---

## ‚úèÔ∏è Design Challenge

### Problem

You are designing the checkout flow for an e-commerce site. The checkout process depends on three external services:

1. **Payment Gateway** (critical, slow, occasionally times out)
2. **Inventory Service** (critical, fast, highly available)
3. **Recommendation Engine** (non-critical, occasionally fails)

Design a fault-tolerant checkout flow that:
- Completes successfully even if the Recommendation Engine is down
- Handles Payment Gateway timeouts gracefully
- Prevents cascading failures if any service becomes slow
- Provides good user experience under degraded conditions

Which fault tolerance patterns would you apply to each dependency, and why?

### Solution

#### üß© Scenario Analysis

| Service | Criticality | Characteristics | Failure Mode |
|---------|------------|-----------------|--------------|
| **Payment Gateway** | Critical | Slow, timeouts | Timing failure |
| **Inventory Service** | Critical | Fast, stable | Rare crashes |
| **Recommendation Engine** | Non-critical | Unreliable | Omission failure |

#### ‚úÖ Step 1: Apply Circuit Breaker to Payment Gateway

**Why?**
- Payment Gateway is slow and occasionally times out
- Don't want to wait for full timeout on every request during an outage
- Need to fail fast to prevent user frustration

**Configuration:**
```python
payment_circuit = CircuitBreaker(
    failure_threshold=5,        # Open after 5 consecutive failures
    timeout=30,                 # Wait 30s before testing recovery
    request_timeout=10          # 10s max per payment attempt
)
```

**Flow:**
```python
def process_payment(order):
    try:
        return payment_circuit.call(
            lambda: payment_gateway.charge(order)
        )
    except CircuitOpenError:
        # Circuit is open - payment gateway is down
        return handle_payment_unavailable(order)
```

**Degraded Behavior:**
- When circuit is OPEN, immediately show user-friendly error
- Offer alternative payment methods (saved cards, PayPal)
- Queue order for later processing
- Send notification when payment gateway recovers

#### ‚úÖ Step 2: Apply Retry with Backoff to Payment Gateway

**Why?**
- Payment gateway timeouts are often transient
- Network blips or momentary overload resolve quickly
- But we need to be careful not to double-charge

**Implementation:**
```python
def charge_with_retry(order):
    return retry_with_backoff(
        func=lambda: payment_gateway.charge(order),
        max_attempts=3,
        base_delay=1,
        max_delay=5,
        idempotency_key=order.id  # Prevent double-charging
    )
```

**Critical: Idempotency**
```python
# Always send idempotency key to prevent double-charging
headers = {
    'Idempotency-Key': f"order-{order.id}-{timestamp}"
}
response = payment_gateway.charge(order, headers=headers)
```

#### ‚úÖ Step 3: Apply Bulkhead Pattern for Service Isolation

**Why?**
- Prevent slow Payment Gateway from exhausting all threads
- Ensure Inventory Service and Recommendations run independently

**Implementation:**
```python
# Separate thread pools
payment_pool = ThreadPoolExecutor(max_workers=10)
inventory_pool = ThreadPoolExecutor(max_workers=20)
recommendation_pool = ThreadPoolExecutor(max_workers=5)

def checkout(order):
    # These run in isolated thread pools
    inventory_future = inventory_pool.submit(reserve_inventory, order)
    recommendation_future = recommendation_pool.submit(get_upsells, order)
    
    # Critical path
    inventory_result = inventory_future.result(timeout=2)
    
    # Non-critical path
    try:
        recommendations = recommendation_future.result(timeout=1)
    except TimeoutError:
        recommendations = []  # Graceful degradation
    
    # Payment in isolated pool
    payment_future = payment_pool.submit(charge_with_retry, order)
    payment_result = payment_future.result(timeout=15)
    
    return create_order_confirmation(order, recommendations)
```

#### ‚úÖ Step 4: Apply Graceful Degradation to Recommendation Engine

**Why?**
- Recommendations are non-critical
- Better to show no recommendations than fail checkout

**Implementation:**
```python
def get_checkout_recommendations(order):
    try:
        # Try to get personalized recommendations
        return recommendation_service.get_upsells(
            order_id=order.id,
            timeout=1  # Short timeout - not critical
        )
    except (TimeoutError, ServiceUnavailableError):
        # Fallback 1: Return popular items
        try:
            return cache.get_popular_items(order.category)
        except:
            # Fallback 2: No recommendations
            return []
```

**User Experience:**
```
‚úÖ With recommendations: "Customers also bought..."
‚úÖ Without recommendations: "Complete your purchase"
(Checkout still works perfectly)
```

#### ‚úÖ Step 5: Apply Timeout Pattern Everywhere

**Why?**
- Prevent indefinite blocking
- Ensure predictable latency

**Configuration:**
```python
TIMEOUTS = {
    'inventory_check': 2,      # Fast, should respond quickly
    'payment_gateway': 10,     # Slower, but cap at 10s
    'recommendations': 1,      # Non-critical, fail fast
    'overall_checkout': 20     # Total checkout max time
}
```

#### ‚úÖ Step 6: Add Monitoring and Alerting

**Critical Metrics:**
```python
metrics = {
    'payment_circuit_state': 'CLOSED|OPEN|HALF-OPEN',
    'payment_success_rate': 0.99,  # Alert if < 95%
    'payment_p95_latency': 3.5,    # Alert if > 5s
    'checkout_completion_rate': 0.95,
    'inventory_errors': 5,          # Alert if > 10/min
    'recommendation_timeouts': 50   # Log only, don't alert
}
```

#### ‚úÖ Complete Checkout Flow

```python
def checkout(order):
    start_time = time.now()
    
    # 1. Reserve inventory (critical, fast)
    try:
        inventory_result = inventory_service.reserve(
            order.items,
            timeout=TIMEOUTS['inventory_check']
        )
    except InventoryError as e:
        return CheckoutError("Items not available", status=409)
    
    # 2. Get recommendations (non-critical, can fail)
    recommendations = get_checkout_recommendations(order)
    
    # 3. Process payment (critical, slow, fault-tolerant)
    try:
        payment_result = payment_circuit.call(
            lambda: charge_with_retry(order)
        )
    except CircuitOpenError:
        # Payment gateway is down
        inventory_service.release(order.items)
        return CheckoutError(
            "Payment service temporarily unavailable. Please try again.",
            status=503,
            retry_after=60
        )
    except PaymentDeclined as e:
        # User's card declined
        inventory_service.release(order.items)
        return CheckoutError("Payment declined", status=402)
    
    # 4. Confirm order
    order.confirm(payment_result, recommendations)
    
    # 5. Monitor total checkout time
    duration = time.now() - start_time
    metrics.record('checkout_duration', duration)
    if duration > TIMEOUTS['overall_checkout']:
        log.warning(f"Slow checkout: {duration}s")
    
    return CheckoutSuccess(order)
```

#### ‚öñÔ∏è Trade-offs Summary

| Pattern | Benefit | Cost |
|---------|---------|------|
| **Circuit Breaker** | Fast failure, prevents cascades | May reject valid requests during recovery |
| **Retry with Backoff** | Handles transient failures | Increases latency, risk of duplicate operations |
| **Bulkhead** | Isolates failures | More complex resource management |
| **Graceful Degradation** | Better UX than complete failure | Requires identifying critical vs. optional features |
| **Timeouts** | Predictable latency | May cut off slow-but-valid operations |

#### ‚úÖ Final Summary

| Service | Patterns Applied | Reason |
|---------|-----------------|--------|
| **Payment Gateway** | Circuit Breaker + Retry + Bulkhead + Timeout | Critical, slow, prone to timeouts |
| **Inventory Service** | Timeout + Bulkhead | Critical, fast, but isolate to prevent blocking |
| **Recommendation Engine** | Graceful Degradation + Short Timeout | Non-critical, allow to fail silently |

**Result:** Resilient checkout flow that:
- ‚úÖ Completes even if recommendations fail
- ‚úÖ Fails fast when payment gateway is down
- ‚úÖ Prevents cascading failures via isolation
- ‚úÖ Provides clear error messages to users
- ‚úÖ Maintains good UX under degraded conditions

