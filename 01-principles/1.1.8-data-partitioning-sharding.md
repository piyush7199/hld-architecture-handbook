# 1.1.8 Data Partitioning and Sharding: Scaling Beyond Single Machine

## Intuitive Explanation

Imagine you're running a library with millions of books. Instead of storing all books in one massive room (which becomes impossible to navigate), you **partition** the books:

- **By Genre:** Fiction, Non-Fiction, Science, History (each in separate sections)
- **By Author Last Name:** A-F, G-M, N-S, T-Z (alphabetical partitioning)
- **By Popularity:** Recent bestsellers in front, classics in back

**Data Partitioning (Sharding)** is the same concept for databases: splitting a large dataset into smaller, manageable pieces distributed across multiple machines. Each machine (shard) holds a subset of the data, allowing the system to scale horizontally beyond the limits of a single server.

---

## In-Depth Analysis

### Definition

**Partitioning (Sharding)** is the process of dividing a large database or dataset into smaller, independent pieces called **partitions** or **shards**. Each shard is stored on a separate machine and operates independently.

**Key Goals:**
- **Horizontal Scaling:** Add more machines to handle more data/load
- **Performance:** Smaller datasets = faster queries
- **Availability:** Failure of one shard doesn't affect others
- **Cost:** Use cheaper, commodity hardware instead of expensive single machines

### Why Partition?

**Single Machine Limitations:**
- **Storage:** Can't store infinite data (disk space limits)
- **Memory:** Can't fit entire dataset in RAM
- **CPU:** Single machine can't process unlimited queries
- **Network:** Single machine has limited network bandwidth
- **Cost:** Expensive to scale vertically (bigger machines cost exponentially more)

**Partitioning Benefits:**
- **Linear Scaling:** Add 10 machines = 10x capacity
- **Parallel Processing:** Multiple shards process queries simultaneously
- **Fault Isolation:** One shard failure doesn't bring down entire system
- **Cost Effective:** Commodity hardware is cheaper than supercomputers

---

## Key Concepts / Tradeoffs

### 1. Partitioning Strategies

#### A. Range-Based Partitioning

**How It Works:**
- Divide data by value ranges
- Example: User IDs 1-1M ‚Üí Shard 1, 1M-2M ‚Üí Shard 2, etc.

**Example:**
```
Shard 1: user_id 1 - 1,000,000
Shard 2: user_id 1,000,001 - 2,000,000
Shard 3: user_id 2,000,001 - 3,000,000
```

**Pros:**
- Simple to implement
- Easy range queries (e.g., "users created in January")
- Good for time-series data (partition by date)

**Cons:**
- **Hotspotting:** Popular ranges (e.g., recent users) overload specific shards
- **Uneven Distribution:** Data may not be evenly distributed across ranges
- **Resharding Complexity:** Adding new shards requires data migration

**Use Cases:**
- Time-series data (partition by date)
- Sequential IDs (user_id ranges)
- Alphabetical data (partition by first letter)

#### B. Hash-Based Partitioning

**How It Works:**
- Apply hash function to partition key
- Use hash value modulo number of shards to determine shard

**Example:**
```
hash(user_id) % 4 = shard_number

user_id = 12345
hash(12345) = 7890123
7890123 % 4 = 3
‚Üí Shard 3
```

**Pros:**
- **Even Distribution:** Hash function distributes data evenly
- **No Hotspotting:** Popular keys distributed across shards
- **Simple Lookup:** O(1) shard determination

**Cons:**
- **No Range Queries:** Can't query "users 1-1000" (scattered across shards)
- **Resharding Complexity:** Changing shard count requires rehashing all data
- **Hash Collisions:** Rare but possible

**Use Cases:**
- User IDs (random distribution)
- Session data
- Any data where range queries aren't needed

#### C. Directory-Based Partitioning

**How It Works:**
- Maintain a lookup table (directory) mapping keys to shards
- Query directory to find which shard contains data

**Example:**
```
Directory:
user_id: 12345 ‚Üí Shard 2
user_id: 67890 ‚Üí Shard 1
user_id: 11111 ‚Üí Shard 3
```

**Pros:**
- **Flexible:** Easy to move data between shards
- **Dynamic:** Can add/remove shards without data migration
- **Custom Logic:** Can use any partitioning logic (not just hash/range)

**Cons:**
- **Single Point of Failure:** Directory becomes bottleneck
- **Latency:** Extra lookup adds latency
- **Complexity:** Must maintain and update directory

**Use Cases:**
- When shard assignment needs custom logic
- When data moves frequently between shards
- When shard count changes often

#### D. Consistent Hashing

**How It Works:**
- Map both data and shards to a hash ring
- Data belongs to first shard clockwise on ring
- Adding/removing shards only affects adjacent data

**Pros:**
- **Minimal Reshuffling:** Adding/removing shards affects minimal data
- **Even Distribution:** Hash ring distributes data evenly
- **No Central Directory:** No single point of failure

**Cons:**
- **Complexity:** More complex than simple hash partitioning
- **Virtual Nodes:** Need virtual nodes for even distribution

**Use Cases:**
- Distributed caches (Redis, Memcached)
- When shards are added/removed frequently
- When resharding cost must be minimized

### 2. Partition Key Selection

**Critical Decision:** Choosing the right partition key determines:
- Data distribution (even vs uneven)
- Query performance (single shard vs multi-shard)
- Resharding complexity

**Good Partition Keys:**
- **High Cardinality:** Many unique values (user_id, order_id)
- **Even Distribution:** Values distributed evenly
- **Query Pattern:** Matches common query patterns

**Bad Partition Keys:**
- **Low Cardinality:** Few unique values (status: active/inactive)
- **Skewed Distribution:** Most data in one value (country: 80% in USA)
- **Frequently Updated:** Causes data movement between shards

**Example:**
```
‚ùå Bad: Partition by "status" (active/inactive)
   ‚Üí 90% of data in "active" shard (hotspotting)

‚úÖ Good: Partition by "user_id"
   ‚Üí Even distribution, matches query pattern (get user by ID)
```

### 3. Multi-Shard Queries

**Challenge:** Queries that need data from multiple shards

**Types:**
1. **Scatter-Gather:** Query all shards, merge results
2. **Broadcast:** Send query to all shards
3. **Fan-Out:** Query specific shards based on criteria

**Example:**
```
Query: "Get all users created in last 7 days"

Range Partitioning:
‚Üí Query only relevant shards (recent date ranges)
‚Üí Merge results

Hash Partitioning:
‚Üí Query ALL shards (data scattered)
‚Üí Merge results (expensive)
```

**Optimization:**
- **Secondary Indexes:** Maintain indexes across shards
- **Materialized Views:** Pre-aggregate data
- **Denormalization:** Duplicate data to avoid joins

### 4. Resharding (Re-partitioning)

**When Needed:**
- Shard grows too large (storage/performance limits)
- Uneven distribution (hotspotting)
- Adding/removing shards

**Strategies:**

**A. Offline Resharding:**
- Take system offline
- Redistribute data
- Bring system back online
- **Pros:** Simple, no complexity
- **Cons:** Downtime (unacceptable for most systems)

**B. Online Resharding:**
- Continue serving traffic
- Gradually move data
- Update routing
- **Pros:** No downtime
- **Cons:** Complex, requires careful coordination

**C. Double-Write Strategy:**
```
1. Write to both old and new shard
2. Background job migrates data
3. Once complete, read from new shard
4. Stop writing to old shard
```

---

## üí° Real-World Use Cases

### Instagram (User Data Sharding)

**Partitioning Strategy:** Hash-based by user_id

**Why:**
- Even distribution of users
- User queries are by user_id (single shard lookup)
- No range queries needed

**Scale:**
- Billions of users
- Thousands of shards
- Each shard: ~1M users

### Twitter (Tweet Storage)

**Partitioning Strategy:** Hash-based by tweet_id

**Why:**
- Even distribution of tweets
- Tweet lookups by ID (single shard)
- Timeline generation queries multiple shards (acceptable)

**Scale:**
- Trillions of tweets
- Thousands of shards

### Amazon (Product Catalog)

**Partitioning Strategy:** Range-based by product_category

**Why:**
- Queries often filter by category (single shard)
- Categories have similar sizes (even distribution)
- Range queries within category (e.g., "electronics under $100")

**Scale:**
- Billions of products
- Hundreds of shards (one per major category)

### YouTube (Video Metadata)

**Partitioning Strategy:** Hash-based by video_id

**Why:**
- Even distribution of videos
- Video lookups by ID (single shard)
- Search queries hit all shards (acceptable with search index)

**Scale:**
- Billions of videos
- Thousands of shards

---

## ‚úèÔ∏è Design Challenge

### Problem

You are designing a social media platform's user post storage system. The system must store 10 billion posts and handle 1 million reads per second.

**Requirements:**
1. Store 10 billion posts (average 2 KB per post = 20 TB)
2. Handle 1 million reads/sec (mostly by post_id)
3. Support queries: "Get posts by user_id" (user's timeline)
4. Support queries: "Get recent posts" (global feed)
5. Scale horizontally (add shards as data grows)

**Constraints:**
- Single machine can store 1 TB and handle 10k reads/sec
- Need to partition data across multiple machines
- Queries by post_id must be fast (<10ms)
- User timeline queries should be efficient

Design a partitioning strategy that:
- Distributes data evenly
- Supports efficient queries
- Handles growth (adding new shards)
- Minimizes cross-shard queries

### Solution

#### üß© Scenario

- **Data:** 10 billion posts, 20 TB total
- **Read Load:** 1 million reads/sec
- **Query Patterns:**
  - By post_id (80% of queries)
  - By user_id (user timeline, 15% of queries)
  - Recent posts (global feed, 5% of queries)
- **Single Machine:** 1 TB storage, 10k reads/sec

**Calculations:**
- **Shards Needed (Storage):** 20 TB √∑ 1 TB = 20 shards minimum
- **Shards Needed (Throughput):** 1M reads/sec √∑ 10k reads/sec = 100 shards minimum
- **Final:** 100 shards (throughput is the bottleneck)

#### ‚úÖ Step 1: Primary Partitioning Strategy

**Choice: Hash-Based Partitioning by post_id**

**Why:**
- **Even Distribution:** Hash function distributes posts evenly across shards
- **Single-Shard Lookups:** 80% of queries (by post_id) hit single shard
- **No Hotspotting:** Popular posts distributed across shards
- **Simple:** O(1) shard determination

**Implementation:**
```
shard_number = hash(post_id) % 100

Example:
post_id = "post_abc123"
hash("post_abc123") = 7890123456
7890123456 % 100 = 56
‚Üí Shard 56
```

**Result:**
- Each shard: ~100M posts, ~200 GB
- Each shard: ~10k reads/sec (within capacity)
- Post lookup: Single shard query (<10ms)

#### ‚úÖ Step 2: Handling User Timeline Queries

**Problem:** "Get posts by user_id" requires querying all 100 shards (expensive)

**Solution: Secondary Index (User ‚Üí Posts Mapping)**

**Option A: Separate User-Posts Index**

```
Shard posts by user_id (separate from post storage):
- Shard posts by: hash(user_id) % 100
- Each shard stores: user_id ‚Üí [post_id1, post_id2, ...]
- Query: Get user's post_ids from index shard ‚Üí Fetch posts from post shards
```

**Option B: Denormalize (Store user_id in post shard)**

```
Each post shard maintains index:
- Index: user_id ‚Üí [post_id1, post_id2, ...]
- Query: Query all 100 shards for user_id (scatter-gather)
- Merge results
```

**Chosen: Option A (Separate Index)**

**Why:**
- **Efficient:** Single shard lookup for user's post list
- **Scalable:** Index shards scale independently
- **Trade-off:** Two-step query (index ‚Üí posts)

**Implementation:**
```
1. Query user-posts index: GET user_posts:{user_id}
   ‚Üí Returns: [post_id1, post_id2, ...]
2. Query post shards: GET post:{post_id1}, GET post:{post_id2}, ...
   ‚Üí Parallel queries to relevant shards
3. Merge results
```

**Performance:**
- Index lookup: 1 shard query (~5ms)
- Post fetches: Parallel queries to ~10 shards (~10ms)
- Total: ~15ms (acceptable)

#### ‚úÖ Step 3: Handling Recent Posts Query

**Problem:** "Get recent posts" requires querying all 100 shards

**Solution: Pre-Computed Global Feed**

**Strategy:**
- Maintain separate "recent posts" cache (Redis Sorted Set)
- Background job updates cache every 60 seconds
- Query cache instead of all shards

**Implementation:**
```
Redis Sorted Set:
Key: recent_posts:global
Members: post_id
Scores: timestamp

Query: ZREVRANGE recent_posts:global 0 99
‚Üí Returns top 100 recent post_ids
‚Üí Fetch posts from relevant shards
```

**Performance:**
- Cache lookup: <1ms
- Post fetches: Parallel queries (~10ms)
- Total: ~11ms (fast)

#### ‚úÖ Step 4: Resharding Strategy

**When:** Shard grows beyond 1 TB or 10k reads/sec

**Strategy: Consistent Hashing with Virtual Nodes**

**Why:**
- Minimal data movement when adding shards
- Even distribution
- No central directory

**Implementation:**
```
1. Map shards to hash ring (with virtual nodes)
2. When adding shard:
   a. Add shard to ring
   b. Migrate data from adjacent shards (only affected data)
   c. Update routing
3. When removing shard:
   a. Migrate data to adjacent shards
   b. Remove from ring
```

**Data Migration:**
- **Online Migration:** Continue serving traffic
- **Double-Write:** Write to both old and new shard during migration
- **Background Job:** Gradually migrate data
- **Cutover:** Once complete, read from new shard

#### ‚úÖ Step 5: Handling Hot Keys

**Problem:** Celebrity posts receive 100k reads/sec (overwhelms single shard)

**Solution: Multi-Level Caching**

**Strategy:**
1. **L1: Application Cache:** Cache popular posts in-memory
2. **L2: Redis Cache:** Distributed cache for hot posts
3. **L3: Shard:** Database shard (source of truth)

**Implementation:**
```
Hot post lookup:
1. Check application cache (hit rate: 50%)
2. If miss, check Redis cache (hit rate: 30%)
3. If miss, query shard (20% of requests)
```

**Result:**
- 80% of requests served from cache
- Shard handles 20k reads/sec (within capacity)

#### ‚úÖ Complete Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Application Layer                      ‚îÇ
‚îÇ  (In-Memory Cache for Hot Posts)                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ              ‚îÇ              ‚îÇ
        ‚ñº              ‚ñº              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Post Shards  ‚îÇ ‚îÇ   Redis  ‚îÇ ‚îÇ User-Posts  ‚îÇ
‚îÇ (100 shards) ‚îÇ ‚îÇ  Cache   ‚îÇ ‚îÇ Index Shards ‚îÇ
‚îÇ              ‚îÇ ‚îÇ          ‚îÇ ‚îÇ (100 shards) ‚îÇ
‚îÇ Hash by      ‚îÇ ‚îÇ Hot Posts‚îÇ ‚îÇ Hash by      ‚îÇ
‚îÇ post_id      ‚îÇ ‚îÇ          ‚îÇ ‚îÇ user_id      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Query Flows:**

**1. Get Post by ID (80% of queries):**
```
post_id ‚Üí hash(post_id) % 100 ‚Üí Shard N ‚Üí Return post
Latency: <10ms (single shard)
```

**2. Get User Timeline (15% of queries):**
```
user_id ‚Üí hash(user_id) % 100 ‚Üí Index Shard ‚Üí Get post_ids
‚Üí Parallel fetch from post shards ‚Üí Merge results
Latency: ~15ms (index + parallel fetches)
```

**3. Get Recent Posts (5% of queries):**
```
‚Üí Redis Sorted Set ‚Üí Get top 100 post_ids
‚Üí Parallel fetch from post shards ‚Üí Merge results
Latency: ~11ms (cache + parallel fetches)
```

#### ‚öñÔ∏è Trade-offs Summary

| Decision | What We Gain | What We Sacrifice |
|----------|--------------|-------------------|
| **Hash by post_id** | Even distribution, single-shard lookups | Can't do range queries efficiently |
| **Separate User-Posts Index** | Efficient user timeline queries | Two-step query (index ‚Üí posts) |
| **Pre-Computed Recent Posts** | Fast global feed | Slight staleness (60-second delay) |
| **Consistent Hashing** | Minimal resharding cost | More complex than simple hash |
| **Multi-Level Caching** | Handles hot keys | Memory cost, cache invalidation complexity |

#### ‚úÖ Final Summary

**Partitioning Strategy:**
- **Primary:** Hash-based by post_id (100 shards)
- **Secondary Index:** Hash-based by user_id (100 index shards)
- **Caching:** Redis for hot posts, in-memory for very hot posts

**Performance:**
- **Post Lookup:** <10ms (single shard)
- **User Timeline:** ~15ms (index + parallel fetches)
- **Recent Posts:** ~11ms (cache + parallel fetches)
- **Throughput:** 1M reads/sec (100 shards √ó 10k reads/sec)

**Scalability:**
- **Storage:** 20 TB √∑ 100 shards = 200 GB per shard (within 1 TB limit)
- **Throughput:** 1M reads/sec √∑ 100 shards = 10k reads/sec per shard (within limit)
- **Growth:** Add shards using consistent hashing (minimal data movement)

**Result:**
- ‚úÖ Even data distribution
- ‚úÖ Efficient single-shard queries
- ‚úÖ Handles user timeline and recent posts
- ‚úÖ Scales horizontally
- ‚úÖ Handles hot keys with caching

