# 1.2.5 Service Discovery: Finding Services in Microservices Architecture

## Intuitive Explanation

Imagine you're in a large office building and need to find a colleague. You have two options:

1. **Hard-Coded Address:** "John is always in Room 305" ‚Äî but what if John moves to a different room? You'd need to
   update every reference.
2. **Service Directory:** Ask the receptionist "Where is John?" and they tell you the current room number. If John
   moves, only the directory needs updating.

**Service Discovery** in microservices works the same way. Instead of hard-coding service addresses (IPs, ports),
services register themselves in a **service registry** (directory), and other services query the registry to find them
dynamically.

**Why It Matters:**

- **Dynamic Scaling:** Services can be added/removed without configuration changes
- **Load Balancing:** Multiple instances of a service can be discovered
- **Failure Handling:** Failed services are automatically removed from registry
- **Zero Configuration:** Services find each other automatically

---

## In-Depth Analysis

### Definition

**Service Discovery** is the automatic detection of services and their network locations (IP addresses and ports) in a
distributed system. It consists of two main components:

1. **Service Registry:** A centralized directory that maintains a list of available services and their locations
2. **Service Registration:** Services register themselves when they start and deregister when they stop

**Key Goals:**

- **Dynamic Service Location:** Find services without hard-coding addresses
- **Load Distribution:** Discover multiple instances of a service
- **Health Monitoring:** Automatically remove unhealthy services
- **Zero-Configuration:** Services discover each other automatically

### Why Service Discovery?

**Problems Without Service Discovery:**

**1. Hard-Coded Addresses:**

```
Service A ‚Üí Service B (hard-coded: 10.0.1.5:8080)
Problem: If Service B moves or scales, Service A breaks
```

**2. Manual Configuration:**

```
Every service must know every other service's address
Problem: Configuration management nightmare, doesn't scale
```

**3. No Load Balancing:**

```
Service A always calls same instance of Service B
Problem: Can't distribute load across multiple instances
```

**4. No Failure Handling:**

```
Service A calls dead instance of Service B
Problem: Requests fail until manual intervention
```

**Benefits With Service Discovery:**

- **Dynamic:** Services added/removed automatically
- **Scalable:** Handles hundreds of services
- **Resilient:** Automatically handles failures
- **Load Balanced:** Distributes requests across instances

---

## Key Concepts / Tradeoffs

### 1. Service Discovery Patterns

#### A. Client-Side Discovery

**How It Works:**

- Client queries service registry to find service instances
- Client selects an instance (load balancing logic in client)
- Client directly calls the selected instance

**Flow:**

```
1. Client ‚Üí Service Registry: "Where is User Service?"
2. Service Registry ‚Üí Client: [instance1, instance2, instance3]
3. Client ‚Üí Load Balancer: Select instance (round-robin, etc.)
4. Client ‚Üí Selected Instance: Make request
```

**Pros:**

- **Simple:** No additional infrastructure (no load balancer)
- **Flexible:** Client can implement custom load balancing
- **Low Latency:** Direct connection to service (no proxy)

**Cons:**

- **Client Complexity:** Each client must implement discovery logic
- **Coupling:** Clients coupled to service registry
- **Language-Specific:** Each language needs discovery library

**Examples:**

- Netflix Eureka (with client-side libraries)
- Consul (with client-side integration)

#### B. Server-Side Discovery

**How It Works:**

- Client calls a load balancer (doesn't know about registry)
- Load balancer queries service registry
- Load balancer routes request to service instance

**Flow:**

```
1. Client ‚Üí Load Balancer: Request to "user-service"
2. Load Balancer ‚Üí Service Registry: "Where is user-service?"
3. Service Registry ‚Üí Load Balancer: [instance1, instance2, instance3]
4. Load Balancer ‚Üí Selected Instance: Route request
5. Selected Instance ‚Üí Load Balancer ‚Üí Client: Response
```

**Pros:**

- **Client Simplicity:** Clients don't need discovery logic
- **Language Agnostic:** Works with any client (HTTP, gRPC)
- **Centralized:** Load balancing logic in one place

**Cons:**

- **Infrastructure:** Requires load balancer (additional component)
- **Latency:** Extra hop through load balancer
- **Single Point of Failure:** Load balancer becomes bottleneck

**Examples:**

- AWS ELB (Elastic Load Balancer)
- Kubernetes Service (with kube-proxy)
- NGINX with Consul template

### 2. Service Registry Types

#### A. Self-Registration (Push Model)

**How It Works:**

- Service registers itself when it starts
- Service sends heartbeats to keep registration alive
- Service deregisters itself when it stops

**Flow:**

```
1. Service starts ‚Üí Register with registry
2. Service ‚Üí Registry: Heartbeat every 30 seconds
3. Service stops ‚Üí Deregister from registry
```

**Pros:**

- **Simple:** Service controls its own registration
- **Fast:** Immediate registration
- **Flexible:** Service can include metadata (version, region)

**Cons:**

- **Coupling:** Service must know about registry
- **Failure Handling:** If service crashes, must rely on TTL/heartbeat
- **Language-Specific:** Each service needs registration library

**Examples:**

- Netflix Eureka (self-registration)
- Consul (service registration API)

#### B. Third-Party Registration (Pull Model)

**How It Works:**

- Service doesn't register itself
- External agent (service registrar) discovers and registers services
- Agent monitors service health

**Flow:**

```
1. Service starts (doesn't register)
2. Service Registrar ‚Üí Discovers service (via deployment system)
3. Service Registrar ‚Üí Service Registry: Register service
4. Service Registrar ‚Üí Monitors service health
5. Service stops ‚Üí Service Registrar ‚Üí Deregister
```

**Pros:**

- **Decoupling:** Service doesn't know about registry
- **Language Agnostic:** Works with any service
- **Centralized:** Registration logic in one place

**Cons:**

- **Infrastructure:** Requires service registrar (additional component)
- **Complexity:** More moving parts
- **Deployment Integration:** Must integrate with deployment system

**Examples:**

- Kubernetes (kubelet registers pods)
- AWS ECS (registers containers)
- Marathon (Mesos service registration)

### 3. Service Registry Implementations

#### A. etcd

**What:** Distributed key-value store with watch support

**Features:**

- **Consistency:** Strong consistency (Raft consensus)
- **Watch:** Clients can watch for changes
- **TTL:** Keys expire automatically
- **High Availability:** Clustered deployment

**Use Case:**

- Kubernetes (service registry)
- When strong consistency is required

#### B. Consul

**What:** Service discovery and configuration management

**Features:**

- **Service Discovery:** Built-in service registry
- **Health Checks:** Automatic health monitoring
- **Multi-Datacenter:** Supports multiple regions
- **DNS Interface:** DNS-based service discovery

**Use Case:**

- Microservices architectures
- Multi-datacenter deployments

#### C. Eureka (Netflix)

**What:** Service registry with client-side discovery

**Features:**

- **Self-Registration:** Services register themselves
- **Client Libraries:** Java, .NET, etc.
- **AP System:** High availability, eventual consistency

**Use Case:**

- Netflix-style microservices
- Java-based services

#### D. Kubernetes Service

**What:** Built-in service discovery in Kubernetes

**Features:**

- **Automatic:** Pods automatically registered
- **DNS-Based:** Services accessible via DNS
- **Load Balancing:** Built-in load balancing

**Use Case:**

- Kubernetes deployments
- Containerized microservices

### 4. Health Checks

**Purpose:** Automatically remove unhealthy services from registry

**Types:**

**A. Active Health Checks:**

- Registry periodically checks service health
- Removes service if health check fails

**B. Passive Health Checks:**

- Service sends heartbeats to registry
- Registry removes service if heartbeat stops

**C. Hybrid:**

- Service sends heartbeats (fast failure detection)
- Registry also does active checks (verification)

**Health Check Endpoints:**

```
GET /health
Response: {
  "status": "healthy",
  "checks": {
    "database": "ok",
    "cache": "ok"
  }
}
```

### 5. Service Discovery in Practice

**Common Patterns:**

**A. DNS-Based Discovery:**

```
Service Name: user-service
DNS Query: user-service.internal
Returns: [10.0.1.5, 10.0.1.6, 10.0.1.7]
```

**B. API-Based Discovery:**

```
GET /services/user-service
Returns: [
  {"host": "10.0.1.5", "port": 8080},
  {"host": "10.0.1.6", "port": 8080}
]
```

**C. Configuration-Based:**

```
Service reads from configuration service (Consul, etcd)
Gets list of service instances
Caches and refreshes periodically
```

---

## üí° Real-World Use Cases

### Kubernetes

**Service Discovery:** Built-in Kubernetes Service

**How It Works:**

- Pods automatically registered via labels
- Service object creates DNS entry
- Services accessible via DNS: `user-service.default.svc.cluster.local`
- kube-proxy load balances requests

**Example:**

```yaml
# Service Definition
apiVersion: v1
kind: Service
metadata:
  name: user-service
spec:
  selector:
    app: user-service
  ports:
    - port: 80
      targetPort: 8080
```

**Access:**

```python
# Service accessible via DNS
response = requests.get('http://user-service/api/users')
# Kubernetes routes to healthy pod
```

### Netflix (Eureka)

**Service Discovery:** Eureka with client-side discovery

**How It Works:**

- Services register with Eureka on startup
- Services query Eureka to find other services
- Client-side load balancing (Ribbon)

**Example:**

```java
// Service registers with Eureka
@SpringBootApplication
@EnableEurekaClient
public class UserService {
    // Automatically registers on startup
}

// Client discovers service
@LoadBalanced
RestTemplate restTemplate;

// Automatically load balances across instances
String response = restTemplate.getForObject(
        "http://order-service/api/orders", String.class
);
```

### AWS ECS

**Service Discovery:** AWS Cloud Map (service registry)

**How It Works:**

- ECS automatically registers tasks with Cloud Map
- Services discover each other via DNS
- Health checks remove unhealthy tasks

**Example:**

```python
# Service automatically registered
# Access via DNS
import requests
response = requests.get('http://user-service.ecs.internal/api/users')
```

### Consul

**Service Discovery:** Consul with DNS or API

**How It Works:**

- Services register with Consul
- Consul provides DNS interface
- Health checks remove unhealthy services

**Example:**

```python
# Service registration
import consul
c = consul.Consul()
c.agent.service.register(
    'user-service',
    service_id='user-service-1',
    address='10.0.1.5',
    port=8080,
    check=consul.Check.http('http://10.0.1.5:8080/health', '10s')
)

# Service discovery (DNS)
# Query: user-service.service.consul
# Returns: 10.0.1.5:8080, 10.0.1.6:8080
```

---

## ‚úèÔ∏è Design Challenge

### Problem

You are designing a microservices architecture with 50 services. Each service has multiple instances (3-10 instances per
service) that are dynamically scaled based on load. Services are deployed in Kubernetes and can be moved between nodes,
causing IP addresses to change.

**Requirements:**

1. **Dynamic Discovery:** Services must find each other without hard-coding addresses
2. **Load Balancing:** Requests must be distributed across service instances
3. **Health Monitoring:** Unhealthy instances must be automatically removed
4. **Low Latency:** Service discovery must not add significant latency (<10ms)
5. **High Availability:** Service discovery must be highly available (99.99%)

**Constraints:**

- Services use HTTP/REST and gRPC
- Services are containerized (Docker)
- Services deployed in Kubernetes
- IP addresses change when pods restart

Design a service discovery solution that:

- Automatically discovers services
- Handles dynamic scaling
- Provides load balancing
- Monitors health
- Meets latency and availability requirements

### Solution

#### üß© Scenario

- **Services:** 50 microservices
- **Instances:** 3-10 instances per service (250-500 total instances)
- **Deployment:** Kubernetes
- **Network:** Pod IPs change on restart
- **Protocols:** HTTP/REST, gRPC
- **Scale:** Dynamic scaling (instances added/removed frequently)

**Requirements:**

- Dynamic discovery (no hard-coded IPs)
- Load balancing across instances
- Health monitoring
- <10ms discovery latency
- 99.99% availability

#### ‚úÖ Step 1: Choose Service Discovery Pattern

**Choice: Server-Side Discovery with Kubernetes Service**

**Why:**

- **Built-in:** Kubernetes provides service discovery natively
- **No Additional Infrastructure:** No need for separate registry
- **Automatic:** Pods automatically registered
- **DNS-Based:** Simple, works with any language
- **Load Balancing:** Built-in via kube-proxy

**Architecture:**

```
Services ‚Üí Kubernetes Service (DNS) ‚Üí Pods (instances)
```

#### ‚úÖ Step 2: Kubernetes Service Configuration

**Service Definition:**

```yaml
apiVersion: v1
kind: Service
metadata:
  name: user-service
  namespace: default
spec:
  selector:
    app: user-service
  ports:
    - name: http
      port: 80
      targetPort: 8080
    - name: grpc
      port: 50051
      targetPort: 50051
  type: ClusterIP
  sessionAffinity: None  # Enable load balancing
```

**How It Works:**

- **Selector:** Matches pods with label `app: user-service`
- **Ports:** Exposes service on port 80 (maps to pod port 8080)
- **Type:** ClusterIP (internal service, not exposed externally)
- **Load Balancing:** kube-proxy distributes requests across pods

#### ‚úÖ Step 3: Pod Registration

**Automatic Registration:**

- Pods don't need to register themselves
- Kubernetes automatically adds pods to service when labels match
- Pods removed from service when labels don't match or pod terminates

**Pod Definition:**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: user-service-pod-1
  labels:
    app: user-service  # Matches service selector
spec:
  containers:
    - name: user-service
      image: user-service:latest
      ports:
        - containerPort: 8080
```

**Flow:**

```
1. Pod starts ‚Üí Kubernetes adds to service
2. Service ‚Üí kube-proxy: Update load balancer
3. Requests to service ‚Üí Routed to pod
4. Pod terminates ‚Üí Kubernetes removes from service
```

#### ‚úÖ Step 4: Service Discovery Mechanism

**DNS-Based Discovery:**

**Internal DNS:**

```
Service Name: user-service
DNS: user-service.default.svc.cluster.local
Returns: Cluster IP (load balanced)
```

**Short Name:**

```
Service Name: user-service
DNS: user-service (in same namespace)
Returns: Cluster IP
```

**Client Usage:**

```python
# HTTP Service
import requests
response = requests.get('http://user-service/api/users')
# Kubernetes DNS resolves to service IP
# kube-proxy load balances to pod

# gRPC Service
import grpc
channel = grpc.insecure_channel('user-service:50051')
# Kubernetes DNS resolves to service IP
# kube-proxy load balances to pod
```

**Why DNS:**

- **Language Agnostic:** Works with any language
- **Standard:** Uses standard DNS protocol
- **Cached:** DNS responses cached (low latency)
- **Simple:** No special libraries needed

#### ‚úÖ Step 5: Load Balancing

**kube-proxy Load Balancing:**

**Modes:**

**A. iptables Mode (Default):**

- Uses Linux iptables rules
- Load balances at kernel level (fast)
- Round-robin distribution

**B. IPVS Mode:**

- Uses Linux IPVS (IP Virtual Server)
- More load balancing algorithms (round-robin, least-connections, etc.)
- Better performance for large number of services

**Configuration:**

```yaml
# kube-proxy configuration
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
mode: "ipvs"  # Use IPVS mode
ipvs:
  scheduler: "rr"  # Round-robin
```

**Load Balancing Flow:**

```
Request ‚Üí Service IP ‚Üí kube-proxy ‚Üí Select Pod (round-robin)
‚Üí Pod 1, Pod 2, Pod 3 (rotated)
```

#### ‚úÖ Step 6: Health Monitoring

**Kubernetes Health Checks:**

**A. Liveness Probe:**

- Determines if pod is alive
- If fails, Kubernetes restarts pod

**B. Readiness Probe:**

- Determines if pod is ready to serve traffic
- If fails, Kubernetes removes pod from service (stops routing traffic)

**Configuration:**

```yaml
apiVersion: v1
kind: Pod
spec:
  containers:
    - name: user-service
      livenessProbe:
        httpGet:
          path: /health/live
          port: 8080
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 5
        failureThreshold: 3

      readinessProbe:
        httpGet:
          path: /health/ready
          port: 8080
        initialDelaySeconds: 5
        periodSeconds: 5
        timeoutSeconds: 3
        failureThreshold: 3
```

**Flow:**

```
1. Pod starts ‚Üí Readiness probe runs
2. If ready ‚Üí Pod added to service (traffic routed)
3. If not ready ‚Üí Pod not in service (no traffic)
4. Liveness probe runs periodically
5. If fails ‚Üí Pod restarted
6. If pod crashes ‚Üí Removed from service automatically
```

#### ‚úÖ Step 7: Handling Service Scaling

**Horizontal Pod Autoscaling (HPA):**

**Automatic Scaling:**

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: user-service-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: user-service
  minReplicas: 3
  maxReplicas: 10
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
```

**Flow:**

```
1. CPU usage > 70% ‚Üí HPA adds pod
2. New pod starts ‚Üí Automatically added to service
3. kube-proxy ‚Üí Updates load balancer
4. Traffic ‚Üí Distributed to new pod
5. CPU usage < 70% ‚Üí HPA removes pod
6. Pod terminates ‚Üí Automatically removed from service
```

**Result:**

- Services scale automatically
- New instances automatically discoverable
- No manual configuration needed

#### ‚úÖ Step 8: Multi-Region Service Discovery

**Problem:** Services in different regions (US, Europe, Asia)

**Solution: Kubernetes Federation or External Service Discovery**

**Option A: Kubernetes Federation (Complex)**

- Federate multiple clusters
- Services accessible across clusters

**Option B: External Service Discovery (Consul)**

- Consul in each region
- Services register with regional Consul
- Cross-region service discovery via Consul WAN

**Chosen: Regional Kubernetes + Consul for Cross-Region**

**Architecture:**

```
Region: US
  ‚îî‚îÄ Kubernetes Cluster (local service discovery)

Region: Europe
  ‚îî‚îÄ Kubernetes Cluster (local service discovery)

Region: Asia
  ‚îî‚îÄ Kubernetes Cluster (local service discovery)

Consul (Cross-Region):
  ‚îî‚îÄ Services register for cross-region discovery
```

#### ‚úÖ Step 9: Performance Optimization

**DNS Caching:**

- Client libraries cache DNS responses
- Reduces DNS lookup latency
- TTL: 30-60 seconds (balance freshness vs performance)

**Connection Pooling:**

- Reuse HTTP/gRPC connections
- Reduces connection establishment overhead

**Service Mesh (Optional):**

- Istio/Linkerd for advanced features
- Automatic retries, circuit breakers
- More overhead but better resilience

#### ‚úÖ Complete Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              Kubernetes Cluster                          ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ User Service ‚îÇ  ‚îÇ Order Service‚îÇ  ‚îÇPayment Service‚îÇ ‚îÇ
‚îÇ  ‚îÇ   Pod 1      ‚îÇ  ‚îÇ   Pod 1      ‚îÇ  ‚îÇ   Pod 1      ‚îÇ ‚îÇ
‚îÇ  ‚îÇ   Pod 2      ‚îÇ  ‚îÇ   Pod 2      ‚îÇ  ‚îÇ   Pod 2      ‚îÇ ‚îÇ
‚îÇ  ‚îÇ   Pod 3      ‚îÇ  ‚îÇ   Pod 3      ‚îÇ  ‚îÇ   Pod 3      ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ         ‚îÇ                  ‚îÇ                  ‚îÇ          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ   Service   ‚îÇ  ‚îÇ   Service    ‚îÇ  ‚îÇ   Service   ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  (DNS)      ‚îÇ  ‚îÇ   (DNS)      ‚îÇ  ‚îÇ   (DNS)      ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ         ‚îÇ                  ‚îÇ                  ‚îÇ          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ            kube-proxy (Load Balancer)              ‚îÇ ‚îÇ
‚îÇ  ‚îÇ            (iptables/IPVS)                         ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ         Kubernetes API Server                      ‚îÇ ‚îÇ
‚îÇ  ‚îÇ         (Service Registry)                         ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Service Discovery Flow:**

```
1. Service A needs to call Service B
2. Service A ‚Üí DNS: Query "service-b"
3. DNS ‚Üí Returns: Service B Cluster IP
4. Service A ‚Üí Service B Cluster IP: HTTP/gRPC request
5. kube-proxy ‚Üí Load balances to healthy pod
6. Pod ‚Üí Processes request ‚Üí Returns response
```

**Health Check Flow:**

```
1. Kubernetes ‚Üí Pod: Readiness probe every 5 seconds
2. If healthy ‚Üí Pod in service (traffic routed)
3. If unhealthy ‚Üí Pod removed from service (no traffic)
4. Kubernetes ‚Üí Pod: Liveness probe every 10 seconds
5. If dead ‚Üí Pod restarted
```

#### ‚öñÔ∏è Trade-offs Summary

| Decision                   | What We Gain                      | What We Sacrifice                    |
|----------------------------|-----------------------------------|--------------------------------------|
| **Kubernetes Service**     | Built-in, no extra infrastructure | Kubernetes-specific (vendor lock-in) |
| **DNS-Based Discovery**    | Language agnostic, simple         | DNS caching adds slight staleness    |
| **Server-Side Discovery**  | Client simplicity                 | Extra hop through kube-proxy         |
| **Automatic Registration** | Zero configuration                | Less control over registration       |
| **Health Checks**          | Automatic failure handling        | Requires health endpoints            |

#### ‚úÖ Final Summary

**Service Discovery Strategy:**

- **Pattern:** Server-Side Discovery
- **Implementation:** Kubernetes Service (built-in)
- **Mechanism:** DNS-based (user-service.default.svc.cluster.local)
- **Load Balancing:** kube-proxy (iptables/IPVS)
- **Health Monitoring:** Kubernetes readiness/liveness probes

**Performance:**

- **Discovery Latency:** <1ms (DNS lookup, cached)
- **Load Balancing:** <1ms (kernel-level, iptables)
- **Total Overhead:** <2ms (meets <10ms requirement)

**Availability:**

- **Kubernetes API Server:** 99.99% (highly available cluster)
- **kube-proxy:** Runs on every node (no single point of failure)
- **DNS:** Kubernetes DNS (CoreDNS) highly available

**Scalability:**

- **Services:** 50 services (handles easily)
- **Instances:** 250-500 instances (Kubernetes handles thousands)
- **Dynamic Scaling:** Automatic (HPA adds/removes pods)

**Result:**

- ‚úÖ Dynamic service discovery (no hard-coded IPs)
- ‚úÖ Automatic load balancing (kube-proxy)
- ‚úÖ Health monitoring (readiness/liveness probes)
- ‚úÖ Low latency (<2ms overhead)
- ‚úÖ High availability (99.99%)
- ‚úÖ Handles dynamic scaling automatically

