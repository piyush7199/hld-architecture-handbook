# 1.1.5 Back-of-the-Envelope Calculations: The System Designer's Math

## Intuitive Explanation

Back-of-the-envelope calculations are quick mental math exercises that help you estimate system capacity, costs, and constraints during system design. Think of it as the "napkin math" engineers use to validate whether an architecture can handle the expected load.

- **The Goal:** Quickly determine if your design is feasible before building it.
- **The Method:** Use approximations and standard numbers to calculate storage, bandwidth, QPS, and memory requirements.
- **Why It Matters:** In interviews and real-world design, you must prove your system won't collapse under load.

---

## In-Depth Analysis

### 1. Latency Numbers Every Programmer Should Know

These are the fundamental building blocks for estimating system performance:

| Operation                           | Latency      | Relative Scale              |
|-------------------------------------|--------------|----------------------------|
| L1 cache reference                  | 0.5 ns       | -                          |
| Branch mispredict                   | 5 ns         | -                          |
| L2 cache reference                  | 7 ns         | 14√ó L1 cache               |
| Mutex lock/unlock                   | 100 ns       | -                          |
| Main memory reference               | 100 ns       | 20√ó L2 cache, 200√ó L1      |
| Compress 1KB with Snappy            | 10 Œºs        | -                          |
| Send 2KB over 1 Gbps network        | 20 Œºs        | -                          |
| Read 1 MB sequentially from memory  | 250 Œºs       | -                          |
| Round trip within same datacenter   | 500 Œºs       | -                          |
| Disk seek (HDD)                     | 10 ms        | 20√ó datacenter roundtrip   |
| Read 1 MB sequentially from SSD     | 1 ms         | 4√ó memory, 10√ó SSD random  |
| Read 1 MB sequentially from disk    | 30 ms        | 30√ó SSD                    |
| Send packet CA ‚Üí Netherlands ‚Üí CA   | 150 ms       | 300√ó same datacenter       |

**Key Takeaways:**
- Memory is ~100,000√ó faster than disk
- Network within datacenter is ~500√ó faster than cross-continent
- Sequential disk reads are ~10√ó faster than random seeks

### 2. Power of Two Table (Memory & Storage)

| Power | Exact Value     | Approximate | Short Name |
|-------|-----------------|-------------|-----------|
| 10    | 1,024           | 1 thousand  | 1 KB      |
| 16    | 65,536          | 64 KB       | -         |
| 20    | 1,048,576       | 1 million   | 1 MB      |
| 30    | 1,073,741,824   | 1 billion   | 1 GB      |
| 32    | 4,294,967,296   | 4 GB        | -         |
| 40    | 1,099,511,627,776 | 1 trillion | 1 TB      |

### 3. Common Estimation Framework

When designing a system, follow this systematic approach:

#### Step 1: Clarify Constraints
- **Users:** How many Daily Active Users (DAU)? Monthly Active Users (MAU)?
- **Usage:** How many requests per user per day?
- **Data:** How much data per request/user?
- **Time:** What's the retention period? (days, months, years)

#### Step 2: Calculate Traffic (QPS)

**Formula:**
```
QPS (Queries Per Second) = Total Operations per Day / 86,400 seconds

Peak QPS ‚âà 2√ó Average QPS  (rule of thumb)
```

**Example:**
- 100 million DAU
- Each user makes 10 requests per day
- Total = 1 billion requests/day

```
Average QPS = 1,000,000,000 / 86,400 ‚âà 11,574 QPS
Peak QPS ‚âà 23,000 QPS
```

#### Step 3: Calculate Storage

**Formula:**
```
Total Storage = Operations per Day √ó Data Size √ó Retention Days
```

**Example:**
- 1 billion operations/day
- Each operation stores 1 KB
- 5-year retention

```
Storage = 1B √ó 1 KB √ó (365 √ó 5)
        = 1B √ó 1 KB √ó 1,825 days
        = 1.825 TB √ó 1,000
        = 1,825 TB ‚âà 1.8 PB
```

#### Step 4: Calculate Bandwidth

**Formula:**
```
Bandwidth (Ingress) = QPS √ó Request Size
Bandwidth (Egress) = QPS √ó Response Size
```

**Example:**
- 10,000 QPS
- Average request: 1 KB
- Average response: 10 KB

```
Ingress = 10,000 √ó 1 KB = 10 MB/s = 80 Mbps
Egress = 10,000 √ó 10 KB = 100 MB/s = 800 Mbps
```

#### Step 5: Calculate Memory (Cache)

**Common Rule: 80/20 Principle**
- 20% of data generates 80% of traffic
- Cache the hottest 20% of data

**Formula:**
```
Cache Size = Total Data √ó 0.20
```

**Example:**
- Total data: 1 TB
- Cache 20%

```
Cache Size = 1 TB √ó 0.20 = 200 GB
```

### 4. Real-World Estimation Examples

#### Example 1: Twitter Timeline Service

**Given:**
- 200M DAU
- Each user views timeline 10 times/day
- Timeline has 100 tweets
- Each tweet is 280 chars + metadata ‚âà 500 bytes

**Calculate Read QPS:**
```
Total reads = 200M √ó 10 = 2B reads/day
QPS = 2B / 86,400 ‚âà 23,148 QPS
Peak QPS ‚âà 46,000 QPS
```

**Calculate Egress Bandwidth:**
```
Each timeline = 100 tweets √ó 500 bytes = 50 KB
Bandwidth = 23,148 QPS √ó 50 KB ‚âà 1.16 GB/s ‚âà 9.3 Gbps
```

#### Example 2: Video Streaming (YouTube-like)

**Given:**
- 1B DAU
- Average: 30 minutes watch time per day
- Average bitrate: 2 Mbps (720p)

**Calculate Egress Bandwidth:**
```
Total watch time = 1B √ó 30 min = 30B minutes/day
                 = 30B √ó 60 sec = 1,800B seconds/day
                 
Average concurrent users = 1,800B sec / 86,400 sec = 20.8M

Bandwidth = 20.8M √ó 2 Mbps = 41.6 Pbps (Petabits/sec)
```

**Calculate Storage (30-day retention):**
```
Videos uploaded = 500k videos/day
Average video size = 100 MB
Retention = 30 days

Storage = 500k √ó 100 MB √ó 30 = 1.5 PB
```

### 5. Common Estimation Shortcuts

| Metric | Shortcut |
|--------|----------|
| **Seconds in a day** | ~100,000 (actually 86,400) |
| **Requests per day to QPS** | Divide by 100,000 |
| **Peak traffic** | 2√ó average |
| **Text message size** | 100 bytes |
| **Image (compressed)** | 200 KB |
| **Video (1 min, 720p)** | 10 MB |
| **Database row** | 1 KB |
| **Cache hit rate** | 80-90% |

### 6. Capacity Planning Anti-Patterns

| Anti-Pattern | Why It's Wrong | Better Approach |
|--------------|----------------|-----------------|
| **Exact precision** | "We need 11,574.07 QPS" | Round to meaningful numbers: "~12K QPS" |
| **Ignoring peaks** | Only planning for average load | Plan for 2-3√ó average, or use actual peak metrics |
| **No growth buffer** | Designing exactly for current load | Add 50-100% headroom for growth |
| **Forgetting replication** | Calculating single-copy storage | Multiply by replication factor (usually 3√ó) |
| **Ignoring overhead** | Using theoretical limits | Account for protocol overhead (~20-30%) |

---

## üí° Real-World Use Cases

- **Interview Scenarios:** Estimating if a proposed architecture can handle stated scale before diving into detailed design.
- **Capacity Planning:** Determining how many servers, databases, or cache nodes you need.
- **Cost Estimation:** Calculating cloud infrastructure costs before deployment.
- **Bottleneck Identification:** Finding where your system will break first (network? storage? CPU?).

---

## ‚úèÔ∏è Design Challenge

### Problem

You are designing a photo-sharing service (like Instagram). Estimate the following:

**Given:**
- 500 Million DAU
- Each user uploads 2 photos per day on average
- Each user views 100 photos per day
- Average photo size: 2 MB (original), 200 KB (compressed for feed)
- Data retention: 10 years
- Read:Write ratio: 100:1

**Calculate:**
1. Storage requirement for 10 years
2. Write QPS (uploads)
3. Read QPS (views)
4. Bandwidth (Ingress and Egress)
5. Cache size (if caching 20% of hot photos)

### Solution

#### üß© Given Data Summary

| Metric | Value |
|--------|-------|
| DAU | 500M |
| Uploads/user/day | 2 |
| Views/user/day | 100 |
| Original photo size | 2 MB |
| Compressed photo | 200 KB |
| Retention | 10 years |
| Read:Write | 100:1 |

#### ‚úÖ Step 1: Calculate Storage (10 Years)

**Daily uploads:**
```
Uploads/day = 500M √ó 2 = 1 billion photos/day
```

**Storage per day:**
```
We store both original and compressed:
Original: 1B √ó 2 MB = 2 PB/day
Compressed: 1B √ó 200 KB = 200 TB/day

Total/day = 2 PB + 200 TB ‚âà 2.2 PB/day
```

**10-year storage (without compression):**
```
Total = 2.2 PB/day √ó 365 days √ó 10 years
      = 2.2 PB √ó 3,650
      = 8,030 PB ‚âà 8 Exabytes (EB)
```

**With 3√ó replication:**
```
Total = 8 EB √ó 3 = 24 EB
```

#### ‚úÖ Step 2: Calculate Write QPS (Uploads)

```
Write QPS = 1B uploads / 86,400 seconds
          ‚âà 11,574 QPS

Peak Write QPS ‚âà 23,000 QPS
```

#### ‚úÖ Step 3: Calculate Read QPS (Views)

```
Total reads = 500M √ó 100 = 50B views/day

Read QPS = 50B / 86,400
         ‚âà 578,703 QPS
         ‚âà 580K QPS

Peak Read QPS ‚âà 1.16M QPS
```

**Validation:**
```
Read:Write ratio = 580K / 11.5K ‚âà 50:1
(Note: Lower than stated 100:1 because not all photos get equal views)
```

#### ‚úÖ Step 4: Calculate Bandwidth

**Ingress (Uploads):**
```
Bandwidth = Write QPS √ó Photo Size
          = 11,574 √ó 2 MB
          = 23,148 MB/s
          ‚âà 23 GB/s
          ‚âà 185 Gbps
```

**Egress (Views):**
```
Bandwidth = Read QPS √ó Compressed Photo
          = 578,703 √ó 200 KB
          = 115,740,600 KB/s
          ‚âà 116 GB/s
          ‚âà 928 Gbps ‚âà ~1 Tbps
```

#### ‚úÖ Step 5: Calculate Cache Size (20% Hot Data)

**Assumption:** Cache 20% of most-viewed photos from last 7 days

```
Photos in last 7 days = 1B/day √ó 7 = 7B photos

Total size (compressed) = 7B √ó 200 KB = 1.4 PB

Cache 20% = 1.4 PB √ó 0.20 = 280 TB
```

#### ‚úÖ Final Summary

| Metric | Value | Notes |
|--------|-------|-------|
| **Storage (10 years)** | 24 EB | Including 3√ó replication |
| **Write QPS** | ~12K QPS (peak: 24K) | Photo uploads |
| **Read QPS** | ~580K QPS (peak: 1.16M) | Photo views |
| **Ingress Bandwidth** | ~185 Gbps | Upload traffic |
| **Egress Bandwidth** | ~1 Tbps | Download traffic |
| **Cache Size** | ~280 TB | Hot 20% of last 7 days |

#### üéØ Key Insights

1. **Storage is massive** - 8 EB base, 24 EB with replication
2. **Read-heavy** - 50√ó more reads than writes
3. **Bandwidth bottleneck** - Egress is the primary concern (~1 Tbps)
4. **CDN is critical** - Must use CDN to offload egress bandwidth
5. **Caching strategy** - Even 20% cache is 280 TB, needs distributed Redis cluster

#### üöÄ Design Implications

- **Storage:** Use object storage (S3, GCS) with tiered archival (hot/cold storage)
- **CDN:** Mandatory for serving photos, can offload 90%+ of egress
- **Database:** Metadata only (photo URLs, user info) - NoSQL for horizontal scaling
- **Cache:** Multi-layer (L1: Edge CDN, L2: Regional CDN, L3: Origin cache)
- **Upload:** Direct to S3 with pre-signed URLs to bypass application servers

