# 1.1.9 Replication Strategies: Data Redundancy for Availability and Performance

## Intuitive Explanation

Imagine you're running a library with a single copy of a popular book. If that copy is checked out, damaged, or lost, no one can read it.

**Replication** is like making multiple copies of the book and placing them in different sections of the library:
- If one copy is unavailable, others are still accessible
- Multiple readers can access different copies simultaneously (better performance)
- If one copy is damaged, others serve as backups

**Data Replication** in distributed systems works the same way: creating multiple copies of data across different machines (nodes) to ensure:
- **Availability:** System continues working if one node fails
- **Performance:** Distribute read load across multiple nodes
- **Durability:** Data survives hardware failures

---

## In-Depth Analysis

### Definition

**Replication** is the process of maintaining multiple copies of the same data across different nodes (servers) in a distributed system.

**Key Goals:**
- **High Availability:** System remains operational when nodes fail
- **Fault Tolerance:** Data survives hardware failures
- **Performance:** Distribute read load across replicas
- **Geographic Distribution:** Serve data from locations closer to users

### Why Replicate?

**Single Node Problems:**
- **Single Point of Failure:** Node failure = complete system failure
- **Limited Capacity:** Single node can't handle unlimited load
- **Geographic Latency:** Users far from node experience high latency
- **Maintenance:** Taking node offline for maintenance = downtime

**Replication Benefits:**
- **Fault Tolerance:** N replicas can tolerate N-1 failures
- **Load Distribution:** Read traffic distributed across replicas
- **Low Latency:** Serve data from nearest replica
- **Zero-Downtime Maintenance:** Update one replica while others serve traffic

---

## Key Concepts / Tradeoffs

### 1. Replication Topologies

#### A. Master-Slave (Primary-Replica)

**How It Works:**
- **Master (Primary):** Accepts all writes, replicates to slaves
- **Slaves (Replicas):** Receive writes from master, serve reads only

**Flow:**
```
Write:
Client ‚Üí Master ‚Üí Write to Master
              ‚Üí Replicate to Slaves (async or sync)

Read:
Client ‚Üí Slave (read-only, no writes)
```

**Pros:**
- **Simple:** Clear separation of read/write
- **Read Scaling:** Add more slaves for read capacity
- **Backup:** Slaves serve as backups
- **Low Latency Reads:** Serve reads from nearest slave

**Cons:**
- **Single Write Point:** Master is bottleneck for writes
- **Replication Lag:** Slaves may have stale data (async replication)
- **Master Failure:** Requires failover (promote slave to master)
- **Write Availability:** Master down = no writes

**Use Cases:**
- Read-heavy workloads (90% reads, 10% writes)
- When eventual consistency is acceptable
- When read scaling is priority

**Examples:**
- MySQL Master-Slave replication
- PostgreSQL Streaming Replication
- MongoDB Replica Sets (with primary-secondary)

#### B. Master-Master (Multi-Master)

**How It Works:**
- **Multiple Masters:** All nodes accept reads and writes
- **Bidirectional Replication:** Writes replicate to all masters

**Flow:**
```
Write:
Client ‚Üí Master A ‚Üí Write to Master A
                  ‚Üí Replicate to Master B, C, D

Client ‚Üí Master B ‚Üí Write to Master B
                  ‚Üí Replicate to Master A, C, D
```

**Pros:**
- **No Single Point of Failure:** Any master can handle writes
- **Write Scaling:** Writes distributed across masters
- **Geographic Distribution:** Write to nearest master
- **High Availability:** Master failure doesn't stop writes

**Cons:**
- **Conflict Resolution:** Concurrent writes to different masters can conflict
- **Complexity:** Must handle write conflicts, merge logic
- **Consistency:** Eventual consistency (conflicts resolved later)
- **Synchronization:** All masters must sync (network overhead)

**Use Cases:**
- Multi-region deployments (write to nearest region)
- When write availability is critical
- When conflicts are rare or easily resolved

**Examples:**
- MySQL Multi-Master Replication
- Cassandra Multi-Datacenter (each DC has masters)
- DynamoDB Global Tables

#### C. Leader-Follower (Consensus-Based)

**How It Works:**
- **Leader:** Accepts writes, replicates via consensus protocol (Raft, Paxos)
- **Followers:** Receive writes via consensus, can be promoted to leader

**Flow:**
```
Write:
Client ‚Üí Leader ‚Üí Propose write
                ‚Üí Consensus (majority of followers agree)
                ‚Üí Commit write to all nodes
```

**Pros:**
- **Strong Consistency:** Consensus ensures all nodes agree
- **Automatic Failover:** Leader election promotes new leader
- **No Split-Brain:** Consensus prevents conflicting writes
- **Durability:** Majority must agree (survives minority failures)

**Cons:**
- **Write Latency:** Must wait for majority consensus
- **Write Throughput:** Limited by consensus overhead
- **Quorum Requirement:** Need majority of nodes available

**Use Cases:**
- When strong consistency is required
- Distributed databases (CockroachDB, TiDB)
- Configuration stores (etcd, Consul)

**Examples:**
- etcd (Raft consensus)
- CockroachDB (Raft for ranges)
- Consul (Raft for leader election)

### 2. Replication Synchronization

#### A. Synchronous Replication

**How It Works:**
- Master waits for all replicas to acknowledge write before confirming to client
- All replicas must have data before write is considered successful

**Flow:**
```
Client ‚Üí Master ‚Üí Write to Master
              ‚Üí Replicate to Replica 1 (wait for ack)
              ‚Üí Replicate to Replica 2 (wait for ack)
              ‚Üí All acks received ‚Üí Confirm to client
```

**Pros:**
- **Strong Consistency:** All replicas have same data
- **No Data Loss:** Write confirmed only after all replicas have it
- **Simple:** No replication lag, no stale reads

**Cons:**
- **High Latency:** Must wait for all replicas (slowest replica determines latency)
- **Low Availability:** One replica down = no writes
- **Limited Scalability:** Can't add many replicas (latency increases)

**Use Cases:**
- Financial systems (can't lose data)
- When consistency is more important than availability
- Small number of replicas (2-3)

**Examples:**
- PostgreSQL Synchronous Replication
- MySQL Semi-Synchronous Replication

#### B. Asynchronous Replication

**How It Works:**
- Master confirms write to client immediately
- Replication happens in background (fire-and-forget)

**Flow:**
```
Client ‚Üí Master ‚Üí Write to Master
              ‚Üí Confirm to client (immediately)
              ‚Üí Replicate to Replicas (async, background)
```

**Pros:**
- **Low Latency:** Client doesn't wait for replication
- **High Throughput:** Writes not blocked by replication
- **Scalability:** Can add many replicas (no latency impact)
- **Availability:** Replica failure doesn't block writes

**Cons:**
- **Replication Lag:** Replicas may have stale data (seconds to minutes)
- **Data Loss Risk:** Master failure before replication = lost data
- **Eventual Consistency:** Reads from replicas may be stale

**Use Cases:**
- Read-heavy workloads (stale reads acceptable)
- When availability > consistency
- When replication lag is acceptable (seconds)

**Examples:**
- MySQL Asynchronous Replication
- PostgreSQL Asynchronous Streaming Replication
- MongoDB Replica Sets (default)

#### C. Semi-Synchronous Replication

**How It Works:**
- Master waits for at least one replica to acknowledge (not all)
- Balance between consistency and latency

**Flow:**
```
Client ‚Üí Master ‚Üí Write to Master
              ‚Üí Replicate to Replica 1 (wait for ack)
              ‚Üí Replicate to Replica 2 (async, don't wait)
              ‚Üí At least 1 ack received ‚Üí Confirm to client
```

**Pros:**
- **Better Consistency:** At least one replica has data
- **Lower Latency:** Don't wait for all replicas
- **Reduced Data Loss:** One replica failure doesn't lose data

**Cons:**
- **Still Some Lag:** Other replicas may be stale
- **Complexity:** More complex than pure sync/async

**Use Cases:**
- When you need balance between consistency and performance
- When one replica is enough for durability

**Examples:**
- MySQL Semi-Synchronous Replication

### 3. Replication Factor

**Definition:** Number of copies of data maintained in the system.

**Common Configurations:**

| Replication Factor | Tolerable Failures | Use Case |
|--------------------|-------------------|----------|
| **RF = 1** | 0 (no redundancy) | Development, non-critical data |
| **RF = 2** | 1 failure | Low availability requirement |
| **RF = 3** | 2 failures | Standard production (most common) |
| **RF = 5** | 4 failures | High availability, multi-region |

**Trade-offs:**
- **Higher RF:** More availability, more storage cost, more network traffic
- **Lower RF:** Less availability, less storage cost, less network traffic

**Example Calculation:**
```
Original Data: 1 TB
Replication Factor: 3
Total Storage: 1 TB √ó 3 = 3 TB
Tolerable Failures: 2 nodes can fail
```

### 4. Read Replicas

**Purpose:** Distribute read load across multiple replicas

**Strategies:**

**A. Round-Robin:**
- Distribute reads evenly across all replicas
- Simple, even load distribution

**B. Geographic Routing:**
- Route reads to nearest replica
- Reduces latency for global users

**C. Replica Lag Awareness:**
- Route reads to replicas with lowest lag
- Ensures freshest data

**D. Read Preference:**
- **Primary:** Read from master (strongest consistency)
- **Secondary:** Read from replicas (may be stale)
- **Nearest:** Read from closest replica (lowest latency)

---

## üí° Real-World Use Cases

### Amazon RDS (MySQL/PostgreSQL)

**Replication Strategy:** Master-Slave with Asynchronous Replication

**Configuration:**
- **Master:** Handles all writes
- **Read Replicas:** 0-15 replicas for read scaling
- **Replication:** Asynchronous (low latency writes)
- **Failover:** Automatic promotion of replica to master

**Use Case:**
- E-commerce (read-heavy: product catalog, user profiles)
- Read replicas in multiple regions (low latency)

### Google Spanner

**Replication Strategy:** Multi-Master with Synchronous Replication

**Configuration:**
- **Multiple Masters:** Across regions
- **Replication:** Synchronous (strong consistency)
- **Consensus:** TrueTime + Paxos for global consistency

**Use Case:**
- Financial systems (strong consistency required)
- Global scale (millions of transactions)

### Netflix (Cassandra)

**Replication Strategy:** Multi-Master with Asynchronous Replication

**Configuration:**
- **Multiple Masters:** Per datacenter
- **Replication:** Asynchronous between datacenters
- **Replication Factor:** 3 per datacenter

**Use Case:**
- Video metadata (high availability, eventual consistency acceptable)
- Multi-region (write to nearest region)

### GitHub (MySQL)

**Replication Strategy:** Master-Slave with Semi-Synchronous Replication

**Configuration:**
- **Master:** Primary datacenter
- **Read Replicas:** Multiple replicas per region
- **Replication:** Semi-synchronous (balance consistency/performance)

**Use Case:**
- Code repository (read-heavy, need durability)
- Multi-region (read from nearest region)

---

## ‚úèÔ∏è Design Challenge

### Problem

You are designing a user profile database for a social media platform with 500 million users. The system must handle:

**Requirements:**
1. **Write Load:** 10,000 profile updates per second
2. **Read Load:** 100,000 profile reads per second (10:1 read/write ratio)
3. **Availability:** 99.99% uptime (52 minutes downtime/year)
4. **Consistency:** Users should see their own updates immediately (read-your-writes)
5. **Geographic:** Users in US, Europe, and Asia (3 regions)

**Constraints:**
- Single database server: 5,000 writes/sec, 50,000 reads/sec
- Network latency: US ‚Üî Europe = 100ms, US ‚Üî Asia = 200ms
- Budget: Minimize infrastructure cost

Design a replication strategy that:
- Handles the read/write load
- Meets availability requirements
- Ensures read-your-writes consistency
- Minimizes latency for global users
- Handles failures gracefully

### Solution

#### üß© Scenario

- **Users:** 500 million
- **Write Load:** 10,000 updates/sec
- **Read Load:** 100,000 reads/sec
- **Regions:** US, Europe, Asia
- **Availability:** 99.99% (52 minutes/year downtime)
- **Consistency:** Read-your-writes required

**Calculations:**
- **Writes:** 10,000/sec √∑ 5,000/sec per server = 2 servers minimum
- **Reads:** 100,000/sec √∑ 50,000/sec per server = 2 servers minimum
- **Per Region:** Need 1 master + 2 read replicas = 3 servers per region
- **Total:** 3 regions √ó 3 servers = 9 servers

#### ‚úÖ Step 1: Replication Topology

**Choice: Master-Slave with Read Replicas**

**Why:**
- **Read-Heavy:** 10:1 read/write ratio (perfect for read replicas)
- **Simple:** Clear separation of read/write
- **Cost-Effective:** Read replicas are cheaper (can use smaller instances)
- **Geographic:** Read replicas in each region (low latency)

**Architecture:**
```
Region: US
  ‚îú‚îÄ Master (Primary) - Handles writes
  ‚îú‚îÄ Read Replica 1 - Handles reads
  ‚îî‚îÄ Read Replica 2 - Handles reads

Region: Europe
  ‚îú‚îÄ Read Replica 1 - Handles reads
  ‚îî‚îÄ Read Replica 2 - Handles reads

Region: Asia
  ‚îú‚îÄ Read Replica 1 - Handles reads
  ‚îî‚îÄ Read Replica 2 - Handles reads
```

#### ‚úÖ Step 2: Replication Synchronization

**Choice: Asynchronous Replication**

**Why:**
- **Low Latency Writes:** Don't wait for replicas (10ms vs 200ms)
- **High Throughput:** Writes not blocked by replication
- **Scalability:** Can add many replicas without impacting writes
- **Trade-off:** Acceptable replication lag (seconds) for read-heavy workload

**Flow:**
```
Write (US Master):
Client ‚Üí US Master ‚Üí Write confirmed (10ms)
                  ‚Üí Replicate to replicas (async, background)
                  ‚Üí Europe replicas updated (~100ms later)
                  ‚Üí Asia replicas updated (~200ms later)
```

**Replication Lag:**
- **US Replicas:** <1 second (same region)
- **Europe Replicas:** 1-2 seconds (cross-region)
- **Asia Replicas:** 2-3 seconds (cross-region)

#### ‚úÖ Step 3: Read-Your-Writes Consistency

**Problem:** User updates profile in US, reads from Asia replica ‚Üí sees stale data

**Solution: Sticky Sessions + Replica Selection**

**Strategy A: Route User's Writes and Reads to Same Region**
```
User in US:
  ‚Üí Writes go to US Master
  ‚Üí Reads go to US Replicas (same region, <1s lag)
  ‚Üí Read-your-writes guaranteed (same region)
```

**Strategy B: Read from Master for Recent Writes**
```
If user wrote in last 5 seconds:
  ‚Üí Read from Master (guaranteed fresh)
Else:
  ‚Üí Read from Replica (acceptable staleness)
```

**Chosen: Strategy A (Geographic Routing)**

**Why:**
- **Simple:** Route based on user's region
- **Low Latency:** Reads from same region (<10ms)
- **Consistency:** Same region replicas have <1s lag
- **Trade-off:** Users traveling between regions may see slight staleness

**Implementation:**
```
1. User's region determined by IP geolocation
2. Writes: Route to region's master (or nearest master)
3. Reads: Route to region's replicas
4. Session sticky: Keep user on same region for session
```

#### ‚úÖ Step 4: Handling Failures

**Master Failure:**

**Scenario:** US Master fails

**Solution: Automatic Failover**

```
1. Health check detects master failure
2. Promote US Replica 1 to Master (automatic)
3. Update DNS/routing (30 seconds)
4. Replication continues from new master
```

**Replica Failure:**

**Scenario:** Europe Replica 1 fails

**Solution: Automatic Removal**

```
1. Health check detects replica failure
2. Remove from read pool (automatic)
3. Traffic routes to Europe Replica 2
4. Replica 1 replaced/restored (background)
```

**Availability Calculation:**
```
Single Server Availability: 99.9% (8.76 hours/year downtime)
With 3 Replicas (Master + 2 Replicas):
  ‚Üí Master failure: 1/3 chance, 30s failover
  ‚Üí Replica failure: 2/3 chance, 0s impact (other replicas handle)
  ‚Üí Total downtime: ~10 minutes/year
  ‚Üí Availability: 99.998% (exceeds 99.99% requirement)
```

#### ‚úÖ Step 5: Load Distribution

**Write Load:**
- **US Master:** 10,000 writes/sec (within 5,000/sec limit? No!)
- **Solution:** Need 2 masters or optimize writes

**Options:**
1. **Shard by user_id:** Split users across 2 masters
2. **Optimize:** Batch writes, reduce write load
3. **Accept:** 10,000/sec is 2x limit, but with optimization may work

**Read Load:**
- **Per Replica:** 100,000/sec √∑ 6 replicas = ~16,667 reads/sec per replica
- **Within Limit:** 16,667 < 50,000 (each replica handles load)

**Geographic Distribution:**
```
US Region:
  - Master: 10,000 writes/sec
  - Replica 1: 33,333 reads/sec
  - Replica 2: 33,333 reads/sec

Europe Region:
  - Replica 1: 33,333 reads/sec
  - Replica 2: 33,333 reads/sec

Asia Region:
  - Replica 1: 33,333 reads/sec
  - Replica 2: 33,333 reads/sec
```

#### ‚úÖ Step 6: Cost Optimization

**Server Sizing:**

**Master (Write-Heavy):**
- **Instance:** Large (8 vCPU, 32 GB RAM)
- **Cost:** $500/month
- **Capacity:** 5,000 writes/sec (with optimization)

**Read Replicas (Read-Heavy):**
- **Instance:** Medium (4 vCPU, 16 GB RAM)
- **Cost:** $250/month per replica
- **Capacity:** 50,000 reads/sec

**Total Cost:**
```
1 Master (US): $500/month
6 Read Replicas: 6 √ó $250 = $1,500/month
Total: $2,000/month
```

**Optimization:**
- Use smaller replicas for non-peak regions
- Use reserved instances (30% discount)
- **Optimized Cost:** ~$1,400/month

#### ‚úÖ Complete Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Global Load Balancer                    ‚îÇ
‚îÇ              (Route by User Region)                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ              ‚îÇ              ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ  US Region  ‚îÇ ‚îÇ  Europe   ‚îÇ ‚îÇ Asia Region‚îÇ
        ‚îÇ             ‚îÇ ‚îÇ  Region   ‚îÇ ‚îÇ            ‚îÇ
        ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ
        ‚îÇ  ‚îÇMaster  ‚îÇ ‚îÇ ‚îÇ ‚îÇReplica ‚îÇ‚îÇ ‚îÇ ‚îÇReplica ‚îÇ‚îÇ
        ‚îÇ  ‚îÇ(Writes)‚îÇ ‚îÇ ‚îÇ ‚îÇ(Reads) ‚îÇ‚îÇ ‚îÇ ‚îÇ(Reads) ‚îÇ‚îÇ
        ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ
        ‚îÇ      ‚îÇ      ‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ
        ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ‚îÇ ‚îÇReplica ‚îÇ‚îÇ ‚îÇ ‚îÇReplica ‚îÇ‚îÇ
        ‚îÇ  ‚îÇReplica ‚îÇ ‚îÇ ‚îÇ ‚îÇ(Reads) ‚îÇ‚îÇ ‚îÇ ‚îÇ(Reads) ‚îÇ‚îÇ
        ‚îÇ  ‚îÇ(Reads) ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ
        ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îÇ            ‚îÇ ‚îÇ            ‚îÇ
        ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ‚îÇ            ‚îÇ ‚îÇ            ‚îÇ
        ‚îÇ  ‚îÇReplica ‚îÇ ‚îÇ ‚îÇ            ‚îÇ ‚îÇ            ‚îÇ
        ‚îÇ  ‚îÇ(Reads) ‚îÇ ‚îÇ ‚îÇ            ‚îÇ ‚îÇ            ‚îÇ
        ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îÇ            ‚îÇ ‚îÇ            ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ              ‚îÇ              ‚îÇ
        (Async Replication)
```

**Query Flows:**

**1. Write Profile (US User):**
```
User (US) ‚Üí US Master ‚Üí Write confirmed (10ms)
                      ‚Üí Replicate to all replicas (async)
```

**2. Read Profile (US User):**
```
User (US) ‚Üí US Replica ‚Üí Read (5ms, <1s lag, read-your-writes)
```

**3. Read Profile (Europe User):**
```
User (Europe) ‚Üí Europe Replica ‚Üí Read (5ms, 1-2s lag)
```

**4. Master Failure:**
```
US Master fails ‚Üí Promote US Replica 1 to Master (30s)
               ‚Üí Update routing
               ‚Üí System continues (99.99% availability)
```

#### ‚öñÔ∏è Trade-offs Summary

| Decision | What We Gain | What We Sacrifice |
|----------|--------------|-------------------|
| **Master-Slave** | Simple, read scaling | Single write point, master bottleneck |
| **Asynchronous Replication** | Low latency, high throughput | Replication lag (1-3 seconds) |
| **Geographic Routing** | Low latency, read-your-writes | Slight staleness when users travel |
| **3 Replicas per Region** | 99.99% availability | Storage cost (3√ó data) |
| **Read Replicas** | 10√ó read capacity | Eventual consistency |

#### ‚úÖ Final Summary

**Replication Strategy:**
- **Topology:** Master-Slave with Read Replicas
- **Synchronization:** Asynchronous (low latency, high throughput)
- **Replication Factor:** 3 (1 master + 2 replicas per region)
- **Geographic:** 3 regions (US, Europe, Asia)

**Performance:**
- **Write Latency:** <10ms (US master)
- **Read Latency:** <10ms (regional replicas)
- **Replication Lag:** <1s (same region), 1-3s (cross-region)
- **Availability:** 99.99% (meets requirement)

**Scalability:**
- **Write Capacity:** 10,000/sec (may need optimization or sharding)
- **Read Capacity:** 300,000/sec (6 replicas √ó 50k/sec, exceeds 100k requirement)

**Result:**
- ‚úÖ Handles read/write load
- ‚úÖ Meets availability (99.99%)
- ‚úÖ Read-your-writes consistency (geographic routing)
- ‚úÖ Low latency for global users
- ‚úÖ Handles failures (automatic failover)

