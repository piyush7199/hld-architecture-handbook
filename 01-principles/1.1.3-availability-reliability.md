# 1.1.3 Availability and Reliability: The Uptime Guarantee

## Intuitive Explanation

Imagine you are ordering a pizza:

- **Reliability:** Can the delivery company actually deliver the pizza in a storm, 99 times out of 100? (The system
  consistently works as expected, even under stress.)
- **Availability:** Is the delivery company's phone number always reachable, even if they sometimes tell you the wait
  time is 3 hours? (The system is operational and accessible when requested.)

Reliability is about correctness over time; Availability is about accessibility at a given time.

---

## In-Depth Analysis

### Availability (The Nines)

Availability is measured as the percentage of time a system is fully operational. It is commonly referred to by the
number of "nines."

| Availability         | Downtime Per Year | Terminology                            |
|----------------------|-------------------|----------------------------------------|
| 99.0% (Two Nines)    | 3.65 days         | Unacceptable for critical services.    |
| 99.9% (Three Nines)  | ~8.76 hours       | Common goal for non-critical services. |
| 99.99% (Four Nines)  | ‚àº52.6 minutes     | High Availability (HA) standard.       |
| 99.999% (Five Nines) | ‚àº5.26 minutes     | Required for mission-critical systems. |

### Reliability and Fault Tolerance

- **Reliability:** The probability that a system will perform its intended function for a specified time under stated
  conditions.
- **Fault Tolerance:** The ability of a system to continue operating without interruption when one or more of its
  components fail.
- **Redundancy (N+1):** The core mechanism for achieving fault tolerance. It means having backup components ready to
  take over. An N+1 setup means you need N servers to handle peak load but have N+1 servers deployed.

### Key Concepts / Tradeoffs

| Concept                        | Explanation                                                                                                                             | Tradeoff                                                                                                                  |
|--------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------|
| Single Point of Failure (SPOF) | Any single component whose failure will cause the entire system or a critical section of it to stop working. Must be eliminated in HLD. | Eliminating SPOF requires duplicating hardware/software (redundancy), which increases cost and complexity.                |
| Mean Time To Repair (MTTR)     | The average time it takes to fix a failed component and bring it back online. High availability depends on low MTTR.                    | Prioritizing low MTTR requires significant investment in DevOps automation (auto-healing, rapid deployment).              |
| Multi-Region Deployment        | Deploying the entire system across multiple, geographically distant data centers (e.g., East Coast and West Coast).                     | Solves region-wide disasters (earthquakes, power grids) but adds significant latency to data replication between regions. |

___

## üí° Real-World Use Cases

- **Google Search (Five Nines):** Requires near-perfect availability; any downtime results in massive lost revenue and
  user impact. Achieved through massive redundancy and multi-region deployment.
- **Load Balancers:** Serve as a fault-tolerance mechanism by redirecting traffic away from unhealthy servers, ensuring
  the system remains available.
- **Database Replication:** A critical reliability technique where data is copied to multiple slave/follower nodes. If
  the primary node fails, a replica can be promoted to master.

___

## ‚úèÔ∏è Design Challenge

### Problem

Your product manager demands "Five Nines" (99.999%) availability for your new user authentication service. Explain why
this is almost impossible to guarantee and detail two architecture components you would prioritize to get as close as
possible.

### Solution

#### üö´ Why 99.999% Availability Is Almost Impossible to Guarantee

That‚Äôs extremely strict. Achieving this in practice is nearly impossible because:

**1. Dependencies Fail:**

- Your auth service depends on external components ‚Äî databases, caches, DNS, cloud providers, or network routing.
- Even if each subsystem is 99.99% available, combined system availability is the product of all components ‚Üí real
  uptime drops quickly.

**2. Software & Human Errors:**

- Code deployments, configuration mistakes, and schema changes cause brief but real outages.
- Human intervention (rollbacks, patching, restarts) is unavoidable.

**3. Network & Infrastructure Limits:**

- Even cloud regions have outages (AWS, GCP, Azure).
- Latency spikes, DDoS, or load balancer issues can make your service effectively unavailable.

**4. Testing & Maintenance Windows:**

- You can‚Äôt patch, upgrade, or test continuously with zero downtime forever.
- Planned maintenance still affects availability.

In short:
99.999% availability requires perfect hardware, perfect software, perfect people, and perfect networks ‚Äî which don‚Äôt
exist.

#### 1Ô∏è‚É£ Multi-Region Active-Active Deployment

**Goal:** Eliminate single points of failure and tolerate regional outages.

- Deploy the authentication service in multiple independent regions (e.g., AWS us-east-1 + eu-west-1).
- Use global load balancing (Route 53, Cloudflare, GSLB) to route requests to the nearest healthy region.
- Keep user sessions and tokens replicated asynchronously across regions via:
    - Global datastore (e.g., DynamoDB Global Tables, CockroachDB, Spanner)
    - Or stateless JWT tokens that remove the need for shared state.

**‚úÖ Benefits:**

- Survives entire region failures.
- Enables zero-downtime failover.
- Low latency for global users.

**‚ö†Ô∏è Challenge:**

- Multi-region replication consistency and token invalidation can be tricky.

#### 2Ô∏è‚É£ Stateless, Redundant, and Self-Healing Infrastructure

**Goal:** Minimize blast radius and recover instantly from node-level failures.

**Stateless Auth API servers:**

- Store no session state; rely on JWT or distributed cache (Redis, DynamoDB).
- Allows instant horizontal scaling and blue-green deployments.

**Redundancy and Auto-Recovery:**

- Use Kubernetes or autoscaling groups with health checks.
- Automatically replace unhealthy instances.
- Use circuit breakers (Hystrix-like) and exponential backoff for downstream dependencies.

**Zero-downtime deployment:**

- Blue-green or canary deployments to avoid downtime during releases.

#### ‚úÖ Summary Answer

| Part                                 | Explanation                                                                                                                                    |
|--------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------|
| **Why 99.999% is nearly impossible** | Real-world systems have failures ‚Äî networks, dependencies, human error, and maintenance make it unachievable to have <5 min downtime per year. |
| **Component #1**                     | **Multi-region active-active deployment** for resilience against regional or infrastructure outages.                                           |
| **Component #2**                     | **Stateless, redundant, self-healing infrastructure** for fast recovery and zero-downtime deployments.                                         |
