# 3.5.6 Design Yelp/Google Maps (Geo-Spatial Search)

> ðŸ“š **Note on Implementation Details:**
> This document focuses on high-level design concepts and architectural decisions.
> For detailed algorithm implementations, see **[pseudocode.md](./pseudocode.md)**.

## ðŸ“Š Visual Diagrams & Resources

- **[High-Level Design Diagrams](./hld-diagram.md)** - System architecture, component design, data flow
- **[Sequence Diagrams](./sequence-diagrams.md)** - Detailed interaction flows and failure scenarios
- **[Design Decisions (This Over That)](./this-over-that.md)** - In-depth analysis of architectural choices
- **[Pseudocode Implementations](./pseudocode.md)** - Detailed algorithm implementations

---

## 1. Problem Statement

Design a **geo-spatial search system** like Yelp or Google Maps that enables users to find businesses, restaurants, and
points of interest (POIs) within a specified radius of their location. The system must handle millions of POIs globally,
support complex filtering (category, rating, hours), and deliver results in under 100ms.

**Key Challenges:**

- **Geo-Spatial Indexing**: Convert 2D geographic coordinates (lat/lng) into a searchable 1D index for fast proximity
  queries
- **Low Latency**: Sub-100ms search results despite querying millions of POIs
- **Complex Filtering**: Support multi-dimensional queries (location + category + rating + hours)
- **Hotspot Management**: Handle densely populated areas (NYC, Tokyo) without overloading specific shards
- **Cross-Boundary Queries**: Handle searches at the edge of geographic boundaries
- **Data Volume**: Store and query billions of reviews and millions of POIs efficiently

---

## 2. Requirements and Scale Estimation

### Functional Requirements (FRs)

1. **Proximity Search**: Find all POIs within N miles/km of a given location (lat/lng)
2. **Point Lookup**: Retrieve detailed information for a specific POI by ID
3. **Complex Filtering**: Support filters on category, rating, price range, hours, and custom attributes
4. **Geo-Indexing**: Store and index millions of POIs with latitude/longitude coordinates
5. **Review Management**: Store and serve billions of user reviews with ratings
6. **Ranking**: Sort results by relevance, distance, rating, or popularity
7. **Auto-complete**: Suggest POI names as users type search queries
8. **Map Rendering**: Serve map tiles and POI markers for visualization

### Non-Functional Requirements (NFRs)

1. **Low Latency**: Search queries must return in < 100ms (p95)
2. **High Availability**: 99.9% uptime (map services are critical for navigation)
3. **Scalability**: Handle 11,574 QPS peak read load
4. **Data Durability**: Store POI and review data permanently with backups
5. **Geographic Distribution**: Support global deployment with regional data centers
6. **Consistency**: POI updates should be eventually consistent (acceptable for non-financial data)

### Scale Estimation

| Metric                | Assumption                   | Calculation                                 | Result                                                 |
|-----------------------|------------------------------|---------------------------------------------|--------------------------------------------------------|
| **Total POIs**        | 50 Million global businesses | -                                           | 50M POIs                                               |
| **Daily Searches**    | 1 Billion searches per day   | $1 \text{B} / 86400 \times 2$ (peak factor) | $\sim 11,574$ $\text{QPS}$ $\text{read}$ $\text{peak}$ |
| **Review Storage**    | 1B reviews Ã— 2KB Ã— 5 years   | $1 \text{B} \times 2 \text{KB} \times 5$    | $\sim 10$ $\text{TB}$ $\text{storage}$                 |
| **POI Storage**       | 50M POIs Ã— 5KB metadata      | $50 \text{M} \times 5 \text{KB}$            | $\sim 250$ $\text{GB}$ $\text{POI}$ $\text{data}$      |
| **Write Load**        | POI updates, new reviews     | $1000$ $\text{writes/sec}$ $\text{peak}$    | Manageable                                             |
| **Geohash Precision** | 7 characters (153m Ã— 153m)   | Standard for POI search                     | 7-char Geohash                                         |

**Geographic Distribution:**

- **Dense Areas**: NYC (100K POIs), Tokyo (150K POIs), London (80K POIs)
- **Rural Areas**: <1000 POIs per region
- **Hotspot Risk**: Top 10 cities contain 30% of all POIs

---

## 3. High-Level Architecture

The architecture centers around a **Geo-Spatial Index** (Geohash/H3) that converts 2D coordinates into searchable 1D
keys, combined with a **Search Engine** (Elasticsearch) for complex filtering and ranking.

### Core Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Mobile/Web Client                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       API Gateway (TLS)                           â”‚
â”‚         Rate Limiting â€¢ Authentication â€¢ Load Balancing           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚              â”‚              â”‚
        â–¼              â–¼              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Geo-Search  â”‚ â”‚  POI Lookupâ”‚ â”‚  Review Serviceâ”‚
â”‚   Service    â”‚ â”‚   Service  â”‚ â”‚                â”‚
â”‚ (Elasticsearch)â”‚ â”‚ (PostgreSQL)â”‚ â”‚  (MongoDB)    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚               â”‚                  â”‚
       â”‚               â–¼                  â”‚
       â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
       â”‚        â”‚   Geohash    â”‚          â”‚
       â”‚        â”‚   Indexer    â”‚          â”‚
       â”‚        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
       â”‚               â”‚                  â”‚
       â”‚               â–¼                  â–¼
       â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚        â”‚   Cache Layer (Redis)         â”‚
       â”‚        â”‚   - Popular queries          â”‚
       â”‚        â”‚   - Ranked results           â”‚
       â”‚        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚               â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  Object Storage (S3) â”‚
            â”‚  - POI photos        â”‚
            â”‚  - Map tiles         â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow

1. **Search Query**: Client sends `{lat, lng, radius, filters}` to API Gateway
2. **Geo-Indexing**: Service converts lat/lng to Geohash, calculates adjacent cells
3. **Query Execution**: Elasticsearch searches POIs in target Geohash cells with filters
4. **Ranking**: Results sorted by distance, rating, relevance
5. **Response**: Returns top N POIs with metadata (name, address, rating, distance)

---

## 4. Detailed Component Design

### 4.1 Geo-Spatial Indexing Strategy

**Challenge**: Transform 2D space (latitude, longitude) into a searchable data structure.

**Solution: Geohash Encoding**

Geohash converts a lat/lng pair into a short alphanumeric string. Nearby locations share common prefixes, enabling:

- Single-dimensional index lookups (fast)
- Proximity search by prefix matching
- Efficient range queries

**Geohash Example:**

```
San Francisco: lat=37.7749, lng=-122.4194 â†’ geohash = "9q8yy9m"

Precision levels:
- 3 chars: 9q8 â†’ ~156 km Ã— 156 km (city-level)
- 5 chars: 9q8yy â†’ ~4.9 km Ã— 4.9 km (neighborhood)
- 7 chars: 9q8yy9m â†’ ~153 m Ã— 153 m (POI-level)
- 9 chars: 9q8yy9mj â†’ ~4.8 m Ã— 4.8 m (building-level)
```

**Why Geohash over Alternatives?**

| Factor                    | Geohash                          | H3 (Uber's Hexagonal)     | PostGIS R-Tree        |
|---------------------------|----------------------------------|---------------------------|-----------------------|
| **Simplicity**            | âœ… String prefix matching         | âŒ Complex hexagonal math  | âŒ Complex SQL queries |
| **Elasticsearch Support** | âœ… Native geo_point               | âš ï¸ Custom implementation  | âŒ Database-only       |
| **Boundary Issues**       | âš ï¸ Edge cases at cell boundaries | âœ… Hexagons tile perfectly | âœ… No boundaries       |
| **Sharding**              | âœ… Easy (prefix-based)            | âš ï¸ Custom sharding logic  | âŒ Database-dependent  |
| **Adoption**              | âœ… Widely supported               | âŒ Uber-specific           | âœ… Industry standard   |

**Decision**: Use **Geohash** for POI search (simpler, Elasticsearch native support).

*See [this-over-that.md: Geohash vs H3](this-over-that.md) for detailed comparison.*

### 4.2 Elasticsearch Geo-Search Architecture

**Why Elasticsearch for Geo-Search?**

1. **Native Geo Support**: Built-in `geo_point` field type with optimized indexing
2. **Complex Queries**: Combine geo filters with category, rating, hours filters
3. **Inverted Index**: Fast text search on POI names, descriptions
4. **Horizontal Scaling**: Shard by Geohash prefix for geographic distribution
5. **Real-time Updates**: Near real-time index updates (<1 second)

**Index Structure:**

```json
{
  "mappings": {
    "properties": {
      "poi_id": {
        "type": "keyword"
      },
      "name": {
        "type": "text",
        "analyzer": "standard"
      },
      "location": {
        "type": "geo_point"
      },
      "geohash": {
        "type": "keyword"
      },
      "category": {
        "type": "keyword"
      },
      "rating": {
        "type": "float"
      },
      "price_range": {
        "type": "integer"
      },
      "hours": {
        "type": "object"
      },
      "review_count": {
        "type": "integer"
      },
      "avg_rating": {
        "type": "float"
      }
    }
  }
}
```

**Query Pattern:**

```json
{
  "query": {
    "bool": {
      "must": [
        {
          "geo_distance": {
            "distance": "5km",
            "location": {
              "lat": 37.7749,
              "lon": -122.4194
            }
          }
        },
        {
          "term": {
            "category": "restaurant"
          }
        },
        {
          "range": {
            "rating": {
              "gte": 4.0
            }
          }
        }
      ]
    }
  },
  "sort": [
    {
      "_geo_distance": {
        "location": {
          "lat": 37.7749,
          "lon": -122.4194
        },
        "order": "asc",
        "unit": "km"
      }
    },
    {
      "rating": {
        "order": "desc"
      }
    }
  ]
}
```

*See pseudocode.md::search_pois() for implementation*

### 4.3 POI Database (PostgreSQL)

**Why PostgreSQL for POI Metadata?**

- **ACID Guarantees**: Ensure POI updates are consistent
- **Rich Schema**: Support complex relationships (POI â†’ categories, hours, attributes)
- **Full-Text Search**: Built-in text search for POI names
- **Mature Ecosystem**: Widely used, well-documented

**Schema Design:**

```sql
CREATE TABLE pois (
    poi_id BIGSERIAL PRIMARY KEY,
    name VARCHAR(255) NOT NULL,
    latitude DECIMAL(10, 8) NOT NULL,
    longitude DECIMAL(11, 8) NOT NULL,
    geohash VARCHAR(12) NOT NULL,
    address TEXT,
    phone VARCHAR(20),
    website VARCHAR(255),
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);

CREATE TABLE poi_categories (
    poi_id BIGINT REFERENCES pois(poi_id),
    category VARCHAR(50) NOT NULL,
    PRIMARY KEY (poi_id, category)
);

CREATE TABLE poi_hours (
    poi_id BIGINT REFERENCES pois(poi_id),
    day_of_week INTEGER CHECK (day_of_week BETWEEN 0 AND 6),
    open_time TIME,
    close_time TIME,
    PRIMARY KEY (poi_id, day_of_week)
);

CREATE INDEX idx_pois_geohash ON pois(geohash);
CREATE INDEX idx_pois_location ON pois USING GIST (
    ll_to_earth(latitude, longitude)
);
```

**Sharding Strategy:**

- **Shard Key**: Geohash prefix (first 3 characters)
- **Shards**: 64 shards (covers major geographic regions)
- **Example**: `9q8` â†’ Shard 1 (West Coast USA), `drm` â†’ Shard 2 (London)

*See pseudocode.md::get_poi_details() for implementation*

### 4.4 Review Storage (MongoDB)

**Why MongoDB for Reviews?**

- **Document Model**: Reviews are unstructured text with varying fields
- **High Write Throughput**: Handle 1M+ reviews per day
- **Scalability**: Horizontal sharding by POI ID
- **Flexibility**: Easy to add new review fields (photos, videos)

**Schema Design:**

```json
{
  "review_id": "rev_123456",
  "poi_id": "poi_789",
  "user_id": "user_456",
  "rating": 4.5,
  "text": "Great pizza place!",
  "photos": [
    "photo1.jpg",
    "photo2.jpg"
  ],
  "helpful_count": 12,
  "created_at": "2024-01-15T10:30:00Z"
}
```

**Sharding Strategy:**

- **Shard Key**: `poi_id` (ensures all reviews for a POI are on same shard)
- **Shards**: 32 shards
- **Replication**: 3 replicas per shard for high availability

**Aggregation Pipeline (Calculate Average Rating):**

```javascript
db.reviews.aggregate([
  { $match: { poi_id: "poi_789" } },
  { $group: {
      _id: "$poi_id",
      avg_rating: { $avg: "$rating" },
      review_count: { $sum: 1 }
    }
  }
])
```

*See pseudocode.md::get_reviews() for implementation*

### 4.5 Caching Strategy

**Why Cache?**

- **Popular Queries**: "Pizza in NYC" searched 100K times/day
- **Ranking Cost**: CPU-heavy ranking calculations
- **Latency Target**: <100ms requires sub-10ms cache lookups

**Cache Layers:**

1. **Query Cache (Redis)**: Cache entire search results
    - Key: `search:{lat}:{lng}:{radius}:{filters_hash}`
    - TTL: 5 minutes (POI data changes infrequently)
    - Size: ~1MB per cached query result

2. **POI Detail Cache (Redis)**: Cache individual POI lookups
    - Key: `poi:{poi_id}`
    - TTL: 1 hour
    - Size: ~5KB per POI

3. **Ranked Results Cache (Redis)**: Pre-computed top results
    - Key: `ranked:{geohash_prefix}:{category}`
    - TTL: 5 minutes
    - Refresh: Background job updates every 5 minutes

**Cache Invalidation:**

- **On POI Update**: Invalidate `poi:{poi_id}` and related search caches
- **On Review Added**: Invalidate `poi:{poi_id}` (affects rating)
- **TTL-Based**: Let caches expire naturally (acceptable stale data)

*See pseudocode.md::cache_search_results() for implementation*

---

## 5. Geo-Spatial Query Processing

### 5.1 Proximity Search Algorithm

**Challenge**: Find all POIs within radius R of point (lat, lng).

**Solution: Geohash Range Query**

1. **Calculate Geohash Precision**: Based on radius R
    - R < 1km â†’ 7-char Geohash (153m Ã— 153m)
    - R < 5km â†’ 6-char Geohash (610m Ã— 610m)
    - R < 20km â†’ 5-char Geohash (4.9km Ã— 4.9km)

2. **Generate Geohash for Center Point**: `encode_geohash(lat, lng, precision)`

3. **Find Adjacent Cells**: Query 8 neighbors + center cell (3Ã—3 grid)
    - Why 8 neighbors? Boundary edge cases (POI at edge of cell)

4. **Query Elasticsearch**: Search POIs in target Geohash cells
    - Filter: `geohash` prefix matches target cells
    - Distance filter: `geo_distance` within radius R

5. **Post-Process**: Filter by exact distance (Geohash is approximate)

*See pseudocode.md::proximity_search() for implementation*

### 5.2 Cross-Boundary Query Handling

**Problem**: A search at the edge of two Geohash cells may miss POIs.

**Example:**

```
Search center: lat=37.7749, lng=-122.4194 (San Francisco)
Geohash: "9q8yy9m"
Adjacent cells: "9q8yy9j", "9q8yy9k", "9q8yy9h", etc.

If POI is in cell "9q8yy9k" but search only queries "9q8yy9m",
we miss the POI even though it's within radius.
```

**Solution**: Query center cell + 8 adjacent cells (3Ã—3 grid).

**Algorithm:**

```
1. Calculate center Geohash: geohash_center
2. Calculate 8 neighbors: geohash_neighbors = get_neighbors(geohash_center)
3. Query Elasticsearch: geohash IN [geohash_center, ...geohash_neighbors]
4. Filter by exact distance: Keep only POIs within radius R
```

*See pseudocode.md::get_adjacent_geohashes() for implementation*

### 5.3 Filtering and Ranking

**Filtering Dimensions:**

1. **Category**: Restaurant, Hotel, Gas Station, etc.
2. **Rating**: Minimum rating (e.g., >4.0 stars)
3. **Price Range**: $, $$, $$$, $$$$
4. **Hours**: Open now, Open on weekends
5. **Custom Attributes**: Wheelchair accessible, Pet-friendly, etc.

**Ranking Algorithm:**

Multi-factor ranking score:

```
score = w1 * distance_score + w2 * rating_score + w3 * popularity_score

where:
- distance_score = 1 / (1 + distance_km)
- rating_score = rating / 5.0
- popularity_score = log(review_count + 1) / log(max_reviews)
- weights: w1=0.4, w2=0.4, w3=0.2
```

**Why This Ranking?**

- **Distance**: Users prefer nearby POIs
- **Rating**: Quality matters
- **Popularity**: Social proof (review count)

*See pseudocode.md::rank_pois() for implementation*

---

## 6. Data Models

### 6.1 POI Document (Elasticsearch)

```json
{
  "poi_id": "poi_123456",
  "name": "Golden Gate Pizza",
  "location": {
    "lat": 37.7749,
    "lon": -122.4194
  },
  "geohash": "9q8yy9m",
  "category": [
    "restaurant",
    "italian",
    "pizza"
  ],
  "rating": 4.5,
  "review_count": 1250,
  "price_range": 2,
  "hours": {
    "monday": {
      "open": "11:00",
      "close": "22:00"
    },
    "tuesday": {
      "open": "11:00",
      "close": "22:00"
    },
    "wednesday": {
      "open": "11:00",
      "close": "22:00"
    },
    "thursday": {
      "open": "11:00",
      "close": "22:00"
    },
    "friday": {
      "open": "11:00",
      "close": "23:00"
    },
    "saturday": {
      "open": "11:00",
      "close": "23:00"
    },
    "sunday": {
      "open": "12:00",
      "close": "21:00"
    }
  },
  "address": {
    "street": "123 Market St",
    "city": "San Francisco",
    "state": "CA",
    "zip": "94102"
  },
  "phone": "+1-415-555-1234",
  "website": "https://goldengatepizza.com",
  "attributes": {
    "wheelchair_accessible": true,
    "parking": true,
    "outdoor_seating": false
  },
  "created_at": "2020-01-15T10:00:00Z",
  "updated_at": "2024-01-15T14:30:00Z"
}
```

### 6.2 Review Document (MongoDB)

```json
{
  "_id": "rev_789012",
  "poi_id": "poi_123456",
  "user_id": "user_456789",
  "user_name": "John Doe",
  "rating": 5,
  "text": "Best pizza in SF! The margherita is amazing.",
  "photos": [
    {
      "url": "https://s3.amazonaws.com/reviews/photo1.jpg",
      "thumbnail": "https://s3.amazonaws.com/reviews/thumb1.jpg"
    }
  ],
  "helpful_count": 23,
  "funny_count": 2,
  "cool_count": 15,
  "created_at": "2024-01-10T18:30:00Z",
  "updated_at": "2024-01-10T18:30:00Z"
}
```

### 6.3 Cache Keys

```
Query Cache:
  search:37.7749:-122.4194:5km:restaurant:4.0

POI Detail Cache:
  poi:poi_123456

Ranked Results Cache:
  ranked:9q8yy:restaurant

Geohash Cell Cache:
  geohash:9q8yy9m:pois
```

---

## 7. Sharding Strategy

### 7.1 Geographic Sharding by Geohash

**Shard Key**: First 3 characters of Geohash

**Why Geohash Prefix?**

- **Locality**: All POIs in a region (e.g., San Francisco) share same prefix
- **Query Efficiency**: Single-shard queries for local searches
- **Load Distribution**: Dense areas (NYC) can be sub-sharded

**Shard Distribution:**

```
Shard 1: 9q8 (West Coast USA - SF, LA)
Shard 2: 9q9 (Pacific Northwest)
Shard 3: dr5 (London, UK)
Shard 4: u09 (Tokyo, Japan)
...
Shard 64: (Rural areas, low POI density)
```

**Hotspot Mitigation:**

For high-traffic prefixes (e.g., `9q8` for SF):

- **Sub-shard by Category**: `9q8:restaurant`, `9q8:hotel`
- **Sub-shard by Rating**: `9q8:high_rating`, `9q8:low_rating`

### 7.2 Cross-Shard Query Handling

**Problem**: Search near shard boundary requires querying multiple shards.

**Solution**: Query coordinator merges results from multiple shards.

**Flow:**

```
1. API Gateway receives search request
2. Geo-Search Service calculates target Geohash cells
3. Determines which shards contain target cells
4. Queries Elasticsearch cluster (routing to specific shards)
5. Elasticsearch merges results from multiple shards
6. Returns top N results
```

**Latency Impact**: Multi-shard queries add 10-20ms (acceptable for <100ms target).

*See pseudocode.md::route_shard_query() for implementation*

---

## 8. Bottlenecks and Optimizations

### 8.1 Hotspotting in Densely Populated Areas

**Problem**: NYC and Tokyo have 10x more POIs than rural areas, overloading specific Geohash shards.

**Symptoms:**

- Shard 1 (NYC) handles 50% of queries
- High latency (200ms+) for NYC searches
- CPU saturation on specific Elasticsearch nodes

**Mitigation: Hierarchical Sharding**

1. **Primary Shard**: By Geohash prefix (3 chars)
2. **Sub-shard**: By category or rating for high-traffic prefixes

**Example:**

```
Before: All NYC POIs in shard "dr5"
After:
  - dr5:restaurant â†’ Shard 1A
  - dr5:hotel â†’ Shard 1B
  - dr5:attraction â†’ Shard 1C
```

**Benefits:**

- Distributes load across more machines
- Reduces query latency (smaller index per shard)
- Maintains locality (same category POIs together)

### 8.2 Cross-Boundary Query Latency

**Problem**: Search at edge of two Geohash cells requires querying both cells, doubling query time.

**Mitigation:**

1. **Optimize Geohash Precision**: Use appropriate precision based on radius
    - Small radius (<1km) â†’ 7-char Geohash (fewer cells to query)
    - Large radius (>10km) â†’ 5-char Geohash (coarser, fewer cells)

2. **Parallel Query Execution**: Query all 9 cells (center + 8 neighbors) in parallel

3. **Cache Adjacent Cells**: Pre-fetch adjacent cell data for popular regions

### 8.3 Multi-DB Join Latency

**Problem**: Final result requires joining Elasticsearch (POI metadata) with MongoDB (reviews).

**Mitigation: Denormalization**

Store essential review data directly in Elasticsearch document:

```json
{
  "poi_id": "poi_123",
  "avg_rating": 4.5,
  // Denormalized from MongoDB
  "review_count": 1250,
  // Denormalized from MongoDB
  "recent_reviews": [
    // Top 3 reviews
    {
      "text": "Great pizza!",
      "rating": 5
    },
    {
      "text": "Nice place",
      "rating": 4
    },
    {
      "text": "Good service",
      "rating": 4
    }
  ]
}
```

**Update Strategy:**

- **Real-time**: Update Elasticsearch when review added (async via Kafka)
- **Batch**: Re-calculate avg_rating every hour (background job)

**Trade-off**: Storage duplication (acceptable for 10-20% increase in size)

### 8.4 Ranking Calculation Cost

**Problem**: CPU-heavy ranking calculations slow down search queries.

**Mitigation: Pre-computed Rankings**

1. **Background Job**: Calculate top 100 POIs for popular regions every 5 minutes
2. **Cache Results**: Store in Redis with TTL=5 minutes
3. **Query Path**: Check cache first, fall back to real-time ranking if cache miss

**Popular Regions:**

- Top 100 cities by search volume
- Popular categories (restaurant, hotel) in each city

**Cache Hit Rate**: 80-90% (most searches are for popular regions)

---

## 9. Availability and Fault Tolerance

### 9.1 Multi-Region Deployment

**Architecture:**

- **Primary Region**: US-East (handles 60% of traffic)
- **Secondary Regions**: EU-West, Asia-Pacific
- **Data Replication**: Async replication of POI and review data

**Failover Strategy:**

- **Elasticsearch**: Cross-region replication (async)
- **PostgreSQL**: Read replicas in each region
- **MongoDB**: Multi-region replica sets
- **Cache**: Regional Redis clusters (no cross-region replication)

**Latency:**

- **Local Queries**: <100ms (same region)
- **Cross-Region Queries**: <200ms (acceptable for global users)

### 9.2 Elasticsearch Cluster Resilience

**Configuration:**

- **Nodes**: 15 nodes per cluster (5 shards Ã— 3 replicas)
- **Replication Factor**: 3 (each shard replicated 3 times)
- **Shard Allocation**: Spread across availability zones

**Failure Scenarios:**

1. **Single Node Failure**:
    - Impact: Minimal (replicas handle traffic)
    - Recovery: Auto-rebalance shards to remaining nodes

2. **Availability Zone Failure**:
    - Impact: Moderate (1/3 of nodes down)
    - Recovery: Replicas in other zones handle traffic

3. **Cluster Split-Brain**:
    - Prevention: Minimum master nodes = (total_nodes / 2) + 1
    - Recovery: Manual intervention (rare)

### 9.3 Database Failover

**PostgreSQL:**

- **Primary**: Write-optimized instance
- **Replicas**: 3 read replicas (async replication)
- **Failover**: Automatic promotion of replica to primary (Patroni, etcd)

**MongoDB:**

- **Replica Set**: 3 nodes (1 primary, 2 secondaries)
- **Failover**: Automatic election of new primary (<30 seconds)

### 9.4 Cache Failure Handling

**Graceful Degradation:**

- **Cache Miss**: Query Elasticsearch directly (acceptable latency increase)
- **Cache Down**: Bypass cache, query Elasticsearch (adds 10-20ms)
- **No User Impact**: System remains functional

**Cache Warm-up:**

- **On Startup**: Pre-load popular queries into cache
- **Background Job**: Refresh cache every 5 minutes

---

## 10. Common Anti-Patterns

### âŒ **1. Scanning Entire Database**

**Problem:**

```sql
SELECT * FROM pois
WHERE 
  ABS(latitude - 37.7749) < 0.05 AND
  ABS(longitude - -122.4194) < 0.05;
```

- Scans all 50M POIs (slow: 10+ seconds)
- No index usage

**Solution:**

```json
// Use Geohash index
{
  "query": {
    "prefix": {
      "geohash": "9q8yy9"
    },
    "geo_distance": {
      "distance": "5km",
      "location": {
        "lat": 37.7749,
        "lon": -122.4194
      }
    }
  }
}
```

- Only queries relevant Geohash cells (fast: <100ms)
- Index-based lookup

### âŒ **2. Ignoring Adjacent Cells**

**Problem:**

```python
# Only query center cell
geohash = encode_geohash(lat, lng, 7)
results = search_pois(geohash)  # Misses POIs at boundaries
```

**Solution:**

```python
# Query center + 8 neighbors
geohash = encode_geohash(lat, lng, 7)
neighbors = get_adjacent_geohashes(geohash)
results = search_pois([geohash] + neighbors)
```

### âŒ **3. Real-Time Joins**

**Problem:**

```python
# Query Elasticsearch for POIs
pois = elasticsearch.search(query)

# Then query MongoDB for reviews (join)
for poi in pois:
    reviews = mongodb.find({"poi_id": poi.id})
    poi.reviews = reviews  # Slow: N+1 queries
```

**Solution:**

```python
# Denormalize review data into Elasticsearch
pois = elasticsearch.search(query)
# Reviews already in POI document (no join needed)
```

### âœ… **4. Proper Geohash Precision**

**Good:**

```python
def get_geohash_precision(radius_km):
    if radius_km < 1:
        return 7  # 153m Ã— 153m
    elif radius_km < 5:
        return 6  # 610m Ã— 610m
    elif radius_km < 20:
        return 5  # 4.9km Ã— 4.9km
    else:
        return 4  # 39km Ã— 39km
```

**Bad:**

```python
# Always use 7-char Geohash (too fine for large radius)
geohash = encode_geohash(lat, lng, 7)  # Queries 100+ cells for 20km radius
```

---

## 11. Alternative Approaches

### Alternative 1: PostgreSQL PostGIS

**Approach**: Use PostgreSQL with PostGIS extension for geo-spatial queries.

**Pros:**

- âœ… ACID guarantees
- âœ… Single database (simpler architecture)
- âœ… Mature spatial indexing (R-Tree, GiST)

**Cons:**

- âŒ Slower queries (10-50ms vs <10ms for Elasticsearch)
- âŒ Write bottleneck (10K writes/sec max)
- âŒ Complex SQL queries (harder to maintain)

**When to Use**: If you need ACID guarantees and can accept slower queries.

### Alternative 2: Redis Geo Commands

**Approach**: Use Redis GEOADD/GEORADIUS for proximity search.

**Pros:**

- âœ… Ultra-fast (sub-millisecond queries)
- âœ… Simple API (GEOADD, GEORADIUS)
- âœ… In-memory (no disk I/O)

**Cons:**

- âŒ Limited filtering (no category, rating filters)
- âŒ Memory constraints (50M POIs Ã— 50 bytes = 2.5GB)
- âŒ No full-text search

**When to Use**: For simple proximity search only (no complex filtering).

### Alternative 3: H3 Hexagonal Grid (Uber's Approach)

**Approach**: Use H3 hexagonal grid instead of Geohash.

**Pros:**

- âœ… Better boundaries (hexagons tile perfectly)
- âœ… Uniform distance (all neighbors equidistant)
- âœ… No edge artifacts

**Cons:**

- âŒ Complex implementation (custom code)
- âŒ Less tooling (fewer libraries)
- âŒ Steeper learning curve

**When to Use**: At Uber scale (millions of POIs, high accuracy requirements).

---

## 12. Monitoring and Observability

### 12.1 Key Metrics

**Latency Metrics:**

- **Search P50**: <50ms
- **Search P95**: <100ms
- **Search P99**: <200ms
- **POI Lookup**: <10ms

**Throughput Metrics:**

- **QPS**: 11,574 peak
- **Cache Hit Rate**: >80%
- **Error Rate**: <0.1%

**Geographic Metrics:**

- **Queries by Region**: Top 10 cities
- **Hotspot Detection**: Shards with >1000 QPS
- **Cross-Boundary Queries**: % of queries hitting multiple shards

### 12.2 Alerts

1. **High Latency**: P95 > 150ms (5-minute window)
2. **Cache Hit Rate Drop**: <70% (indicates cache issue)
3. **Elasticsearch Cluster Health**: Red status
4. **Database Connection Pool Exhaustion**: >90% pool usage
5. **Shard Hotspot**: Single shard handling >30% of queries

### 12.3 Logging

**Structured Logs:**

```json
{
  "timestamp": "2024-01-15T10:30:00Z",
  "service": "geo-search",
  "operation": "search_pois",
  "lat": 37.7749,
  "lng": -122.4194,
  "radius": 5,
  "filters": {
    "category": "restaurant",
    "rating": 4.0
  },
  "results_count": 25,
  "latency_ms": 45,
  "cache_hit": true,
  "shards_queried": 1
}
```

**Use Cases:**

- Debug slow queries
- Analyze search patterns
- Detect anomalies

---

## 13. Trade-offs Summary

| What You Gain                                      | What You Sacrifice                                       |
|----------------------------------------------------|----------------------------------------------------------|
| âœ… **Fast Queries** (<100ms)                        | âŒ **Complex Architecture** (multiple databases)          |
| âœ… **Scalability** (50M POIs)                       | âŒ **Eventual Consistency** (POI updates propagate async) |
| âœ… **Flexible Filtering** (category, rating, hours) | âŒ **Storage Overhead** (denormalized review data)        |
| âœ… **Geographic Distribution** (multi-region)       | âŒ **Cross-Region Latency** (200ms for global queries)    |
| âœ… **Cache Performance** (80% hit rate)             | âŒ **Stale Data** (5-minute TTL)                          |
| âœ… **Horizontal Scaling** (sharding by Geohash)     | âŒ **Hotspot Risk** (dense areas need sub-sharding)       |

---

## 14. Real-World Examples

### Google Maps

**Architecture:**

- **Geo-Indexing**: S2 Geometry Library (Google's spherical geometry)
- **Search Engine**: Custom distributed search (not Elasticsearch)
- **Storage**: Spanner (global distributed database)
- **Caching**: Multi-tier CDN + in-memory cache

**Scale:**

- 200M+ POIs globally
- 1B+ searches per day
- Sub-100ms latency

**Key Innovation:**

- S2 cells for precise geographic indexing
- Pre-computed map tiles for rendering

### Yelp

**Architecture:**

- **Geo-Indexing**: Geohash
- **Search Engine**: Elasticsearch
- **Storage**: PostgreSQL (POI metadata), MongoDB (reviews)
- **Caching**: Redis for popular queries

**Scale:**

- 50M+ POIs
- 100M+ reviews
- 10K+ QPS peak

**Key Innovation:**

- Hybrid search (Elasticsearch + PostgreSQL)
- Aggressive caching of popular queries

### Foursquare

**Architecture:**

- **Geo-Indexing**: H3 (hexagonal grid)
- **Storage**: PostgreSQL with PostGIS
- **Caching**: Redis

**Scale:**

- 100M+ POIs
- Real-time check-ins

**Key Innovation:**

- H3 for better boundary handling
- Real-time location updates

---

## 15. References

- **[Official Docs]:**
    - [Elasticsearch Geo Queries](https://www.elastic.co/guide/en/elasticsearch/reference/current/geo-queries.html)
    - [Geohash Algorithm](https://en.wikipedia.org/wiki/Geohash)
    - [PostgreSQL PostGIS](https://postgis.net/)

- **Related Chapters:**
    - [Uber Ride Matching (3.3.2)](../3.3.2-uber-ride-matching/3.3.2-design-uber-ride-matching.md) - Geohash indexing
      for driver matching
    - [Elasticsearch Deep Dive (2.1.13)](../../02-components/2.1-databases/2.1.13-elasticsearch-deep-dive.md) - Search
      engine architecture
    - [Caching Deep Dive (2.2.1)](../../02-components/2.2-caching/2.2.1-caching-deep-dive.md) - Caching strategies
    - [MongoDB Deep Dive (2.1.10)](../../02-components/2.1-databases/2.1.10-mongodb-deep-dive.md) - Document store for
      reviews

---

## 16. Deployment and Infrastructure

### 16.1 Infrastructure as Code

**Terraform Configuration:**

```hcl
resource "aws_elasticsearch_domain" "poi_search" {
  domain_name = "poi-search"
  cluster_config {
    instance_type = "r6g.xlarge.elasticsearch"
    instance_count = 15
  }
  ebs_options {
    ebs_enabled = true
    volume_size = 500
  }
}

resource "aws_rds_instance" "poi_db" {
  engine = "postgres"
  instance_class = "db.r6g.2xlarge"
  allocated_storage = 1000
  multi_az = true
}
```

### 16.2 Container Orchestration

**Kubernetes Deployment:**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: geo-search-service
spec:
  replicas: 10
  template:
    spec:
      containers:
        - name: geo-search
          image: geo-search:1.0.0
          resources:
            requests:
              cpu: 500m
              memory: 1Gi
            limits:
              cpu: 2000m
              memory: 4Gi
```

### 16.3 CI/CD Pipeline

**Stages:**

1. **Build**: Docker image build
2. **Test**: Unit tests, integration tests
3. **Deploy**: Blue-green deployment to staging
4. **Canary**: 10% traffic to new version
5. **Full Rollout**: 100% traffic if canary succeeds

---

## 17. Advanced Features

### 17.1 Auto-Complete Search

**Requirement**: Suggest POI names as user types.

**Implementation:**

- **Elasticsearch Suggester**: Edge n-gram tokenization
- **Cache**: Popular queries cached in Redis
- **Latency**: <50ms for suggestions

**Example:**

```
User types: "piz"
Suggestions:
  - "Pizza Hut"
  - "Pizzeria Uno"
  - "Pizza Express"
```

### 17.2 Personalized Recommendations

**Requirement**: Show POIs based on user's past behavior.

**Implementation:**

- **ML Model**: Collaborative filtering (user-based)
- **Features**: Past visits, ratings, categories
- **Real-time**: Query-time personalization (adds 10-20ms)

**Example:**

```
User A (likes Italian food) searches "restaurants":
  - Higher ranking for Italian restaurants
  - Lower ranking for fast food
```

### 17.3 Real-Time Updates

**Requirement**: Show POI status changes (hours, temporary closures).

**Implementation:**

- **Kafka**: Stream POI update events
- **Elasticsearch**: Near real-time index updates (<1 second)
- **Cache Invalidation**: Invalidate affected cache keys

**Example:**

```
POI updates hours: "Closed on Sundays"
  â†’ Kafka event published
  â†’ Elasticsearch index updated
  â†’ Cache invalidated
  â†’ Users see updated hours within 1 second
```

---

## 18. Performance Optimization

### 18.1 Connection Pooling

**PostgreSQL:**

- **Pool Size**: 50 connections per instance
- **Max Connections**: 200 per database
- **Idle Timeout**: 10 minutes

**MongoDB:**

- **Pool Size**: 100 connections per instance
- **Max Connections**: 500 per replica set

**Elasticsearch:**

- **HTTP Client**: Connection pool of 50
- **Keep-Alive**: Reuse connections

### 18.2 Query Optimization

**Elasticsearch:**

- **Index Settings**: `refresh_interval: 5s` (balance freshness vs performance)
- **Field Data**: Enable fielddata for aggregations (memory trade-off)
- **Shard Size**: 50GB per shard (optimal for search performance)

**PostgreSQL:**

- **Indexes**: B-tree on `geohash`, GIST on `location`
- **Query Plan**: Use EXPLAIN ANALYZE to optimize slow queries
- **Vacuum**: Regular VACUUM to prevent bloat

### 18.3 Compression

**Elasticsearch:**

- **Index Compression**: `best_compression` (trade CPU for storage)
- **HTTP Compression**: Gzip responses (reduces bandwidth)

**Cache:**

- **Redis**: Use compression for large values (>1KB)
- **Algorithm**: Snappy (fast, low CPU)

---

## 19. Interview Discussion Points

### Question 1: How do you handle searches at Geohash cell boundaries?

**Answer**: Query center cell + 8 adjacent cells.

**Deep Dive:**

1. Calculate center Geohash for search point
2. Generate 8 neighbors (north, south, east, west, northeast, northwest, southeast, southwest)
3. Query all 9 cells in parallel
4. Post-filter by exact distance (Geohash is approximate)

**Follow-up**: What if radius is large (20km)?

- Use coarser Geohash precision (5 chars instead of 7)
- Reduces number of cells to query
- Trade-off: Less precise, but acceptable for large radius

### Question 2: How do you prevent hotspotting in dense areas like NYC?

**Answer**: Hierarchical sharding.

**Deep Dive:**

1. **Primary Shard**: By Geohash prefix (3 chars)
2. **Sub-shard**: By category or rating for high-traffic prefixes

**Example:**

```
NYC Geohash prefix: "dr5"
Sub-shards:
  - dr5:restaurant â†’ Shard 1A
  - dr5:hotel â†’ Shard 1B
  - dr5:attraction â†’ Shard 1C
```

**Benefits:**

- Distributes load across more machines
- Maintains locality (same category POIs together)
- Reduces query latency (smaller index per shard)

### Question 3: Why Elasticsearch instead of PostgreSQL PostGIS?

**Answer**: Elasticsearch is optimized for search use cases.

**Comparison:**

| Factor                 | Elasticsearch                        | PostGIS                      |
|------------------------|--------------------------------------|------------------------------|
| **Query Latency**      | <10ms                                | 10-50ms                      |
| **Complex Filtering**  | âœ… Native (category + rating + hours) | âŒ Complex SQL                |
| **Full-Text Search**   | âœ… Inverted index                     | âš ï¸ Requires additional index |
| **Horizontal Scaling** | âœ… Native sharding                    | âŒ Manual sharding            |
| **ACID**               | âŒ Eventual consistency               | âœ… ACID guarantees            |

**Trade-off**: We sacrifice ACID guarantees for better search performance (acceptable for POI data).

### Question 4: How do you scale to 11K QPS?

**Answer**: Multi-level scaling approach.

**Horizontal Scaling:**

1. **API Gateway**: 50 instances behind load balancer
2. **Geo-Search Service**: 100 instances (stateless)
3. **Elasticsearch**: 15-node cluster (5 shards Ã— 3 replicas)
4. **PostgreSQL**: 10 read replicas
5. **Redis Cache**: 10-node cluster

**Caching:**

- 80% cache hit rate (reduces backend load by 80%)
- Effective QPS to Elasticsearch: 11K Ã— 20% = 2.2K QPS

**Vertical Optimization:**

- Connection pooling (reuse connections)
- Query optimization (indexes, query plans)
- Compression (reduce bandwidth)

---

## 20. Final Architecture Summary

### Component Count

| Component              | Instances                      | Purpose                          |
|------------------------|--------------------------------|----------------------------------|
| **API Gateway**        | 50                             | Load balancing, rate limiting    |
| **Geo-Search Service** | 100                            | Search logic, query coordination |
| **Elasticsearch**      | 15 nodes                       | POI search index                 |
| **PostgreSQL**         | 1 primary + 10 replicas        | POI metadata                     |
| **MongoDB**            | 3-node replica set Ã— 32 shards | Review storage                   |
| **Redis Cache**        | 10-node cluster                | Query caching                    |

### Data Flow Summary

```
Search Request:
  1. API Gateway (rate limit, auth)
  2. Geo-Search Service (Geohash calculation)
  3. Redis Cache (check cache)
  4. Elasticsearch (query POIs)
  5. PostgreSQL (POI details if needed)
  6. Response (ranked POIs)
```

### Performance Characteristics

- **Search Latency**: <100ms (p95)
- **Cache Hit Rate**: 80%
- **Throughput**: 11,574 QPS peak
- **Availability**: 99.9% (multi-region)
- **Storage**: 10TB reviews + 250GB POIs

---

## 21. Conclusion

The Yelp/Google Maps geo-spatial search system leverages **Geohash indexing** and **Elasticsearch** to deliver sub-100ms
search results for millions of POIs. Key architectural decisions include:

1. **Geohash for Indexing**: Converts 2D coordinates to 1D searchable keys
2. **Elasticsearch for Search**: Handles complex queries with low latency
3. **Denormalization**: Eliminates joins by storing review data in Elasticsearch
4. **Hierarchical Sharding**: Prevents hotspotting in dense areas
5. **Multi-tier Caching**: Achieves 80% cache hit rate for popular queries

The system scales horizontally to handle 11K+ QPS while maintaining <100ms latency through geographic sharding,
aggressive caching, and optimized query processing.