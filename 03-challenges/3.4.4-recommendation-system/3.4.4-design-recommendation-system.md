# 3.4.4 Design a Recommendation System (Real-Time Personalization)

> ðŸ“š **Note on Implementation Details:**
> This document focuses on high-level design concepts and architectural decisions.
> For detailed algorithm implementations, see **[pseudocode.md](./pseudocode.md)**.

## ðŸ“Š Visual Diagrams & Resources

- **[High-Level Design Diagrams](./hld-diagram.md)** - Lambda architecture, feature store, model serving flow
- **[Sequence Diagrams](./sequence-diagrams.md)** - Real-time serving, batch training, feature updates
- **[Design Decisions (This Over That)](./this-over-that.md)** - In-depth analysis of architectural choices
- **[Pseudocode Implementations](./pseudocode.md)** - Detailed algorithm implementations

---

## 1. Problem Statement

Design a recommendation system that provides real-time, highly personalized recommendations to users (products, content,
videos, articles) with extremely low latency (<50ms). The system must:

- **Serve** personalized recommendations instantly (100k QPS, <50ms p99 latency)
- **Train** ML models offline on petabytes of historical data (clickstream, purchases, ratings)
- **Update** real-time features (user's last 5 views, current session context)
- **Scale** to 100M daily active users with diverse recommendation scenarios
- **Handle** cold-start problem for new users and new items

**Similar Systems:** Netflix recommendations, Amazon product recommendations, YouTube video suggestions, TikTok "For
You" feed, Spotify Discover Weekly

---

## 2. Requirements and Scale Estimation

### Functional Requirements

1. **Personalized Feed** - Generate "For You" recommendations based on user history and preferences
2. **Similar Items** - "You may also like" recommendations based on current item
3. **Real-Time Context** - Incorporate current session behavior (last 5 views, search queries)
4. **Cold Start Handling** - Provide recommendations for new users (no history) and new items
5. **A/B Testing** - Support multiple recommendation algorithms simultaneously
6. **Explainability** - Provide reasons for recommendations ("Because you watched...")

### Non-Functional Requirements

1. **Low Latency (Serving):** <50ms p99 latency for recommendation API calls
2. **High Throughput:** 100k QPS for read requests (recommendation serving)
3. **High Availability:** 99.99% uptime (recommendations critical for engagement)
4. **Model Freshness:** Models retrained daily (24-hour lag acceptable)
5. **Feature Freshness:** Real-time features updated within 1 second
6. **Scalability:** Handle 100M DAU with 10+ recommendations per user per day

### Scale Estimation

| Metric                      | Calculation                                  | Result                        |
|-----------------------------|----------------------------------------------|-------------------------------|
| **Daily Active Users**      | Given                                        | 100 Million DAU               |
| **Recommendation Requests** | 100M users Ã— 10 requests/user/day            | 1 Billion requests/day        |
| **Peak QPS**                | 1B requests / 86,400 sec Ã— 3x peak           | **100,000 QPS** (peak)        |
| **Avg QPS**                 | 1B requests / 86,400 sec                     | 11,574 QPS (average)          |
| **Latency Target**          | Given                                        | <50ms p99                     |
| **Training Data**           | 100M users Ã— 100 actions/user/day Ã— 365 days | **3.65 trillion events/year** |
| **Training Data Size**      | 3.65T events Ã— 100 bytes/event               | **365 TB/year**               |
| **Feature Store QPS**       | Same as recommendation QPS                   | 100,000 reads/sec             |
| **Model Storage**           | 100M users Ã— 128D embeddings Ã— 4 bytes       | ~50 GB (user embeddings)      |

**Storage Breakdown:**

- **User Embeddings:** 100M users Ã— 128 dimensions Ã— 4 bytes = 50 GB
- **Item Embeddings:** 10M items Ã— 128 dimensions Ã— 4 bytes = 5 GB
- **Feature Store:** 100M users Ã— 1 KB features = 100 GB
- **Training Data (1 year):** 365 TB (compressed: ~100 TB with Parquet)

---

## 3. High-Level Architecture

The system follows **Lambda Architecture** with two distinct processing paths:

**1. Batch Layer (Offline Training)**

- Process historical data (petabytes) to train accurate ML models
- Runs daily (24-hour lag acceptable)
- Uses Spark/Hadoop for distributed training

**2. Speed Layer (Real-Time Serving)**

- Serves recommendations in <50ms using pre-trained models
- Updates real-time features (user's last actions) from Kafka stream
- Uses Redis Feature Store for low-latency lookups

**ASCII Architecture Diagram:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         CLIENT LAYER                             â”‚
â”‚  [Mobile App]  [Web Browser]  [Smart TV]  [Third-Party Apps]    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â”‚ HTTP/REST (100k QPS)
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       SERVING LAYER                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Recommendation API (gRPC/REST)                            â”‚ â”‚
â”‚  â”‚  - User personalization                                    â”‚ â”‚
â”‚  â”‚  - Similar items                                           â”‚ â”‚
â”‚  â”‚  - Cold start handling                                     â”‚ â”‚
â”‚  â”‚  Load Balancer: 100 instances (1000 QPS each)             â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚                       â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Feature Store   â”‚   â”‚   Model Store        â”‚
    â”‚  (Redis)         â”‚   â”‚   (S3 + Cache)       â”‚
    â”‚  - User features â”‚   â”‚   - User embeddings  â”‚
    â”‚  - Real-time ctx â”‚   â”‚   - Item embeddings  â”‚
    â”‚  <1ms latency    â”‚   â”‚   - Trained models   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       SPEED LAYER (Real-Time)                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Kafka Streams / Flink                                     â”‚ â”‚
â”‚  â”‚  - Process user events in real-time                        â”‚ â”‚
â”‚  â”‚  - Update feature store (last 5 views, session context)   â”‚ â”‚
â”‚  â”‚  - <1 second latency                                       â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  EVENT COLLECTION                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Kafka Cluster                                             â”‚ â”‚
â”‚  â”‚  - Topics: user_events, item_events, feedback             â”‚ â”‚
â”‚  â”‚  - Throughput: 50k events/sec                             â”‚ â”‚
â”‚  â”‚  - Retention: 7 days (for real-time), Archive to S3       â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   BATCH LAYER (Offline Training)                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Spark Cluster (Daily Training Jobs)                      â”‚ â”‚
â”‚  â”‚  - Collaborative Filtering (Matrix Factorization/ALS)     â”‚ â”‚
â”‚  â”‚  - Deep Learning (Two-Tower Model, BERT4Rec)              â”‚ â”‚
â”‚  â”‚  - Training Time: 4-6 hours                               â”‚ â”‚
â”‚  â”‚  - Training Data: S3 (365 TB/year)                        â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Model Store â”‚
         â”‚  (S3 + Cache)â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Components:**

1. **Recommendation API** (Serving Layer)
    - Handles 100k QPS with <50ms latency
    - Fetches features from Redis, loads model from cache
    - Combines signals (collaborative filtering + content-based + real-time)

2. **Feature Store** (Redis)
    - Stores user features (last 5 views, preferences, demographics)
    - Updated in real-time (<1 second) from Kafka stream
    - Key-value store for O(1) lookups

3. **Model Store** (S3 + In-Memory Cache)
    - Stores trained ML models (user/item embeddings, neural networks)
    - Updated daily after batch training completes
    - Cached in serving instances (hot-loaded)

4. **Speed Layer** (Kafka Streams/Flink)
    - Processes real-time events (views, clicks, purchases)
    - Updates Feature Store with latest user actions
    - Maintains session context (current browsing session)

5. **Batch Layer** (Spark)
    - Trains ML models on historical data (365 TB)
    - Runs daily (overnight batch jobs)
    - Generates user/item embeddings using collaborative filtering

---

## 4. Data Model

### User Features (Redis Feature Store)

```
Key: user:{user_id}:features
Value: {
  "user_id": "123",
  "recent_views": ["item_456", "item_789", "item_012", "item_345", "item_678"],
  "recent_searches": ["laptop", "gaming mouse"],
  "preferences": {
    "categories": ["Electronics", "Gaming"],
    "price_range": [100, 1000],
    "brands": ["Apple", "Samsung", "Sony"]
  },
  "demographics": {
    "age_group": "25-34",
    "gender": "M",
    "location": "US-CA"
  },
  "session_context": {
    "current_category": "Electronics",
    "session_start": 1698765432,
    "page_views_in_session": 5
  },
  "last_updated": 1698765432
}
TTL: 24 hours
```

### Item Features (Redis Feature Store)

```
Key: item:{item_id}:features
Value: {
  "item_id": "456",
  "title": "Apple MacBook Pro M3",
  "category": "Electronics > Laptops",
  "price": 1999.00,
  "brand": "Apple",
  "tags": ["laptop", "m3", "apple", "macbook", "pro"],
  "popularity_score": 0.95,
  "rating": 4.8,
  "num_reviews": 1234,
  "embedding_version": "v2024-10"
}
TTL: 7 days
```

### User Embeddings (Model Store)

```
Key: user_embedding:{user_id}
Value: float32[128]  // 128-dimensional vector
Size: 512 bytes per user
Storage: In-memory cache (hot users) + S3 (all users)
```

### Item Embeddings (Model Store)

```
Key: item_embedding:{item_id}
Value: float32[128]  // 128-dimensional vector
Size: 512 bytes per item
Storage: In-memory cache (hot items) + S3 (all items)
ANN Index: HNSW for fast similarity search
```

### Training Data (S3 - Parquet Format)

```
Table: user_interactions
Partitioned by: date (YYYY-MM-DD)

Columns:
- user_id (string)
- item_id (string)
- action_type (string): view, click, add_to_cart, purchase, rate
- timestamp (bigint): Unix timestamp
- session_id (string)
- context (json): device, location, referrer
- rating (float): For explicit feedback (1-5 stars)
- dwell_time (int): Time spent on item page (seconds)

Size: ~1 GB per partition (100M events)
Retention: 2 years (730 partitions Ã— 1 GB = 730 GB compressed)
```

---

## 5. Detailed Component Design

### 5.1 Recommendation API (Serving Layer)

**Tech Stack:**

- Language: Go or Java (low latency, high throughput)
- Framework: gRPC (internal), REST (external)
- Load Balancer: Envoy or Nginx
- Instances: 100 instances (1000 QPS each)

**Recommendation Flow:**

1. **Receive Request**
   ```
   GET /recommendations?user_id=123&context=homepage&num_results=20
   ```

2. **Fetch User Features** (from Redis Feature Store)
    - Recent views, preferences, demographics
    - Latency: <1ms (Redis local cache)

3. **Load ML Model** (from in-memory cache)
    - User embedding, item embeddings
    - Latency: ~0ms (pre-loaded in memory)

4. **Candidate Generation** (100-1000 candidates)
    - Collaborative Filtering: Top 500 items based on user embedding similarity
    - Content-Based: Top 300 items based on recent views
    - Popular Items: Top 200 items (trending, high engagement)

5. **Ranking** (Score 1000 candidates)
    - Multi-layer perceptron (MLP) model
    - Features: user embedding Â· item embedding, popularity, price, category match
    - Output: Personalized score for each candidate

6. **Post-Processing**
    - Deduplication (remove already viewed items)
    - Diversity (ensure variety in categories)
    - Business rules (promote sponsored items, new arrivals)

7. **Return Top N** (e.g., 20 items)
    - Latency: ~30ms (p50), ~50ms (p99)

**Optimization Techniques:**

1. **Caching**
    - **User embedding cache:** In-memory (hot users)
    - **Item embedding cache:** In-memory (hot items, 10% of catalog)
    - **Popular recommendations cache:** Pre-computed top 1000 items for generic queries

2. **Parallelization**
    - Fetch user features and item candidates in parallel (goroutines/threads)
    - Scoring: Batch matrix multiplication (GPU acceleration for deep models)

3. **Fallback Strategies**
    - If user features unavailable â†’ Use demographic-based recommendations
    - If model unavailable â†’ Use popularity-based recommendations
    - Circuit breaker for downstream dependencies (Feature Store, Model Store)

### 5.2 Feature Store (Redis)

**Purpose:** Store and serve real-time user/item features with <1ms latency.

**Tech Stack:**

- Redis Cluster (6 nodes, 3 replicas)
- Memory: 500 GB (100M users Ã— 5 KB features)
- Persistence: AOF (Append-Only File) for durability

**Feature Update Pipeline:**

1. **Event Collection** (Kafka)
    - User action (view, click, purchase) â†’ Kafka topic `user_events`

2. **Stream Processing** (Kafka Streams or Flink)
    - Aggregate recent actions (last 5 views) in sliding window
    - Compute session context (current category, page views)

3. **Write to Redis**
    - Update `user:{user_id}:features` with latest data
    - Latency: <100ms (event â†’ Redis)

**Example: Update Recent Views**

*See pseudocode.md::update_recent_views() for implementation*

When user views item "789":

1. Fetch current recent_views: ["456", "123", "999", "888", "777"]
2. Add new item to front: ["789", "456", "123", "999", "888"]
3. Trim to max 5 items
4. Write back to Redis
5. Set TTL: 24 hours

### 5.3 Batch Training (Spark)

**Purpose:** Train ML models on historical data (365 TB) to generate accurate user/item embeddings.

**Tech Stack:**

- Apache Spark (100-node cluster)
- Training Data: S3 (Parquet format, partitioned by date)
- Training Framework: Spark MLlib (ALS), PyTorch (Deep Learning)

**Training Pipeline (Daily Job):**

1. **Data Collection** (6 hours, 12 AM - 6 AM)
    - Read last 30 days of user interactions from S3
    - Filter out bots, invalid events
    - Aggregate implicit feedback (views, clicks) and explicit feedback (ratings)

2. **Feature Engineering** (1 hour)
    - User features: Interaction history, demographics, engagement metrics
    - Item features: Category, price, popularity, tags

3. **Model Training** (4 hours)
    - **Collaborative Filtering:** Matrix Factorization (ALS algorithm)
        - Input: User-item interaction matrix (100M users Ã— 10M items)
        - Output: User embeddings (128D), Item embeddings (128D)
        - Algorithm: Alternating Least Squares (Spark MLlib)

    - **Deep Learning:** Two-Tower Model (optional, for higher accuracy)
        - User tower: [user features] â†’ MLP â†’ 128D embedding
        - Item tower: [item features] â†’ MLP â†’ 128D embedding
        - Training: Contrastive learning (positive pairs: user-item interactions)

4. **Model Evaluation** (1 hour)
    - Offline metrics: Precision@10, Recall@10, NDCG@10, AUC
    - Hold-out validation set (last 7 days)
    - Compare to baseline model (previous day)

5. **Model Deployment** (30 min)
    - Export embeddings to S3 (Parquet format)
    - Blue-green deployment (gradual rollout to 10% â†’ 100%)
    - Monitor online metrics (CTR, engagement)

**Total Runtime:** ~12 hours (overnight job)

### 5.4 Real-Time Feature Updates (Kafka Streams)

**Purpose:** Process user events in real-time and update Feature Store within 1 second.

**Tech Stack:**

- Kafka Streams or Apache Flink
- Stateful processing (maintain user session state)
- Throughput: 50k events/sec

**Processing Logic:**

*See pseudocode.md::process_user_event() for implementation*

1. **Consume Event** from Kafka topic `user_events`
   ```json
   {
     "user_id": "123",
     "event_type": "view",
     "item_id": "789",
     "timestamp": 1698765432,
     "session_id": "sess-abc-123"
   }
   ```

2. **Update Session Context** (stateful operation)
    - Track page views in current session
    - Identify current category (infer from recent views)

3. **Update Recent Views** (Redis)
    - Append item to recent_views list (max 5 items)
    - Update timestamp

4. **Update Aggregates** (windowed aggregation)
    - Category preferences (count views per category in last 7 days)
    - Brand preferences

5. **Write to Redis** (Feature Store)
    - Latency: <100ms (event â†’ Redis)

### 5.5 Candidate Generation and Ranking

**Candidate Generation (Recall):**

Goal: Generate 1000 candidates from 10M item catalog in <10ms.

**Three Sources:**

1. **Collaborative Filtering** (500 candidates)
    - Compute: user_embedding Â· item_embeddings (dot product)
    - Use Approximate Nearest Neighbors (ANN) for fast search
    - Algorithm: HNSW (Hierarchical Navigable Small World)
    - Latency: <5ms for 500 candidates

2. **Content-Based** (300 candidates)
    - Based on user's recent views and preferences
    - Filter items by category, price range, brand
    - Use inverted index for fast filtering

3. **Popular Items** (200 candidates)
    - Trending items (high engagement in last 24 hours)
    - New arrivals (recently added to catalog)
    - Editorial picks (curated by content team)

**Ranking (Precision):**

Goal: Score 1000 candidates and return top 20 in <20ms.

**Ranking Model:**

*See pseudocode.md::rank_candidates() for implementation*

Input features (per candidate):

- User embedding Â· Item embedding (dot product)
- User features: age, gender, location
- Item features: price, category, popularity, rating
- Context features: time of day, device type
- Interaction features: category match, brand match

Model: Multi-layer Perceptron (MLP) with 3 hidden layers

- Layer 1: 128 neurons (ReLU)
- Layer 2: 64 neurons (ReLU)
- Layer 3: 32 neurons (ReLU)
- Output: 1 neuron (sigmoid) â†’ score [0, 1]

Training: Logistic regression (binary classification)

- Positive examples: User clicked/purchased item
- Negative examples: User viewed but did not click

**Inference:**

- Batch inference: Score all 1000 candidates in one forward pass
- Latency: <10ms (CPU) or <2ms (GPU)

---

## 6. Bottlenecks and Scaling Strategies

### Bottleneck 1: Serving Latency (>50ms)

**Problem:**

- Multiple network calls: Feature Store (Redis), Model Store (S3)
- Complex computation: Candidate generation (ANN), ranking (MLP inference)
- Target: <50ms p99, but currently ~80ms

**Solutions:**

1. **Pre-Compute Popular Recommendations**
    - Cache top 1000 recommendations for generic queries (no user context)
    - Covers 30% of traffic (anonymous users, cold start)
    - Latency: <5ms (Redis cache hit)

2. **In-Memory Caching**
    - Load user/item embeddings in serving instance memory
    - Avoids Redis roundtrip for hot users/items
    - Cache hit rate: 80% (Pareto principle: 20% of users generate 80% of traffic)

3. **Model Simplification**
    - Use lightweight ranking model (logistic regression instead of deep MLP)
    - Trade-off: 2% accuracy loss for 10x faster inference

4. **Parallelization**
    - Fetch features and generate candidates in parallel (goroutines)
    - Use gRPC multiplexing for batch requests

5. **Edge Caching (CDN)**
    - Cache recommendations at CDN edge (CloudFlare, Fastly)
    - TTL: 1 minute (acceptable staleness for non-personalized scenarios)

### Bottleneck 2: Feature Store Overload (100k QPS)

**Problem:**

- Redis cluster handles 100k reads/sec
- Each read: ~1 KB data (user features)
- Memory: 500 GB (exceeds single-node capacity)

**Solutions:**

1. **Redis Cluster Sharding**
    - Shard by user_id (consistent hashing)
    - 10 Redis nodes (10k QPS each)
    - Replication: 3x (master + 2 replicas)

2. **Feature Compression**
    - Store only essential features (recent 5 views, not full history)
    - Use efficient encoding (protobuf instead of JSON)
    - Reduce payload size: 1 KB â†’ 200 bytes (5x compression)

3. **Read Replicas**
    - Route reads to replicas (read-heavy workload)
    - Master: Handles writes only (~1k QPS)
    - Replicas: Handle reads (100k QPS split across replicas)

4. **Local Cache (Serving Instance)**
    - Cache hot user features in serving instance memory
    - Eviction: LRU (Least Recently Used)
    - Cache hit rate: 70% (reduces Redis load by 70%)

### Bottleneck 3: Batch Training Scalability (365 TB)

**Problem:**

- Training data: 365 TB/year (3.65 trillion events)
- Training time: 12 hours (too long, blocks next-day deployment)
- Spark cluster: 100 nodes (expensive)

**Solutions:**

1. **Incremental Training**
    - Train only on last 30 days (not full year)
    - Reduces data: 365 TB â†’ 30 TB (12x smaller)
    - Training time: 12 hours â†’ 2 hours

2. **Sample-Based Training**
    - Train on 10% random sample (representative)
    - Reduces data: 30 TB â†’ 3 TB (10x smaller)
    - Trade-off: 1-2% accuracy loss

3. **Model Distillation**
    - Train large, accurate model offline (weekly)
    - Distill to smaller model (daily fine-tuning)
    - Faster training, similar accuracy

4. **Feature Pre-Aggregation**
    - Pre-compute aggregates (user's top categories, brands) offline
    - Training uses pre-aggregated features (not raw events)
    - Reduces training complexity

### Bottleneck 4: Cold Start Problem

**Problem:**

- New users: No interaction history â†’ Cannot generate personalized recommendations
- New items: No user interactions â†’ Cannot compute item embeddings

**Solutions:**

1. **New User Cold Start**
    - **Option 1:** Demographic-based recommendations (age, gender, location)
    - **Option 2:** Onboarding quiz ("Select your interests")
    - **Option 3:** Popular items (trending, editorial picks)

2. **New Item Cold Start**
    - **Option 1:** Content-based features (category, tags, price)
    - **Option 2:** Similar items (based on content features, not interactions)
    - **Option 3:** Promote as "New Arrivals" (boost in ranking)

3. **Hybrid Approach**
    - Combine collaborative filtering (if available) + content-based (always available)
    - Weight: 70% collaborative + 30% content-based (for warm users)
    - Weight: 0% collaborative + 100% content-based (for cold users)

---

## 7. Multi-Algorithm Serving (A/B Testing)

**Challenge:** Run multiple recommendation algorithms simultaneously for A/B testing.

**Architecture:**

```
                     [Recommendation API]
                             |
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                  â”‚                  â”‚
    [Algorithm A]      [Algorithm B]      [Algorithm C]
  Collaborative      Content-Based      Deep Learning
   Filtering             (Baseline)       (Two-Tower)
      50%                  25%                 25%
```

**Implementation:**

1. **User Bucketing** (Consistent Hashing)
    - Hash user_id â†’ Assign to bucket [A, B, C]
    - Bucket A: 50% of users (Collaborative Filtering)
    - Bucket B: 25% of users (Content-Based)
    - Bucket C: 25% of users (Deep Learning)

2. **Model Versioning**
    - Store multiple models in Model Store (S3)
    - Model A: `s3://models/collaborative-v2024-10-29.pkl`
    - Model B: `s3://models/content-based-v2024-10-29.pkl`
    - Model C: `s3://models/two-tower-v2024-10-29.pkl`

3. **Metrics Collection**
    - Track per-algorithm metrics (CTR, engagement, conversion)
    - Send to analytics pipeline (Kafka â†’ ClickHouse â†’ Grafana)

4. **Winner Selection**
    - After 1 week: Statistical significance test (t-test)
    - Promote winning algorithm to 100% traffic

---

## 8. Common Anti-Patterns

### âŒ Anti-Pattern 1: Synchronous Model Training During Serving

**Problem:**

- Training model on-the-fly during recommendation request
- Latency: >5 seconds (unacceptable)

**Solution:**
âœ… **Pre-Train Models Offline** (Batch Layer)

- Train models once per day in batch job
- Serve pre-computed embeddings (cached in memory)
- Latency: <50ms

### âŒ Anti-Pattern 2: Single Recommendation Algorithm

**Problem:**

- Only collaborative filtering (fails for cold start)
- Or only content-based (ignores user preferences)

**Solution:**
âœ… **Hybrid Approach** (Ensemble)

- Combine collaborative filtering + content-based + popularity
- Weight: 60% collaborative + 30% content-based + 10% popular
- Handles cold start gracefully

### âŒ Anti-Pattern 3: No Real-Time Features

**Problem:**

- Only use batch features (updated daily)
- Ignores user's current session behavior

**Solution:**
âœ… **Real-Time Feature Store** (Speed Layer)

- Update features in real-time (<1 second) from Kafka stream
- Incorporate current session context (last 5 views)
- Improves personalization significantly (+15% CTR)

### âŒ Anti-Pattern 4: No Diversity in Recommendations

**Problem:**

- All recommendations from same category (filter bubble)
- Example: User views 1 laptop â†’ Recommended 20 laptops

**Solution:**
âœ… **Diversity Post-Processing**

- Ensure variety in categories (max 5 items per category)
- Add exploration (10% random items from different categories)
- Balance exploitation (user preferences) vs exploration (discovery)

### âŒ Anti-Pattern 5: Ignoring Negative Signals

**Problem:**

- Only use positive feedback (clicks, purchases)
- Ignores negative signals (user skipped item, closed page quickly)

**Solution:**
âœ… **Incorporate Negative Feedback**

- Explicit: User dislikes item (thumbs down)
- Implicit: Low dwell time (<5 seconds), did not click, did not purchase
- Use in training: Negative examples for logistic regression

---

## 9. Alternative Approaches

### Approach 1: Fully Online Learning (Kappa Architecture)

**Description:**

- Replace batch training with continuous online learning
- Train model incrementally on real-time stream (no batch jobs)

**Pros:**

- âœ… No 24-hour lag (model always up-to-date)
- âœ… Adapts to user behavior changes instantly
- âœ… Handles concept drift (user preferences change over time)

**Cons:**

- âŒ More complex (stateful stream processing)
- âŒ Harder to debug (no reproducible batch runs)
- âŒ Requires careful model update strategy (avoid catastrophic forgetting)

**When to Use:** When freshness is critical (TikTok "For You" feed, news recommendations)

### Approach 2: Rule-Based Recommendations (No ML)

**Description:**

- Use hand-crafted rules instead of ML models
- Example: "Users who viewed X also viewed Y"

**Pros:**

- âœ… Simple to implement and debug
- âœ… No training infrastructure needed
- âœ… Explainable (business can understand rules)

**Cons:**

- âŒ Not personalized (same for all users)
- âŒ Requires manual tuning (rules become outdated)
- âŒ Lower accuracy than ML models (10-20% worse CTR)

**When to Use:** Small-scale systems (<1M users), simple use cases

### Approach 3: Graph-Based Recommendations (Neo4j)

**Description:**

- Model users and items as graph (nodes = users/items, edges = interactions)
- Use graph algorithms (PageRank, community detection) for recommendations

**Pros:**

- âœ… Natural representation of relationships
- âœ… Explainable (show path: User â†’ Item1 â†’ Item2)
- âœ… Handles complex relationships (multi-hop: friend-of-friend)

**Cons:**

- âŒ Slow queries (graph traversal expensive)
- âŒ Hard to scale (graph databases limited to single node)
- âŒ Not suitable for real-time serving (<50ms)

**When to Use:** Social networks (LinkedIn, Facebook), relationship-heavy domains

---

## 10. Monitoring and Observability

### Key Metrics

**Business Metrics:**

| Metric                   | Target        | Alert Threshold |
|--------------------------|---------------|-----------------|
| Click-Through Rate (CTR) | >5%           | <4%             |
| Conversion Rate          | >2%           | <1.5%           |
| Engagement (dwell time)  | >60 seconds   | <45 seconds     |
| Diversity (categories)   | >5 categories | <3 categories   |
| Cold Start Coverage      | >95%          | <90%            |

**System Metrics:**

| Metric                | Target    | Alert Threshold |
|-----------------------|-----------|-----------------|
| API Latency (p99)     | <50ms     | >100ms          |
| API QPS               | 100k      | <50k (degraded) |
| Feature Store Latency | <1ms      | >5ms            |
| Cache Hit Rate        | >80%      | <70%            |
| Model Freshness       | <24 hours | >48 hours       |
| Error Rate            | <0.1%     | >1%             |

**Dashboards:**

1. **Real-Time Dashboard** (Grafana)
    - API QPS, latency (p50, p95, p99)
    - Error rate, timeout rate
    - Cache hit rate (Feature Store, Model Store)

2. **Business Dashboard** (Looker/Tableau)
    - Daily CTR, conversion rate
    - A/B test results (algorithm comparison)
    - Revenue impact (attributable to recommendations)

3. **Training Dashboard** (MLflow)
    - Training job status (running, failed, completed)
    - Offline metrics (Precision@10, NDCG@10)
    - Model version, deployment status

### Alerts

**Critical Alerts** (PagerDuty, 24/7 on-call):

- API latency >100ms for 5 minutes â†’ Page on-call engineer
- Error rate >1% for 5 minutes â†’ Page on-call engineer
- Feature Store down (Redis cluster unavailable) â†’ Page on-call engineer

**Warning Alerts** (Slack, email):

- CTR drops >10% compared to yesterday â†’ Alert ML team
- Model training job failed â†’ Alert ML engineers
- Cache hit rate <70% â†’ Alert infrastructure team

---

## 11. Cost Analysis

**Monthly Infrastructure Cost (AWS):**

| Component                  | Configuration                          | Monthly Cost       |
|----------------------------|----------------------------------------|--------------------|
| **Recommendation API**     | 100 instances (c5.xlarge)              | $6,000             |
| **Feature Store (Redis)**  | 10 nodes (r5.xlarge, 500 GB total)     | $5,000             |
| **Kafka Cluster**          | 6 brokers (i3.xlarge)                  | $3,000             |
| **Spark Training Cluster** | 100 nodes (c5.4xlarge, spot instances) | $8,000             |
| **Model Storage (S3)**     | 100 GB (models, embeddings)            | $50                |
| **Training Data (S3)**     | 100 TB (compressed)                    | $2,500             |
| **Load Balancers**         | 3 ALBs                                 | $300               |
| **Total**                  |                                        | **~$24,850/month** |

**Cost per Million Requests:** ~$7.50 (at 100k QPS = 8.6B requests/day)

**Cost Optimization:**

1. Use Spot Instances for training (70% savings) â†’ $8,000 â†’ $2,400
2. Compress training data (Parquet) â†’ 365 TB â†’ 100 TB (3.6x compression)
3. Cache popular recommendations (CDN) â†’ Reduce API QPS by 30%
4. Auto-scale serving instances (scale down during low traffic) â†’ $6,000 â†’ $4,000

**Optimized Cost:** ~$17,250/month

---

## 12. Trade-offs Summary

| What We Gain                              | What We Sacrifice                               |
|-------------------------------------------|-------------------------------------------------|
| âœ… Low latency serving (<50ms)             | âŒ Complex architecture (Lambda with two layers) |
| âœ… High accuracy (collaborative filtering) | âŒ 24-hour model lag (batch training)            |
| âœ… Real-time features (<1 second)          | âŒ Additional infrastructure (Kafka, Redis)      |
| âœ… Scalable (100k QPS)                     | âŒ High cost ($25k/month)                        |
| âœ… Handles cold start (hybrid approach)    | âŒ Model complexity (multiple algorithms)        |
| âœ… A/B testing support                     | âŒ More operational overhead                     |

---

## 13. Real-World Examples

### Netflix

- **Scale:** 200M users, 10k+ recommendations per user per day
- **Architecture:** Lambda (offline training + online serving)
- **Algorithms:** Matrix factorization, deep learning (two-tower), bandits
- **Key Innovation:** Row-based personalization (even thumbnail images personalized)

### Amazon

- **Scale:** 300M users, item-to-item collaborative filtering
- **Architecture:** Batch pre-computation (item-item similarity matrix)
- **Algorithms:** "Customers who bought X also bought Y"
- **Key Innovation:** Real-time updates (purchase â†’ update recommendations within 1 minute)

### YouTube

- **Scale:** 2B users, 1B hours watched daily
- **Architecture:** Two-stage (candidate generation + ranking)
- **Algorithms:** Deep neural networks (watch time prediction)
- **Key Innovation:** Real-time features (watch history in current session)

---

## 14. References

### Technical Papers

- **Netflix:** *Recommender Systems Handbook* (2015)
- **YouTube:** *Deep Neural Networks for YouTube Recommendations* (2016)
- **Amazon:** *Item-to-Item Collaborative Filtering* (2003)

### Related Chapters

- [2.3.5 Batch vs Stream Processing](../../02-components/2.3.5-batch-vs-stream-processing.md) - Lambda Architecture
- [2.2.1 Caching Deep Dive](../../02-components/2.2.1-caching-deep-dive.md) - Feature Store caching
- [3.4.2 Global News Feed](../3.4.2-news-feed/) - Real-time personalization
