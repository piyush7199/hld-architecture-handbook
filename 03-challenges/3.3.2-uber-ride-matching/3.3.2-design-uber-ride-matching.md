# 3.3.2 Design Uber/Lyft Ride Matching System

> üìö **Note on Implementation Details:**
> This document focuses on high-level design concepts and architectural decisions.
> For detailed algorithm implementations, see **[pseudocode.md](./pseudocode.md)**.

## üìä Visual Diagrams & Resources

- **[High-Level Design Diagrams](./hld-diagram.md)** - System architecture, component design, data flow
- **[Sequence Diagrams](./sequence-diagrams.md)** - Detailed interaction flows and failure scenarios
- **[Design Decisions (This Over That)](./this-over-that.md)** - In-depth analysis of architectural choices
- **[Pseudocode Implementations](./pseudocode.md)** - Detailed algorithm implementations

---

## 1. Problem Statement

Design a real-time ride-matching system like Uber or Lyft that connects riders with nearby drivers within seconds. The system must handle millions of concurrent drivers continuously updating their GPS coordinates, support sub-100ms proximity searches to find the nearest available drivers, provide accurate ETAs, handle trip lifecycle management (request ‚Üí match ‚Üí pickup ‚Üí trip ‚Üí payment), and ensure high availability despite network failures and connection drops.

**Core Challenge:** Efficiently index and query millions of moving objects (drivers) in two-dimensional space (latitude/longitude) with sub-second latency while handling 1M+ location updates per second.

---

## 2. Requirements and Scale Estimation

### Functional Requirements (FRs)

| Requirement | Description | Priority |
|-------------|-------------|----------|
| **Driver Location Updates** | Drivers report GPS coordinates every 4 seconds (heartbeat) | Must Have |
| **Proximity Search** | Find N nearest available drivers within radius for a rider | Must Have |
| **Real-Time ETA** | Calculate estimated time of arrival for matched drivers | Must Have |
| **Trip Matching** | Match rider with optimal driver (distance, rating, vehicle type) | Must Have |
| **Trip State Management** | Track trip lifecycle: requested ‚Üí matched ‚Üí pickup ‚Üí active ‚Üí completed | Must Have |
| **Driver Status** | Track driver availability: online, offline, on-trip, break | Must Have |
| **Surge Pricing** | Dynamic pricing based on supply/demand in geohash cells | Should Have |
| **Driver Routing** | Navigate driver to pickup location, then to destination | Should Have |
| **Multi-Ride Types** | Support UberX, UberXL, Uber Black (different vehicle types) | Nice to Have |

### Non-Functional Requirements (NFRs)

| Requirement | Target | Rationale |
|-------------|--------|-----------|
| **Low Latency (Search)** | < 100 ms (p99) | Fast matching prevents rider abandonment |
| **High Availability** | 99.99% uptime | Service downtime = revenue loss |
| **Write Throughput** | 1M updates/sec | 1M drivers √ó 1 update/sec |
| **Read Throughput** | 50K searches/sec | Peak ride requests |
| **Geo-Query Accuracy** | 99.9% | Missed drivers = poor UX |
| **Scalability** | Global, multi-city | Horizontal scaling required |

### Scale Estimation

| Metric | Assumption | Calculation | Result |
|--------|-----------|-------------|--------|
| **Total Drivers (Global)** | 1 Million active drivers | - | 1M drivers |
| **Driver Location Updates** | 1 update per 4 seconds per driver | $1\text{M} / 4 = 250\text{K}$ writes/sec | 250K writes/sec (avg) |
| **Peak Updates** | 3√ó average during rush hour | $250\text{K} \times 3$ | 750K writes/sec (peak) |
| **Concurrent Riders** | 50,000 active riders searching | - | 50K riders |
| **Proximity Searches** | 1 search per rider request | $50\text{K}$ searches/sec | 50K reads/sec |
| **Total Throughput** | Writes + Reads | $750\text{K} + 50\text{K}$ | ~800K ops/sec (peak) |
| **Driver Data per Record** | lat, lng, status, timestamp, driver_id | $40$ bytes/record | 40 bytes |
| **Total Memory (In-Memory Index)** | 1M drivers √ó 40 bytes | $1\text{M} \times 40$ | ~40 MB (fits in RAM) |

**Key Insights:**
- **Write-heavy workload:** 750K writes/sec vs 50K reads/sec (15:1 ratio)
- **In-memory feasible:** 40 MB index fits entirely in RAM (Redis)
- **Low latency critical:** Rider abandonment increases exponentially after 3 seconds
- **Global sharding needed:** Single datacenter can't serve all cities

---

## 3. High-Level Architecture

### System Overview

```
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ      Driver App (Mobile)           ‚îÇ
                    ‚îÇ  GPS: lat=37.7749, lng=-122.4194   ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                  ‚îÇ HTTPS (every 4s)
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ       API Gateway / Load Balancer  ‚îÇ
                    ‚îÇ     (Geo-aware routing by city)    ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                  ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ  Location Ingestion Service        ‚îÇ
                    ‚îÇ  - Validate coordinates            ‚îÇ
                    ‚îÇ  - Publish to Kafka                ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                  ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ        Kafka Cluster               ‚îÇ
                    ‚îÇ   (Buffer 750K updates/sec)        ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚îÇ                ‚îÇ
           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
           ‚îÇ Indexer Workers ‚îÇ      ‚îÇ Driver State Worker‚îÇ
           ‚îÇ (100 consumers) ‚îÇ      ‚îÇ (status, rating)   ‚îÇ
           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ                    ‚îÇ
           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
           ‚îÇ  Redis Cluster (Geo)    ‚îÇ     ‚îÇ
           ‚îÇ  - Geohash Index        ‚îÇ     ‚îÇ
           ‚îÇ  - 20 shards by city    ‚îÇ     ‚îÇ
           ‚îÇ  - GEOADD commands      ‚îÇ     ‚îÇ
           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
                      ‚îÇ                    ‚îÇ
                      ‚îÇ         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                      ‚îÇ         ‚îÇ  Redis (Driver State)   ‚îÇ
                      ‚îÇ         ‚îÇ  - available/busy       ‚îÇ
                      ‚îÇ         ‚îÇ  - rating, vehicle type ‚îÇ
                      ‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ   Rider App (Mobile)       ‚îÇ
        ‚îÇ   "Find me a ride"         ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ HTTPS
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ  Matching Service            ‚îÇ
        ‚îÇ  - GEORADIUS query           ‚îÇ
        ‚îÇ  - Filter by availability    ‚îÇ
        ‚îÇ  - Calculate ETA             ‚îÇ
        ‚îÇ  - Return top 5 drivers      ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ  Trip Management Service     ‚îÇ
        ‚îÇ  - PostgreSQL (ACID)         ‚îÇ
        ‚îÇ  - State: request ‚Üí match    ‚îÇ
        ‚îÇ  - Payment processing        ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Key Components

| Component | Responsibility | Technology | Scalability |
|-----------|---------------|------------|-------------|
| **API Gateway** | Route requests to nearest datacenter | NGINX, Kong | Horizontal + GeoDNS |
| **Location Ingestion** | Validate GPS, publish to Kafka | Go, Java | Horizontal (stateless) |
| **Kafka Cluster** | Buffer 750K writes/sec, decouple producers/consumers | Apache Kafka | Horizontal (partitioned) |
| **Indexer Workers** | Consume from Kafka, update Redis geo-index | Go workers | Horizontal (100+ consumers) |
| **Redis Geo Cluster** | In-memory geohash index for proximity search | Redis with Geo commands | Horizontal (sharded by city) |
| **Driver State Store** | Store driver status, rating, vehicle type | Redis | Horizontal (sharded) |
| **Matching Service** | Find nearest drivers, calculate ETA | Go, Rust | Horizontal (stateless) |
| **Trip Management** | ACID transactions for trip lifecycle | PostgreSQL (sharded) | Horizontal (sharded by city) |
| **ETA Service** | Calculate travel time using road network | GraphHopper, OSRM | Horizontal |

---

## 4. Detailed Component Design

### 4.1 Geo-Spatial Indexing Strategy

**Challenge:** Transform 2D space (latitude, longitude) into a searchable data structure.

**Solution: Geohash Encoding**

Geohash converts a lat/lng pair into a short alphanumeric string. Nearby locations share common prefixes, enabling:
- Single-dimensional index lookups (fast)
- Proximity search by prefix matching
- Efficient range queries

**Geohash Example:**
```
San Francisco: lat=37.7749, lng=-122.4194 ‚Üí geohash = "9q8yy"

Precision levels:
- 3 chars: 9q8 ‚Üí ~156 km √ó 156 km
- 5 chars: 9q8yy ‚Üí ~4.9 km √ó 4.9 km
- 7 chars: 9q8yywe ‚Üí ~153 m √ó 153 m
```

**Why Geohash over H3?**

| Factor | Geohash | H3 (Uber's Hexagonal Grid) |
|--------|---------|----------------------------|
| **Simplicity** | String prefix matching ‚úÖ | Complex hexagonal math ‚ùå |
| **Redis Support** | Native GEOADD, GEORADIUS ‚úÖ | Custom implementation ‚ùå |
| **Boundary Issues** | Yes (edge cases at cell boundaries) ‚ö†Ô∏è | Better (hexagons tile perfectly) ‚úÖ |
| **Adoption** | Widely supported ‚úÖ | Uber-specific, less tooling ‚ùå |

**Decision:** Use **Geohash** for MVP, migrate to **H3** for production at Uber scale.

*See [this-over-that.md: Geohash vs H3](this-over-that.md) for detailed comparison.*

---

### 4.2 Redis Geo Commands

Redis provides native geospatial commands that internally use Geohash.

**Key Operations:**

1. **Add Driver Location:**
   ```
   GEOADD drivers:sf -122.4194 37.7749 "driver:123"
   ```
   - Adds driver to index with coordinates
   - Internally converts to geohash and stores in sorted set
   - Time complexity: O(log N)

2. **Find Nearby Drivers:**
   ```
   GEORADIUS drivers:sf -122.4194 37.7749 5 km WITHDIST COUNT 10
   ```
   - Returns 10 nearest drivers within 5 km radius
   - Sorted by distance
   - Time complexity: O(N log N) where N = drivers in search area

3. **Update Driver Location:**
   ```
   GEOADD drivers:sf -122.4200 37.7755 "driver:123"
   ```
   - Overwrites previous location (upsert behavior)
   - No need to delete old entry

**Data Structure:**
```
Key: drivers:{city_code}
Type: Sorted Set (internally)
Members: driver:{driver_id}
Score: Geohash integer (52-bit)
```

*See [pseudocode.md::update_driver_location()](pseudocode.md) for implementation.*

---

### 4.3 Location Ingestion Pipeline

**Why Kafka?**

**Problem:** 750K location updates/sec overwhelm any synchronous database write path.

**Solution:** Kafka acts as a buffer, absorbing bursts and decoupling producers (drivers) from consumers (indexers).

**Flow:**

1. **Driver App:** POST /location ‚Üí API Gateway
2. **Location Service:** Validate coordinates, publish to Kafka topic "location-updates"
3. **Kafka:** Persist message, replicate to 3 brokers
4. **Indexer Workers:** 100 consumers pull from Kafka, update Redis
5. **Redis:** GEOADD command updates driver position

**Why This Over Synchronous Writes?**

| Approach | Latency (Driver POV) | Database Load | Fault Tolerance |
|----------|---------------------|---------------|-----------------|
| **Synchronous (Direct Redis)** | 50-100ms (waits for DB) | 750K writes/sec directly | Single point of failure ‚ùå |
| **Async (Kafka Buffer)** | 5-10ms (fire-and-forget) ‚úÖ | Workers consume at sustainable rate ‚úÖ | Kafka replication ‚úÖ |

**Trade-off:** Slight delay (200-500ms) between driver movement and index update (acceptable for 4-second update intervals).

*See [sequence-diagrams.md: Location Update Flow](sequence-diagrams.md) for detailed flow.*

---

### 4.4 Matching Service

**Challenge:** Find optimal driver for a rider request within 100ms.

**Algorithm:**

1. **Proximity Search:**
   ```
   GEORADIUS drivers:sf {rider_lat} {rider_lng} 5km WITHDIST COUNT 20
   ```
   - Returns 20 nearest drivers within 5 km
   - Sorted by distance

2. **Filter by Availability:**
   - Query Driver State Store (Redis): `MGET driver:123:status driver:456:status ...`
   - Remove busy/offline drivers

3. **Calculate ETA:**
   - For each available driver, call ETA Service
   - ETA Service uses road network graph (GraphHopper, OSRM)
   - Returns travel time considering traffic

4. **Rank Drivers:**
   - Score = 0.7 √ó distance + 0.2 √ó ETA + 0.1 √ó driver_rating
   - Return top 5 drivers to rider

5. **Send Ride Requests:**
   - Send push notifications to top 5 drivers simultaneously
   - First to accept gets the trip

**Optimization: Smart Radius Expansion**

If < 5 drivers found within 5 km:
- Expand search to 10 km
- If still < 5, expand to 20 km
- Maximum radius: 50 km (prevent excessive searches)

*See [pseudocode.md::find_nearest_drivers()](pseudocode.md) for implementation.*

---

### 4.5 Sharding Strategy

**Problem:** 1M drivers globally can't fit in single Redis instance.

**Solution: Geographic Sharding**

**Approach:**
- Shard by city/region (e.g., `drivers:sf`, `drivers:nyc`, `drivers:london`)
- API Gateway routes requests to correct shard based on rider location
- Each city has dedicated Redis cluster (3-5 nodes)

**Sharding Logic:**
```
1. Rider location: lat=37.7749, lng=-122.4194
2. Reverse geocode to city: "san_francisco"
3. Route to Redis shard: drivers:sf
```

**Benefits:**
- ‚úÖ **Isolation:** SF traffic doesn't impact NYC
- ‚úÖ **Scalability:** Add new cities independently
- ‚úÖ **Latency:** Data co-located with users

**Challenge: Cross-City Trips**
- Rider near city boundary might need drivers from adjacent city
- Solution: Query both shards, merge results

*See [this-over-that.md: Sharding Strategies](this-over-that.md) for alternatives.*

---

### 4.6 Driver State Management

**Challenge:** Geo-index only stores location, not status (available/busy).

**Solution: Separate State Store**

**Data Model (Redis):**
```
Key: driver:{driver_id}:state
Value: {
    "status": "available",  // available, on_trip, offline
    "vehicle_type": "sedan",
    "rating": 4.8,
    "current_trip_id": null,
    "last_update": timestamp
}
TTL: 300 seconds (5 minutes)
```

**Operations:**

1. **Set Driver Available:**
   ```
   HSET driver:123:state status "available" vehicle_type "sedan" rating 4.8
   EXPIRE driver:123:state 300
   ```

2. **Check Availability (Batch):**
   ```
   HMGET driver:123:state driver:456:state driver:789:state
   ```

3. **Update to Busy (on trip match):**
   ```
   HSET driver:123:state status "on_trip" current_trip_id "trip-abc-123"
   ```

**Why Separate from Geo-Index?**

| Aspect | Combined (Single Redis Key) | Separated (Two Keys) |
|--------|---------------------------|---------------------|
| **Update Frequency** | Location: every 4s, Status: on trip change | Optimized per use case ‚úÖ |
| **Query Pattern** | Geo query + status filter mixed | Clean separation ‚úÖ |
| **TTL Management** | Complex (different lifetimes) | Independent TTLs ‚úÖ |

---

### 4.7 Trip State Machine

**States:**
1. **REQUESTED:** Rider submitted request
2. **SEARCHING:** Matching service finding drivers
3. **MATCHED:** Driver accepted, navigating to pickup
4. **ARRIVED:** Driver at pickup location
5. **IN_PROGRESS:** Trip started
6. **COMPLETED:** Trip finished
7. **CANCELLED:** Trip cancelled by rider/driver

**State Transitions:**
```
REQUESTED ‚Üí SEARCHING ‚Üí MATCHED ‚Üí ARRIVED ‚Üí IN_PROGRESS ‚Üí COMPLETED
     ‚Üì          ‚Üì          ‚Üì          ‚Üì           ‚Üì
  CANCELLED  CANCELLED  CANCELLED  CANCELLED  COMPLETED
```

**Database: PostgreSQL (ACID)**

**Why PostgreSQL over NoSQL?**

Trip state involves:
- Payment processing (requires ACID)
- Refunds/cancellation fees (requires transactions)
- Audit trail (requires strong consistency)

**Schema:**
```sql
CREATE TABLE trips (
    trip_id UUID PRIMARY KEY,
    rider_id UUID NOT NULL,
    driver_id UUID,
    status VARCHAR(20) NOT NULL,
    pickup_lat DECIMAL(10, 8),
    pickup_lng DECIMAL(11, 8),
    dropoff_lat DECIMAL(10, 8),
    dropoff_lng DECIMAL(11, 8),
    estimated_fare DECIMAL(10, 2),
    actual_fare DECIMAL(10, 2),
    created_at TIMESTAMP DEFAULT NOW(),
    matched_at TIMESTAMP,
    completed_at TIMESTAMP,
    cancelled_at TIMESTAMP,
    cancellation_reason VARCHAR(255)
);

CREATE INDEX idx_rider_trips ON trips(rider_id, created_at DESC);
CREATE INDEX idx_driver_trips ON trips(driver_id, created_at DESC);
```

*See [pseudocode.md::trip_state_machine()](pseudocode.md) for state transition logic.*

---

### 4.8 ETA Calculation

**Challenge:** Calculate accurate travel time considering:
- Road network (not straight-line distance)
- Traffic conditions
- One-way streets, turn restrictions

**Solution: GraphHopper / OSRM**

**GraphHopper:**
- Open-source routing engine
- Pre-processes OpenStreetMap (OSM) data into road network graph
- Dijkstra's algorithm with A* optimization
- Considers traffic via real-time APIs (Google Traffic, TomTom)

**API Call:**
```
GET /route?point=37.7749,-122.4194&point=37.7849,-122.4094&vehicle=car
‚Üí {
    "distance": 2500 (meters),
    "time": 420 (seconds = 7 minutes),
    "path": [...coordinates...]
}
```

**Caching Strategy:**
- Cache common routes (e.g., airport ‚Üí downtown)
- TTL: 15 minutes (traffic changes)
- Key: `route:{start_geohash}:{end_geohash}:{time_bucket}`

**Fallback (if GraphHopper fails):**
- Haversine distance √ó average city speed
- Example: 2 km / 30 km/h = 4 minutes

*See [pseudocode.md::calculate_eta()](pseudocode.md) for implementation.*

---

## 5. Why This Over That?

### Decision 1: Redis Geo vs PostgreSQL PostGIS

**Chosen:** Redis with GEOADD/GEORADIUS

**Why:**
- ‚úÖ **Speed:** In-memory (sub-millisecond queries)
- ‚úÖ **Throughput:** Handles 750K writes/sec easily
- ‚úÖ **Native Commands:** GEORADIUS built-in
- ‚úÖ **Simplicity:** No complex SQL spatial queries

**Alternatives:**

**PostgreSQL PostGIS:**
- ‚ùå **Slower:** Disk-based, 10-50ms queries
- ‚ùå **Write Bottleneck:** 10K writes/sec max (needs sharding)
- ‚úÖ **ACID:** Better for persistent trip data
- **When to Use:** For historical trip data, not real-time indexing

**MongoDB with Geo Indexes:**
- ‚ö†Ô∏è **Moderate Speed:** 5-20ms queries
- ‚ö†Ô∏è **Write Throughput:** 50K writes/sec (better than Postgres, worse than Redis)
- ‚úÖ **Document Model:** Good for complex driver profiles
- **When to Use:** If driver profiles are complex documents

*See [this-over-that.md: Redis vs PostGIS vs MongoDB](this-over-that.md)*

---

### Decision 2: Kafka vs Direct Database Writes

**Chosen:** Kafka Buffer

**Why:**
- ‚úÖ **Burst Handling:** Absorbs 750K writes/sec spikes
- ‚úÖ **Decoupling:** Drivers don't wait for database writes
- ‚úÖ **Fault Tolerance:** Kafka replication prevents data loss
- ‚úÖ **Replay:** Can reprocess location history

**Alternative: Synchronous Redis Writes:**
- ‚ùå **Latency:** Driver app waits 50-100ms for Redis ACK
- ‚ùå **Overload Risk:** Single Redis failure blocks all writes
- ‚úÖ **Simpler:** No Kafka infrastructure
- **When to Use:** MVP with <10K drivers

---

### Decision 3: Geohash vs H3 Hexagonal Grid

**Chosen:** Geohash (for MVP)

**Why:**
- ‚úÖ **Redis Native:** GEOADD supports Geohash internally
- ‚úÖ **Standard:** Widely adopted, more libraries
- ‚úÖ **Simple:** String prefix matching

**Alternative: H3 (Uber's Choice):**
- ‚úÖ **Better Boundaries:** Hexagons tile perfectly (no edge artifacts)
- ‚úÖ **Uniform Distance:** All neighbors equidistant
- ‚ùå **Custom Implementation:** Redis doesn't support H3 natively
- ‚ùå **Complexity:** Requires custom indexing logic
- **When to Use:** At Uber scale (millions of drivers)

**Uber's Actual Architecture:**
- Started with Geohash
- Migrated to H3 at 1M+ drivers for better accuracy
- Custom H3-optimized database (no Redis)

*See [this-over-that.md: Geohash vs H3 Deep Dive](this-over-that.md)*

---

## 6. Bottlenecks and Future Scaling

### Bottleneck 1: Write Contention on Redis

**Problem:**
- 750K writes/sec concentrated in peak hours
- Single Redis instance: ~100K ops/sec max
- Need 8-10 Redis instances just for writes

**Solutions:**

1. **Geographic Sharding:**
   - Split by city: `drivers:sf`, `drivers:nyc`, etc.
   - Each city: 20K-50K drivers ‚Üí manageable
   - SF: 50K drivers √ó 0.25 writes/sec = 12.5K writes/sec ‚úÖ

2. **Write Batching:**
   - Indexer workers batch 100 GEOADD commands
   - Single pipeline: `GEOADD drivers:sf {100 lat/lng pairs}`
   - Reduces network roundtrips by 100√ó

3. **Read Replicas:**
   - Master: Handles writes
   - 3 Read replicas: Handle GEORADIUS queries
   - Replication lag: <100ms (acceptable)

*See [pseudocode.md::batch_update_locations()](pseudocode.md)*

---

### Bottleneck 2: Geohash Boundary Issues

**Problem:**
- Geohash cells have sharp boundaries
- Driver 10 meters away but in different cell = not found

**Example:**
```
Rider at:    geohash = 9q8yy (SF downtown)
Driver A at: geohash = 9q8yy (same cell, 500m away) ‚úÖ Found
Driver B at: geohash = 9q8yw (adjacent cell, 50m away) ‚ùå Missed!
```

**Solutions:**

1. **Check Adjacent Cells:**
   - Query 9 cells: center + 8 neighbors
   - 9 GEORADIUS queries ‚Üí merge results
   - Increases latency: 10ms ‚Üí 90ms ‚ö†Ô∏è

2. **Larger Search Radius:**
   - Instead of 5 km, search 7 km
   - Covers boundary cases with margin
   - More results to filter (trade-off)

3. **H3 Hexagonal Grid:**
   - Hexagons have no sharp corners
   - Better neighbor coverage
   - Requires custom implementation

**Uber's Solution:** H3 grid eliminates boundary issues.

*See [hld-diagram.md: Boundary Problem Visualization](hld-diagram.md)*

---

### Bottleneck 3: ETA Service Overload

**Problem:**
- Matching service calls ETA service for 20 drivers per search
- 50K searches/sec √ó 20 = 1M ETA requests/sec
- GraphHopper can't handle 1M requests/sec

**Solutions:**

1. **Caching:**
   - Key: `eta:{start_geohash_5}:{end_geohash_5}:{time_bucket_15min}`
   - Hit rate: ~60% (common routes cached)
   - 1M requests ‚Üí 400K cache misses (manageable)

2. **Pre-Computation:**
   - Pre-compute ETA grid for city at 15-minute intervals
   - Store in Redis: `eta_grid:sf:{time_bucket}`
   - Lookup: O(1) instead of routing calculation

3. **Approximation:**
   - Use straight-line distance √ó 1.3 (Manhattan distance factor)
   - Accurate within 20% for most urban areas
   - Fallback when ETA service overloaded

*See [pseudocode.md::calculate_eta_with_cache()](pseudocode.md)*

---

### Bottleneck 4: Cold Start Problem

**Problem:**
- Driver logs in ‚Üí location not in index yet
- First update via Kafka ‚Üí 200-500ms delay
- Rider searches immediately ‚Üí driver not found

**Solutions:**

1. **Synchronous Initial Update:**
   - On login: Driver app ‚Üí POST /location (synchronous)
   - API directly writes to Redis (bypass Kafka)
   - Subsequent updates ‚Üí async via Kafka

2. **Pre-Warm Index:**
   - When driver goes online, mark "status = available" in Driver State
   - Use last known location from database
   - Update with fresh GPS within 4 seconds

3. **Hybrid Approach:**
   - Login: Sync write to Redis (100ms)
   - Background: Also publish to Kafka (for audit trail)
   - Best of both worlds

---

## 7. Common Anti-Patterns

### ‚ùå Anti-Pattern 1: Polling for Nearby Drivers

**Bad Approach:**
```
while (not matched):
    drivers = search_nearby_drivers(rider_location)
    if drivers.length > 0:
        send_ride_request(drivers[0])
    sleep(5 seconds)
```

**Why Bad:**
- Wastes resources (repeated searches)
- 5-second latency = poor UX
- Doesn't scale (50K riders √ó 1 search/5s = 10K searches/sec wasted)

**‚úÖ Best Practice:**
- **Event-Driven:** Rider subscribes to WebSocket
- Matching service pushes driver matches in real-time
- Driver acceptance notification via push

*See [sequence-diagrams.md: Event-Driven Matching](sequence-diagrams.md)*

---

### ‚ùå Anti-Pattern 2: Storing Full Trip History in Redis

**Bad Approach:**
```
GEOADD driver_history:123 {lat} {lng} {timestamp} ‚Üí Store all locations
```

**Why Bad:**
- Redis memory expensive ($1/GB/month)
- 1M drivers √ó 15 trips/day √ó 200 locations/trip = 3B locations/day
- 3B √ó 40 bytes = 120 GB/day = $120/day just for storage

**‚úÖ Best Practice:**
- **Redis:** Only current location (40 MB total)
- **Cassandra:** Historical trip data (cheap storage)
- **S3:** Raw GPS logs for analytics (pennies per GB)

---

### ‚ùå Anti-Pattern 3: No Driver State TTL

**Bad Approach:**
```
SET driver:123:status "available"  // No TTL
```

**Why Bad:**
- Driver goes offline (app crash, network drop)
- Status remains "available" forever
- Matching service sends ride requests to ghost drivers

**‚úÖ Best Practice:**
```
SET driver:123:status "available" EX 300  // 5-minute TTL
Driver heartbeat every 60 seconds refreshes TTL
```

If no heartbeat ‚Üí key expires ‚Üí driver marked offline automatically

---

### ‚ùå Anti-Pattern 4: Single Global Redis Instance

**Bad Approach:**
```
All drivers worldwide ‚Üí single Redis instance "drivers:global"
```

**Why Bad:**
- Latency: US rider queries Redis in EU (150ms network)
- Bottleneck: 750K writes/sec overwhelms single instance
- Failure: Redis down = global outage

**‚úÖ Best Practice:**
- **Geographic Sharding:** `drivers:sf`, `drivers:nyc`, `drivers:london`
- **Regional Deployment:** Redis co-located with users
- **Isolation:** SF outage doesn't affect NYC

---

## 8. Alternative Approaches

### Alternative 1: Elasticsearch for Geo-Search

**Architecture:**
- Replace Redis with Elasticsearch
- Use `geo_distance` query
- Store driver documents with `geo_point` field

**Pros:**
- ‚úÖ **Rich Queries:** Filter by rating, vehicle type, distance simultaneously
- ‚úÖ **Full-Text Search:** Search drivers by name, license plate
- ‚úÖ **Analytics:** Built-in aggregations (surge pricing zones)

**Cons:**
- ‚ùå **Slower Writes:** 10K writes/sec/node (vs Redis 100K)
- ‚ùå **Higher Latency:** 20-50ms queries (vs Redis <5ms)
- ‚ùå **Complex:** Requires cluster management, replication

**When to Use:** If you need rich filtering (e.g., "5-star drivers with SUV within 2 km").

---

### Alternative 2: DynamoDB with Geohashing

**Architecture:**
- Store drivers in DynamoDB
- Partition key: geohash (first 4 chars)
- Sort key: geohash (full) + driver_id

**Pros:**
- ‚úÖ **Serverless:** No infrastructure management
- ‚úÖ **Scalability:** Auto-scales to millions of writes/sec
- ‚úÖ **Durability:** Multi-AZ replication

**Cons:**
- ‚ùå **No Native Geo:** Manual geohash querying (query 9 cells)
- ‚ùå **Cost:** $1.25 per million writes ($937/day for 750K writes/sec)
- ‚ùå **Latency:** 10-30ms (vs Redis <5ms)

**When to Use:** If using AWS ecosystem and willing to pay for serverless simplicity.

---

### Alternative 3: QuadTree In-Memory Structure

**Architecture:**
- Custom in-memory QuadTree
- Each node: Bounding box with driver list
- Recursively split when node exceeds 100 drivers

**Pros:**
- ‚úÖ **Fast:** O(log N) insertion, O(k + log N) query
- ‚úÖ **No External DB:** Pure in-memory
- ‚úÖ **Fine-Tuned:** Optimize for specific use case

**Cons:**
- ‚ùå **Complex:** Implement tree rebalancing, concurrency
- ‚ùå **No Persistence:** Lost on server restart
- ‚ùå **Scaling:** Difficult to shard across nodes

**When to Use:** For embedded systems or specialized hardware (not distributed systems).

---

## 9. Monitoring and Observability

### Critical Metrics

| Metric | Target | Alert Threshold |
|--------|--------|-----------------|
| **Matching Latency (P99)** | < 100ms | > 200ms üî¥ |
| **Driver Location Update Rate** | 750K/sec peak | < 500K/sec üü° |
| **Redis Memory Usage** | < 80% | > 90% üî¥ |
| **Kafka Consumer Lag** | < 1000 msgs | > 10K msgs üî¥ |
| **Match Success Rate** | > 95% | < 90% üî¥ |
| **Driver Availability** | 1M active | < 800K üü° |
| **ETA Accuracy** | ¬±20% | ¬±40% üü° |

### Dashboards

1. **Real-Time Operations:**
   - Active drivers by city (map view)
   - Location updates/sec (line chart)
   - Matching requests/sec (line chart)
   - Average matching latency (histogram)

2. **Geo-Index Health:**
   - Redis memory usage (gauge)
   - GEOADD operations/sec (line chart)
   - GEORADIUS query latency (histogram)
   - Cache hit rate (gauge)

3. **Trip Funnel:**
   - Requests ‚Üí Searches ‚Üí Matches ‚Üí Completions
   - Conversion rate at each stage
   - Drop-off analysis

### Alerts

**üî¥ Critical (Page On-Call):**
- Matching service down (no ride requests processed)
- Redis cluster down (no geo-index)
- Kafka broker failure (location updates lost)
- Match success rate < 90% (insufficient drivers)

**üü° Warning (Investigate Next Day):**
- ETA service latency > 500ms
- Redis memory > 90% (scale up)
- Kafka consumer lag > 10K (add consumers)

*See [hld-diagram.md: Monitoring Dashboard](hld-diagram.md)*

---

## 10. Trade-offs Summary

### What We Gained ‚úÖ

| Benefit | Explanation |
|---------|-------------|
| **Low Latency** | <100ms matching via in-memory Redis geo-index |
| **High Throughput** | 750K location updates/sec via Kafka buffering |
| **Scalability** | Geographic sharding enables horizontal scaling |
| **Availability** | Redis replication + Kafka replication = 99.99% |
| **Accuracy** | Geohash enables precise proximity search |

### What We Sacrificed ‚ùå

| Trade-off | Impact |
|-----------|--------|
| **Eventual Consistency** | 200-500ms delay between driver movement and index update |
| **Complexity** | Kafka + Redis + PostgreSQL = 3 data stores |
| **Cost** | Redis RAM expensive for large fleets ($1/GB/month) |
| **Boundary Issues** | Geohash cells have sharp boundaries (missed nearby drivers) |
| **No Historical Queries** | Redis only stores current location, not trip history |

---

## 11. Real-World Implementations

### Uber

**Architecture:**
- **Geo-Index:** Custom H3 hexagonal grid (not Geohash)
- **Database:** RocksDB (custom key-value store, not Redis)
- **Routing:** Internal "Otto" routing engine (not GraphHopper)
- **Scale:** 5M+ drivers, 100M+ riders, 15M trips/day

**Key Insights:**
- Migrated from Geohash to H3 for better boundary handling
- Custom database (RocksDB) for cost efficiency (Redis too expensive at scale)
- Multi-region deployment (20+ cities, 100+ datacenters)

**Why Custom Tech Stack?**
- Off-the-shelf Redis couldn't handle 5M drivers
- H3 provides 15% better matching accuracy than Geohash
- RocksDB: 10√ó cheaper than Redis for same storage

---

### Lyft

**Architecture:**
- **Geo-Index:** S2 geometry library (Google's spherical indexing)
- **Database:** DynamoDB + Redis hybrid
- **Routing:** Mapbox Directions API
- **Scale:** 2M drivers, 20M+ riders

**Key Insights:**
- Uses S2 cells (similar to H3, but spherical coordinates)
- DynamoDB for durability, Redis for speed
- Heavy use of AWS managed services (less custom infrastructure)

---

### Grab (Southeast Asia)

**Architecture:**
- **Geo-Index:** Redis Geo (Geohash)
- **Database:** MySQL (sharded by city)
- **Routing:** Open-source OSRM
- **Scale:** 9M drivers, 187M users (across 8 countries)

**Key Insights:**
- Stuck with Redis Geo (Geohash) due to cost constraints
- MySQL instead of Cassandra (team expertise)
- OSRM for routing (free, vs Google Maps $7/1000 requests)

---

## 12. Deep Dive: Advanced Topics

### 12.1 Surge Pricing

**Problem:** Too many riders, not enough drivers.

**Solution: Dynamic Pricing by Geohash Cell**

1. **Calculate Supply/Demand Ratio:**
   ```
   For each geohash cell (1 km √ó 1 km):
   available_drivers = COUNT(drivers with status=available in cell)
   pending_requests = COUNT(unmatched riders in cell)
   ratio = available_drivers / pending_requests
   ```

2. **Apply Surge Multiplier:**
   ```
   if ratio < 0.5:  surge = 2.0√ó  (high demand)
   elif ratio < 1.0: surge = 1.5√ó
   else: surge = 1.0√ó (no surge)
   ```

3. **Update Every 2 Minutes:**
   - Background job recalculates surge for all cells
   - Riders see surge pricing before requesting ride

**Data Storage:**
```
Key: surge:{city}:{geohash_5}
Value: {"multiplier": 1.5, "updated_at": timestamp}
TTL: 300 seconds (5 minutes)
```

---

### 12.2 Driver Repositioning

**Problem:** Drivers cluster in downtown, but demand is in suburbs.

**Solution: Incentivize Movement**

1. **Heat Map:** Show drivers where demand is high
2. **Destination Mode:** Driver sets preferred direction (e.g., "heading home")
3. **Bonuses:** "Drive to Airport, get $5 bonus for next trip"

**ML Model:**
- Predict demand by geohash cell for next 30 minutes
- Input: historical trip data, events (concerts, games), weather
- Output: "Probability of ride request in cell X at time T"

---

### 12.3 Multi-Pickup (UberPool)

**Problem:** Multiple riders, similar routes ‚Üí share ride.

**Challenge:** Find riders with overlapping routes in real-time.

**Approach:**
1. **Rider A requests ride:** pickup A ‚Üí dropoff A
2. **Check for compatible riders:**
   - Rider B: pickup B within 500m of A's route, dropoff B within 500m of A's dropoff
3. **Calculate detour:**
   - Original trip A: 10 minutes
   - With pickup B: 13 minutes (30% detour)
   - If detour < 30% ‚Üí match

**Data Structure:**
```
Active pool requests:
[
  {rider_id: A, pickup: {lat, lng}, dropoff: {lat, lng}, route: [...]},
  {rider_id: B, pickup: {lat, lng}, dropoff: {lat, lng}, route: [...]},
]

For new rider C:
  - Check all active requests
  - Calculate route overlap
  - If overlap > 70% ‚Üí suggest pool
```

---

## 13. Interview Discussion Points

### Q1: How Would You Handle 10√ó Traffic Growth?

**Answer:**

**From 1M ‚Üí 10M drivers:**

1. **Redis Sharding:**
   - Current: 20 city shards (50K drivers each)
   - New: 200 city shards (50K drivers each)
   - Geographic + sub-city sharding (e.g., sf-north, sf-south)

2. **Kafka Partitioning:**
   - Current: 100 partitions
   - New: 1000 partitions
   - More consumers (1000 indexer workers)

3. **Regional Datacenters:**
   - Deploy Redis clusters in 50+ cities (vs current 20)
   - Reduce cross-region latency

4. **Cost:**
   - Redis: 40 MB ‚Üí 400 MB (still fits in RAM, $5/month/shard)
   - Kafka: Add brokers (100 ‚Üí 200 brokers)
   - Total infrastructure: $50K/month ‚Üí $500K/month

---

### Q2: What If Redis Goes Down?

**Answer:**

**Failure Scenarios:**

1. **Single Node Failure:**
   - Redis Sentinel detects failure (heartbeat timeout)
   - Promotes replica to master (<5 seconds)
   - Impact: <5 seconds of degraded matching

2. **Entire Shard Failure (e.g., sf shard):**
   - Fallback: Query adjacent cities (oakland, san_jose)
   - Wider search radius (20 km instead of 5 km)
   - Temporary: "Finding drivers, please wait..."

3. **Complete Redis Cluster Failure:**
   - Rebuild index from Kafka replay (last 7 days retained)
   - Time to rebuild: 10 minutes for 1M drivers
   - Downtime: 10 minutes (worst case)

**Mitigation:**
- Multi-AZ Redis deployment (3 nodes across availability zones)
- Cross-region replication (replicate sf shard to nyc as backup)

---

### Q3: How to Prevent Driver Spoofing (Fake GPS)?

**Answer:**

**Attack:** Driver spoofs GPS to appear closer to rider.

**Detection:**

1. **Velocity Check:**
   - If driver moves 10 km in 1 second ‚Üí physically impossible
   - Flag: `driver_behavior:suspicious_velocity`

2. **Accelerometer Data:**
   - Mobile app sends accelerometer readings
   - If GPS shows movement but accelerometer shows stillness ‚Üí spoofing

3. **Cell Tower Triangulation:**
   - Cross-check GPS with cell tower location
   - If mismatch > 1 km ‚Üí suspicious

4. **ML Model:**
   - Train on historical trip patterns
   - Detect anomalies (e.g., driver teleporting)

**Response:**
- Warning: "We detected unusual location activity"
- Suspension: Temporary account lock
- Permanent ban: Repeat offenders

---

### Q4: How to Handle Cross-Border Trips?

**Problem:** Rider in Detroit requests ride to Windsor, Canada.

**Challenges:**
- Different currency (USD ‚Üí CAD)
- Different regulations (insurance, licensing)
- Driver may not have cross-border permit

**Solutions:**

1. **Block Cross-Border by Default:**
   - Geofence: Detect dropoff location in different country
   - Show error: "Cross-border trips not supported"

2. **Special Cross-Border Mode:**
   - Only drivers with permits can accept
   - Filter: `driver:123:permits = ["US", "CA"]`
   - Higher fare (cross-border surcharge)

3. **Hand-Off:**
   - Driver A (US) drives to border
   - Rider transfers to Driver B (CA) at border
   - Two separate trips, two payments

---

### Q5: How to Optimize for Electric Vehicles (EVs)?

**Problem:** EV range limited, need charging station routing.

**Solutions:**

1. **Battery-Aware Matching:**
   - Track driver battery level: `driver:123:battery = 40%`
   - Don't assign long trips (>50 km) to drivers with <50% battery

2. **Charging Station Routing:**
   - If driver battery <20%, suggest route to nearby charging station
   - ETA service considers charging time: "15 min drive + 30 min charge"

3. **Incentives:**
   - "Drive to charging station during off-peak, earn $10 bonus"
   - Prevents EV drivers from running out of charge mid-shift

---

## 14. References and Further Reading

### Internal References

- [Geohash Deep Dive](../../02-components/2.5-algorithms/2.5.5-geohash-indexing.md)
- [Redis Geo Commands](../../02-components/2.1-databases/2.1.11-redis-deep-dive.md)
- [Kafka Streaming](../../02-components/2.3-messaging-streaming/2.3.2-kafka-deep-dive.md)
- [Consistent Hashing](../../02-components/2.2-caching/2.2.2-consistent-hashing.md)
- [PostgreSQL Sharding](../../02-components/2.1-databases/2.1.4-database-scaling.md)

### External References

- **Uber Engineering Blog:** "H3: Uber's Hexagonal Hierarchical Spatial Index"
- **Redis Documentation:** Geo Commands (GEOADD, GEORADIUS)
- **GraphHopper:** Open-source routing engine
- **H3 Library:** https://h3geo.org/
- **OSRM:** Open Source Routing Machine

---

## Summary

**Uber/Lyft Ride Matching** is a geo-spatial indexing problem requiring:

- **750K location updates/sec** buffered via Kafka
- **Redis Geo (Geohash)** for sub-100ms proximity search
- **Geographic sharding** by city for horizontal scaling
- **Separate state store** for driver availability
- **PostgreSQL** for ACID trip transactions
- **ETA service** for accurate travel time calculation

**Key Challenges Solved:**
‚úÖ Low latency matching (<100ms)
‚úÖ High write throughput (750K/sec)
‚úÖ Geo-spatial accuracy (99.9%)
‚úÖ Horizontal scalability (global)
‚úÖ High availability (99.99%)

**Trade-offs Accepted:**
‚ùå Eventual consistency (200-500ms index lag)
‚ùå Geohash boundary issues (H3 solves, but complex)
‚ùå High infrastructure cost ($4-7M/year)
‚ùå Complex multi-store architecture

---

For **visual diagrams**, see [hld-diagram.md](hld-diagram.md) and [sequence-diagrams.md](sequence-diagrams.md).

For **detailed implementations**, see [pseudocode.md](pseudocode.md).

For **design decision analysis**, see [this-over-that.md](this-over-that.md).
