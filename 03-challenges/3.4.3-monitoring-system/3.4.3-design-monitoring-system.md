# 3.4.3 Design a Distributed Monitoring System

> üìö **Note on Implementation Details:**
> This document focuses on high-level design concepts and architectural decisions.
> For detailed algorithm implementations, see **[pseudocode.md](./pseudocode.md)**.

## üìä Visual Diagrams & Resources

- **[High-Level Design Diagrams](./hld-diagram.md)** - System architecture, component design, data flow
- **[Sequence Diagrams](./sequence-diagrams.md)** - Detailed interaction flows and failure scenarios
- **[Design Decisions (This Over That)](./this-over-that.md)** - In-depth analysis of architectural choices
- **[Pseudocode Implementations](./pseudocode.md)** - Detailed algorithm implementations

---

## 1. Problem Statement

Design a distributed monitoring system that collects, stores, and analyzes metrics from millions of endpoints (servers, containers, applications) in real-time. The system must:

- **Collect** metrics (CPU, memory, disk, latency, QPS, error rates) from 1M+ endpoints
- **Store** time-series data efficiently for long-term retention (2+ years)
- **Alert** on anomalies and threshold violations in real-time (<5 seconds)
- **Visualize** metrics through dashboards for historical analysis and troubleshooting
- **Scale** horizontally to handle massive write throughput (1M writes/sec)

**Similar Systems:** Prometheus, Datadog, New Relic, Grafana Cloud, AWS CloudWatch

---

## 2. Requirements and Scale Estimation

### Functional Requirements (FRs)

1. **Metrics Collection**
   - Pull-based collection (scraping endpoints like Prometheus)
   - Push-based collection (agents push metrics)
   - Support for custom metrics and labels
   - Auto-discovery of new endpoints

2. **Time-Series Storage**
   - Store metrics with high resolution (1-second granularity)
   - Support retention policies (rollup and downsampling over time)
   - Efficient compression for long-term storage

3. **Real-Time Alerting**
   - Evaluate alert rules in real-time (e.g., `CPU > 80% for 5 minutes`)
   - Support complex queries (aggregations, percentiles)
   - Trigger notifications (email, Slack, PagerDuty)
   - Alert deduplication and grouping

4. **Visualization**
   - Dashboards with customizable graphs
   - Support for ad-hoc queries
   - Historical data analysis
   - Anomaly detection and forecasting

### Non-Functional Requirements (NFRs)

1. **High Write Throughput:** 1M writes/sec sustained, 5M writes/sec peak
2. **Low Write Latency:** <100ms p99 from metric collection to storage
3. **Fast Query Performance:** <1 second for dashboard queries (last 1 hour)
4. **High Availability:** 99.9% uptime for alerting and query services
5. **Durability:** No data loss for critical metrics (best-effort for non-critical)
6. **Scalability:** Horizontal scaling for both writes and reads

### Scale Estimation

| Metric                    | Assumption                                        | Calculation                                               | Result                                                 |
|---------------------------|---------------------------------------------------|-----------------------------------------------------------|--------------------------------------------------------|
| **Total Endpoints**       | 1M servers/containers                             | -                                                         | 1 Million endpoints                                    |
| **Metrics per Endpoint**  | 100 metrics (CPU, memory, disk, network, custom) | -                                                         | 100 metrics per endpoint                               |
| **Collection Frequency**  | 1 sample per second                               | -                                                         | 1 data point/sec per metric                            |
| **Write Throughput**      | $1\text{M}$ endpoints √ó $100$ metrics √ó $1$/sec   | -                                                         | **100M data points/sec** ($100\text{M}$ $\text{QPS}$) |
| **Storage (1 Day)**       | $100\text{M}$ points/sec √ó $86,400$ sec/day       | -                                                         | 8.64 trillion data points/day                          |
| **Storage (2 Years)**     | $8.64$ trillion √ó $365$ √ó $2$                     | -                                                         | **6.3 quadrillion data points** (petabyte scale)       |
| **Storage Size (Raw)**    | 16 bytes per point (timestamp + value + labels)   | $6.3$ quadrillion √ó $16$ bytes                            | **~100 PB** (before compression)                       |
| **Storage Size (Compressed)** | 10:1 compression ratio                        | $100$ PB / $10$                                           | **~10 PB** (after compression)                         |
| **Query Throughput**      | 100k dashboards √ó 10 queries/min                  | -                                                         | **~16k QPS** (read queries)                            |

**Key Insight:** The system is **write-heavy** (100M writes/sec vs 16k reads/sec). Storage must be optimized for sequential writes and time-based compression.

---

## 3. High-Level Architecture

The architecture follows a classic **time-series data pipeline** optimized for high write throughput and efficient time-based queries.

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                          MONITORED INFRASTRUCTURE                           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ
‚îÇ  ‚îÇ Server 1‚îÇ ‚îÇ Server 2‚îÇ ‚îÇContainer‚îÇ ‚îÇ   App   ‚îÇ   ...   ‚îÇ Server  ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ (1M endpoints)        ‚îÇ         ‚îÇ         ‚îÇ         ‚îÇ   1M    ‚îÇ      ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ           ‚îÇ           ‚îÇ           ‚îÇ                  ‚îÇ
        ‚îÇ scrape    ‚îÇ scrape    ‚îÇ push      ‚îÇ push             ‚îÇ scrape
        ‚îÇ (pull)    ‚îÇ (pull)    ‚îÇ (agent)   ‚îÇ (SDK)            ‚îÇ (pull)
        ‚ñº           ‚ñº           ‚ñº           ‚ñº                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                          COLLECTION LAYER                                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îÇ
‚îÇ  ‚îÇ Collector 1     ‚îÇ ‚îÇ Collector 2     ‚îÇ ‚îÇ Collector N     ‚îÇ              ‚îÇ
‚îÇ  ‚îÇ (Prometheus)    ‚îÇ ‚îÇ (Prometheus)    ‚îÇ ‚îÇ (100 instances) ‚îÇ              ‚îÇ
‚îÇ  ‚îÇ 1M metrics/sec  ‚îÇ ‚îÇ 1M metrics/sec  ‚îÇ ‚îÇ                 ‚îÇ              ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ                   ‚îÇ                   ‚îÇ
            ‚îÇ write             ‚îÇ write             ‚îÇ write
            ‚ñº                   ‚ñº                   ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                            BUFFER LAYER                                     ‚îÇ
‚îÇ                         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                 ‚îÇ
‚îÇ                         ‚îÇ  Kafka Cluster  ‚îÇ                                 ‚îÇ
‚îÇ                         ‚îÇ  (Message Queue)‚îÇ                                 ‚îÇ
‚îÇ                         ‚îÇ  100M msgs/sec  ‚îÇ                                 ‚îÇ
‚îÇ                         ‚îÇ  24 partitions  ‚îÇ                                 ‚îÇ
‚îÇ                         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                   ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ                         ‚îÇ                         ‚îÇ
         ‚ñº                         ‚ñº                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ ALERTING       ‚îÇ      ‚îÇ STORAGE            ‚îÇ      ‚îÇ REAL-TIME        ‚îÇ
‚îÇ ENGINE         ‚îÇ      ‚îÇ (TSDB)             ‚îÇ      ‚îÇ AGGREGATION      ‚îÇ
‚îÇ (Flink/Streams)‚îÇ      ‚îÇ                    ‚îÇ      ‚îÇ (Stream Processor‚îÇ
‚îÇ                ‚îÇ      ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ      ‚îÇ                  ‚îÇ
‚îÇ - Evaluate     ‚îÇ      ‚îÇ ‚îÇ InfluxDB/M3DB  ‚îÇ ‚îÇ      ‚îÇ - Downsampling   ‚îÇ
‚îÇ   rules        ‚îÇ      ‚îÇ ‚îÇ Cassandra+TSDB ‚îÇ ‚îÇ      ‚îÇ - Rollups        ‚îÇ
‚îÇ - Windowed     ‚îÇ      ‚îÇ ‚îÇ                ‚îÇ ‚îÇ      ‚îÇ                  ‚îÇ
‚îÇ   aggregations ‚îÇ      ‚îÇ ‚îÇ Sharded by:    ‚îÇ ‚îÇ      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îÇ - Trigger      ‚îÇ      ‚îÇ ‚îÇ - Time         ‚îÇ ‚îÇ             ‚îÇ
‚îÇ   alerts       ‚îÇ      ‚îÇ ‚îÇ - Metric name  ‚îÇ ‚îÇ             ‚îÇ write
‚îÇ                ‚îÇ      ‚îÇ ‚îÇ                ‚îÇ ‚îÇ             ‚îÇ (rollups)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ ‚îÇ 10 PB storage  ‚îÇ ‚îÇ             ‚îÇ
         ‚îÇ              ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ             ‚îÇ
         ‚îÇ notify       ‚îÇ          ‚îÇ         ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚ñº              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ
‚îÇ NOTIFICATION   ‚îÇ                ‚îÇ read queries
‚îÇ SERVICE        ‚îÇ                ‚îÇ
‚îÇ - Email        ‚îÇ                ‚ñº
‚îÇ - Slack        ‚îÇ      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ - PagerDuty    ‚îÇ      ‚îÇ QUERY SERVICE      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ (API Layer)        ‚îÇ
                        ‚îÇ                    ‚îÇ
                        ‚îÇ - Query optimizer  ‚îÇ
                        ‚îÇ - Cache layer      ‚îÇ
                        ‚îÇ   (Redis)          ‚îÇ
                        ‚îÇ - Query result     ‚îÇ
                        ‚îÇ   cache            ‚îÇ
                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                   ‚îÇ
                                   ‚îÇ HTTP API
                                   ‚ñº
                        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                        ‚îÇ VISUALIZATION      ‚îÇ
                        ‚îÇ (Grafana)          ‚îÇ
                        ‚îÇ                    ‚îÇ
                        ‚îÇ - Dashboards       ‚îÇ
                        ‚îÇ - Ad-hoc queries   ‚îÇ
                        ‚îÇ - Alerting UI      ‚îÇ
                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Key Components:**

1. **Collectors (Prometheus/Agents)** - Scrape or receive metrics from endpoints
2. **Kafka Buffer** - Decouples collectors from storage, handles write bursts
3. **Time-Series Database (TSDB)** - Optimized storage (InfluxDB, M3DB, Cassandra)
4. **Alerting Engine** - Real-time rule evaluation on Kafka stream
5. **Real-Time Aggregation** - Downsampling and rollups for long-term storage
6. **Query Service** - API layer with caching for dashboard queries
7. **Notification Service** - Sends alerts to external systems
8. **Grafana** - Visualization and dashboard UI

---

## 4. Data Model

### 4.1 Time-Series Data Structure

Time-series data consists of:
- **Metric name** (e.g., `cpu_usage`, `http_request_latency`)
- **Timestamp** (Unix epoch, millisecond precision)
- **Value** (numeric: float, int)
- **Labels/Tags** (key-value pairs for dimensions)

**Example:**
```
Metric: http_request_latency_ms
Timestamp: 1698765432000
Value: 234.5
Labels: {
  service: "api-gateway",
  method: "POST",
  endpoint: "/users",
  region: "us-east-1",
  status_code: "200"
}
```

**Storage Format (Prometheus-style):**
```
http_request_latency_ms{service="api-gateway",method="POST",endpoint="/users",region="us-east-1",status_code="200"} 234.5 1698765432000
```

### 4.2 TSDB Schema (Cassandra Example)

**Table 1: Raw Metrics (High-Resolution)**
```sql
CREATE TABLE metrics_raw (
    metric_name text,           -- e.g., cpu_usage
    labels map<text, text>,     -- {host: "server-1", region: "us-east-1"}
    timestamp timestamp,        -- Time of measurement
    value double,               -- Metric value
    PRIMARY KEY ((metric_name, labels), timestamp)
) WITH CLUSTERING ORDER BY (timestamp DESC)
  AND compaction = {'class': 'TimeWindowCompactionStrategy'};
```

**Partitioning Strategy:** Partition by `(metric_name, labels)` to group all data points for a specific metric+label combination. Cluster by `timestamp DESC` for efficient time-range queries.

**Table 2: Rollup Metrics (Downsampled)**
```sql
CREATE TABLE metrics_rollup_1min (
    metric_name text,
    labels map<text, text>,
    timestamp_bucket timestamp,  -- Rounded to 1-minute boundary
    count bigint,                -- Number of samples in window
    sum double,                  -- Sum of values
    min double,                  -- Min value
    max double,                  -- Max value
    avg double,                  -- Average value
    p50 double,                  -- 50th percentile
    p95 double,                  -- 95th percentile
    p99 double,                  -- 99th percentile
    PRIMARY KEY ((metric_name, labels), timestamp_bucket)
) WITH CLUSTERING ORDER BY (timestamp_bucket DESC);
```

**Rollup Hierarchy:**
- **Raw** (1-second resolution) ‚Üí Retained for 1 day
- **1-minute rollup** ‚Üí Retained for 7 days
- **5-minute rollup** ‚Üí Retained for 30 days
- **1-hour rollup** ‚Üí Retained for 1 year
- **1-day rollup** ‚Üí Retained for 2+ years

---

## 5. Detailed Component Design

### 5.1 Metrics Collection

**Pull-Based Collection (Prometheus Model):**
1. **Service Discovery** - Automatically discover endpoints (Kubernetes pods, AWS instances)
2. **Scraping** - HTTP GET to `/metrics` endpoint every 1 second
3. **Parsing** - Parse Prometheus text format (metric name, labels, value, timestamp)
4. **Buffering** - Write to local buffer before sending to Kafka

**Push-Based Collection (Agent Model):**
1. **Agent Installation** - Deploy agent on each endpoint (e.g., Telegraf, Datadog agent)
2. **Metric Collection** - Agent collects system metrics (CPU, memory, disk, network)
3. **Push to Collector** - HTTP POST to collector endpoint
4. **Batching** - Agent batches metrics (100 points) before sending

**Comparison:**

| Aspect | Pull-Based (Prometheus) | Push-Based (Agent) |
|--------|-------------------------|---------------------|
| **Discovery** | Centralized (scraper knows endpoints) | Distributed (agents register) |
| **Network** | Collector initiates connection | Endpoint initiates connection |
| **Firewall** | Requires inbound access to endpoints | Requires outbound access only |
| **Failure Handling** | Scraper detects endpoint down | Endpoint can retry |
| **Scale** | Collector bottleneck (horizontal scaling) | Endpoints self-managing |

**Decision:** Use **hybrid approach** - pull for internal services, push for external/edge devices.

### 5.2 Kafka Buffer Layer

**Why Kafka?**
- **Decouples** collectors from storage (if TSDB is slow, Kafka absorbs writes)
- **Replay** capability for reprocessing (e.g., backfill rollups)
- **Durability** - Data persisted to disk before acknowledgment
- **Partitioning** - Distributes load across TSDB writers

**Kafka Topic Configuration:**
```
Topic: metrics.raw
Partitions: 24 (for 100M msgs/sec throughput)
Replication factor: 3 (durability)
Retention: 24 hours (enough for retry/replay)
Compression: snappy (reduces storage by 5x)
```

**Partitioning Strategy:** Partition by `hash(metric_name + labels)` to ensure all data points for a specific time series go to the same partition (maintains order).

**Consumer Groups:**
1. **TSDB Writers** - Consume and write to time-series database
2. **Alerting Engine** - Consume for real-time alert evaluation
3. **Real-Time Aggregation** - Consume for downsampling and rollups

### 5.3 Time-Series Database (TSDB)

**TSDB Selection Criteria:**
- High write throughput (100M writes/sec)
- Efficient time-range queries
- Built-in compression
- Support for retention policies and downsampling

**Option 1: InfluxDB (Specialized TSDB)**
- **Pros:** Purpose-built for time-series, excellent compression, fast time-range queries
- **Cons:** Horizontal scaling is limited (InfluxDB Enterprise required), expensive

**Option 2: M3DB (Uber's TSDB)**
- **Pros:** Designed for scale (handles billions of writes/sec), Prometheus-compatible
- **Cons:** Complex to operate, relatively new (smaller community)

**Option 3: Cassandra + Custom TSDB Layer**
- **Pros:** Proven scalability, flexible schema, strong consistency options
- **Cons:** Not optimized for time-series out-of-the-box (requires custom compression/indexing)

**Decision:** **M3DB** for write-heavy workloads (100M writes/sec) with Prometheus compatibility. *See [this-over-that.md](./this-over-that.md) for detailed analysis.*

**M3DB Architecture:**
- **M3DB Nodes** - Distributed storage nodes (50+ nodes)
- **M3Coordinator** - Query aggregator and downsampler
- **M3Aggregator** - Real-time aggregation and rollups
- **M3Query** - PromQL-compatible query engine

**Sharding Strategy:**
- **Shard by metric name + labels** (consistent hashing)
- **Replication factor:** 3 (for durability)
- **Shard count:** 4096 (for fine-grained distribution)

**Compression:**
- **Gorilla compression** (Facebook's time-series compression algorithm)
- **Compression ratio:** 10:1 (100 PB ‚Üí 10 PB)
- **Delta-of-delta encoding** for timestamps
- **XOR encoding** for float values

### 5.4 Alerting Engine

**Requirements:**
- Evaluate alert rules in real-time (<5 seconds)
- Support complex queries (aggregations, percentiles, windows)
- Handle stateful queries (e.g., "CPU > 80% for 5 minutes")
- Deduplicate and group alerts

**Architecture:**

**Stream Processor:** Apache Flink or Kafka Streams
- **Input:** Kafka topic `metrics.raw`
- **Processing:** Stateful windowed aggregations
- **Output:** Kafka topic `alerts.triggered`

**Alert Rule Example:**
```
Alert: HighCPUUsage
Condition: avg(cpu_usage{service="api-gateway"}) > 80 for 5 minutes
Severity: warning
Notification: slack-channel=#alerts
```

**Processing Flow:**
1. **Consume metrics** from Kafka
2. **Group by alert rule** (e.g., all `cpu_usage` metrics for `api-gateway`)
3. **Windowed aggregation** (5-minute tumbling window)
4. **Evaluate condition** (avg > 80%)
5. **Trigger alert** if condition met for entire window
6. **Deduplicate** (don't send alert if already firing)
7. **Publish** to `alerts.triggered` topic

**Stateful Processing:**
- **State store:** RocksDB (embedded key-value store)
- **State:** Current window aggregations (count, sum, min, max)
- **Checkpointing:** Periodic snapshots for fault tolerance

**Alert Deduplication:**
- Track alert state (firing, resolved)
- Don't send duplicate alerts if already firing
- Send "resolved" notification when condition clears

**Alert Grouping:**
- Group alerts by labels (e.g., all alerts for `region=us-east-1`)
- Single notification for multiple related alerts
- Reduces notification spam

### 5.5 Real-Time Aggregation (Rollups)

**Why Rollups?**
- Storing 6.3 quadrillion data points at 1-second resolution is too expensive
- Dashboard queries over long time ranges (e.g., "last 30 days") would be slow
- Downsampling reduces storage and improves query performance

**Rollup Strategy:**

**Retention Policy:**
- **Raw (1-second):** 1 day (8.64 trillion points)
- **1-minute rollup:** 7 days (1.4 trillion points)
- **5-minute rollup:** 30 days (8.6 billion points)
- **1-hour rollup:** 1 year (876 million points)
- **1-day rollup:** 2+ years (730,000 points per metric)

**Rollup Computation:**
- **Stream processor** (Flink or Kafka Streams)
- **Input:** Kafka topic `metrics.raw`
- **Processing:** Windowed aggregations (1-minute, 5-minute, 1-hour, 1-day)
- **Output:** Write to TSDB rollup tables

**Aggregation Functions:**
- `count` - Number of samples in window
- `sum` - Sum of values
- `min`, `max` - Min/max values
- `avg` - Average value
- `p50`, `p95`, `p99` - Percentiles

**Example Rollup:**
```
Raw data (1-second):
cpu_usage{host="server-1"} 65.2 @ 12:00:00
cpu_usage{host="server-1"} 67.5 @ 12:00:01
cpu_usage{host="server-1"} 70.1 @ 12:00:02
...
cpu_usage{host="server-1"} 72.3 @ 12:00:59

1-minute rollup (12:00:00 - 12:00:59):
cpu_usage_1min{host="server-1"} {
  count: 60,
  sum: 4080.5,
  min: 65.2,
  max: 75.1,
  avg: 68.0,
  p50: 67.8,
  p95: 74.2,
  p99: 75.0
} @ 12:00:00
```

### 5.6 Query Service

**Requirements:**
- Fast query performance (<1 second for dashboard queries)
- Support for complex queries (aggregations, joins, percentiles)
- Caching for frequently accessed data
- Query optimization (push down filters, use rollups)

**Architecture:**

**Query API Layer:**
- **Protocol:** HTTP REST API + PromQL support
- **Query optimizer:** Analyzes query and determines optimal execution plan
- **Cache layer:** Redis for query result caching
- **Rate limiting:** Prevent expensive queries from overloading TSDB

**Query Optimization:**

1. **Automatic Rollup Selection**
   - Query: "Show CPU usage for last 30 days"
   - Optimizer: Use 1-hour rollup (not raw data)
   - Benefit: 3600x fewer data points to fetch

2. **Predicate Pushdown**
   - Push filters down to TSDB (not in application layer)
   - Example: Filter by `region=us-east-1` at storage level

3. **Query Result Caching**
   - Cache key: `query_hash + time_range`
   - TTL: 1 minute (for recent data), 1 hour (for old data)
   - Invalidation: On alert trigger (stale data)

4. **Query Parallelization**
   - Split query into sub-queries (per shard)
   - Execute in parallel
   - Merge results

**Caching Strategy:**

**Cache Layer 1: Query Result Cache (Redis)**
- Key: `hash(query + time_range)`
- Value: Serialized query result
- TTL: 1 minute (for last hour), 1 hour (for older data)
- Hit rate: 80% (popular dashboards)

**Cache Layer 2: TSDB Block Cache (In-Memory)**
- Cache recently accessed TSDB blocks (1-hour chunks)
- LRU eviction policy
- Size: 100 GB per TSDB node

---

## 6. Bottlenecks and Scaling Strategies

### 6.1 Write Path Bottlenecks

**Bottleneck 1: Kafka Write Throughput**

**Problem:** 100M writes/sec requires high Kafka throughput (sustained 200 MB/s per partition).

**Solution:**
- **Increase partitions:** 24 ‚Üí 48 partitions (distribute load)
- **Batching:** Collectors batch 1000 metrics before sending to Kafka
- **Compression:** Use snappy compression (5x reduction)
- **Async producers:** Non-blocking writes

**Performance:**
- Before: 100M writes/sec, 200 MB/s per partition (at limit)
- After: 100M writes/sec, 100 MB/s per partition (comfortable headroom)

**Bottleneck 2: TSDB Write Throughput**

**Problem:** TSDB struggles with random writes (even though time-series data is mostly sequential, labels create randomness).

**Solution:**
- **Write buffer (M3DB aggregator):** Buffer writes in memory (1 minute), batch to disk
- **LSM-Tree storage engine:** Optimized for write-heavy workloads
- **Compression:** Gorilla compression reduces disk writes by 10x
- **Horizontal scaling:** Add more TSDB nodes (50+ nodes)

**Performance:**
- Single node: 1M writes/sec
- 100 nodes: 100M writes/sec (linear scaling)

### 6.2 Read Path Bottlenecks

**Bottleneck 3: Dashboard Query Latency**

**Problem:** Dashboard queries over long time ranges (e.g., "last 30 days") fetch millions of data points (slow).

**Solution:**
- **Automatic rollup selection:** Query optimizer uses 1-hour rollup (3600x fewer points)
- **Query result caching:** Redis cache with 80% hit rate
- **Downsampling at query time:** If query asks for 1000 points over 30 days, downsample to 1000 points (not millions)

**Performance:**
- Before: 30-day query = 2.6 billion points, 10 seconds
- After: 30-day query (1-hour rollup) = 720 points, 100ms

**Bottleneck 4: Ad-Hoc Query Overload**

**Problem:** Expensive ad-hoc queries (e.g., "Show p99 latency for all services over last year") can overwhelm TSDB.

**Solution:**
- **Query rate limiting:** Max 10 concurrent queries per user
- **Query timeout:** Kill queries running >30 seconds
- **Query cost estimation:** Warn users before running expensive queries
- **Dedicated query pool:** Separate TSDB replicas for ad-hoc queries (don't impact dashboards)

### 6.3 Storage Bottlenecks

**Bottleneck 5: Storage Capacity**

**Problem:** 10 PB storage (after compression) is expensive and hard to manage.

**Solution:**
- **Tiered storage:** Hot (SSD), warm (HDD), cold (S3/Glacier)
  - **Hot tier (0-7 days):** SSD, fast queries
  - **Warm tier (7-90 days):** HDD, slower queries
  - **Cold tier (90+ days):** S3/Glacier, archive only
- **Aggressive rollups:** Only store 1-day rollups for data >1 year old
- **Selective retention:** Critical metrics retained longer, non-critical metrics deleted after 30 days

**Cost Analysis:**
```
Hot tier (7 days): 800 TB √ó $0.10/GB-month = $80k/month
Warm tier (83 days): 8 PB √ó $0.02/GB-month = $160k/month
Cold tier (2 years): 1 PB √ó $0.004/GB-month = $4k/month

Total storage cost: $244k/month
```

---

## 7. Multi-Region Deployment

**Requirements:**
- Low-latency metric collection (collect in region where endpoint is located)
- Global view of metrics (aggregate across regions)
- Region failover (if one region fails, others continue)

**Architecture:**

**Regional Deployment:**
- Each region has full stack: Collectors ‚Üí Kafka ‚Üí TSDB ‚Üí Query Service
- Metrics collected locally (low latency)
- Regional dashboards for local troubleshooting

**Global Aggregation:**
- **Cross-region replication:** Kafka MirrorMaker replicates metrics to central region
- **Global TSDB:** Aggregates metrics from all regions
- **Global dashboards:** View metrics across all regions

**Trade-offs:**
- **Eventual consistency:** Global metrics lag behind regional metrics by 5-30 seconds
- **Higher cost:** 3x infrastructure (3 regions)
- **Complexity:** Cross-region networking and replication

---

## 8. Common Anti-Patterns

### ‚ùå **Anti-Pattern 1: Using RDBMS for Time-Series Data**

**Problem:** Using PostgreSQL/MySQL to store time-series metrics.

**Why It's Bad:**
- RDBMS not optimized for high write throughput (B-Tree indexes, write amplification)
- Time-range queries slow (requires index scan)
- No built-in compression or retention policies
- Disk usage explodes (no downsampling)

**Solution:**
‚úÖ Use specialized TSDB (InfluxDB, M3DB, TimescaleDB)

### ‚ùå **Anti-Pattern 2: No Rollups/Downsampling**

**Problem:** Storing all metrics at full resolution (1-second) forever.

**Why It's Bad:**
- Storage cost explodes (100 PB for 2 years)
- Dashboard queries over long time ranges are slow (fetching billions of points)
- Most queries don't need 1-second resolution for historical data

**Solution:**
‚úÖ Implement aggressive rollups (1-minute, 1-hour, 1-day) with retention policies

### ‚ùå **Anti-Pattern 3: Synchronous Alert Evaluation on TSDB Queries**

**Problem:** Alerting engine queries TSDB every 10 seconds to evaluate alert rules.

**Why It's Bad:**
- High query load on TSDB (10k alert rules √ó 6 queries/min = 1M queries/min)
- Alert latency high (10+ seconds)
- TSDB query load competes with dashboard queries

**Solution:**
‚úÖ Use stream processing (Flink/Kafka Streams) to evaluate alerts on Kafka stream in real-time

### ‚ùå **Anti-Pattern 4: No Cardinality Control**

**Problem:** Allowing unlimited unique label combinations (high cardinality).

**Why It's Bad:**
- Each unique label combination creates a new time series
- Example: `request_id` label with 1 billion unique values = 1 billion time series
- TSDB memory and storage explodes
- Queries become slow (needs to scan millions of time series)

**Solution:**
‚úÖ Limit label cardinality:
- Don't use high-cardinality labels (e.g., `user_id`, `request_id`)
- Enforce label whitelist (only allow approved labels)
- Monitor cardinality growth and alert

### ‚ùå **Anti-Pattern 5: No Query Cost Control**

**Problem:** Allowing users to run expensive ad-hoc queries without limits.

**Why It's Bad:**
- A single expensive query can overload TSDB (e.g., "Show all metrics for all services over last year")
- Impacts all users (dashboard queries slow)
- TSDB nodes crash due to memory exhaustion

**Solution:**
‚úÖ Implement query cost control:
- **Query timeout:** Kill queries running >30 seconds
- **Query rate limiting:** Max 10 concurrent queries per user
- **Query cost estimation:** Warn users before running expensive queries
- **Dedicated query pool:** Separate TSDB replicas for ad-hoc queries

---

## 9. Alternative Approaches

### Approach 1: Serverless Monitoring (AWS CloudWatch Model)

**Architecture:**
- Managed service (no infrastructure to manage)
- Push-based collection (agents push metrics to CloudWatch)
- Pay-per-use pricing (per metric, per query)

**Pros:**
- ‚úÖ No operational overhead (fully managed)
- ‚úÖ Scales automatically (no capacity planning)
- ‚úÖ Integrated with cloud services (AWS, GCP, Azure)

**Cons:**
- ‚ùå Expensive at scale (100M metrics/sec = $500k/month)
- ‚ùå Vendor lock-in (can't migrate easily)
- ‚ùå Limited customization (fixed retention, limited query capabilities)
- ‚ùå Higher latency (shared infrastructure)

**When to use:** Small to medium scale (<1M metrics/sec), cloud-native apps, no ops team.

### Approach 2: Prometheus + Thanos (Prometheus Long-Term Storage)

**Architecture:**
- Prometheus for collection and short-term storage (2 weeks)
- Thanos for long-term storage (S3/GCS) and global view
- Sidecar architecture (Thanos sidecar uploads data to object storage)

**Pros:**
- ‚úÖ Cost-effective (S3 storage is cheap: $0.023/GB-month)
- ‚úÖ Unlimited retention (store data forever)
- ‚úÖ Prometheus-compatible (same query language)
- ‚úÖ Multi-cluster aggregation (global view)

**Cons:**
- ‚ùå Slower queries (fetching from S3 is slower than local disk)
- ‚ùå Complex architecture (multiple components: Prometheus, Thanos Sidecar, Thanos Query, Thanos Store)
- ‚ùå Eventual consistency (Thanos lags behind Prometheus by minutes)

**When to use:** Medium scale (1-10M metrics/sec), need long-term retention (>1 year), cost-sensitive.

### Approach 3: VictoriaMetrics (High-Performance TSDB)

**Architecture:**
- Single-binary TSDB (simpler than M3DB)
- Prometheus-compatible (drop-in replacement)
- High compression (10-50x better than Prometheus)

**Pros:**
- ‚úÖ Extremely cost-effective (10x cheaper storage than Prometheus)
- ‚úÖ Fast queries (optimized query engine)
- ‚úÖ Simple to operate (single binary, no dependencies)
- ‚úÖ High write throughput (1M writes/sec per node)

**Cons:**
- ‚ùå Smaller community (less mature than Prometheus)
- ‚ùå Single-vendor (no other implementations)
- ‚ùå Limited horizontal scaling (clustering is complex)

**When to use:** Cost-sensitive deployments, need high compression, simple architecture preferred.

---

## 10. Monitoring the Monitoring System (Meta-Monitoring)

**Challenge:** How do you monitor the monitoring system itself?

**Solution: Dual-Layered Monitoring**

**Layer 1: Self-Monitoring (Monitoring System Monitors Itself)**
- Collectors emit metrics about themselves (scrape duration, errors)
- Kafka emits metrics (consumer lag, partition rebalancing)
- TSDB emits metrics (write throughput, query latency, disk usage)
- Alerting engine emits metrics (alert evaluation time, alerts triggered)

**Layer 2: External Monitoring (Separate System)**
- Deploy lightweight external monitoring system (e.g., Prometheus on separate infrastructure)
- Monitor critical metrics of primary monitoring system
- Alert if primary monitoring system is down

**Key Metrics to Monitor:**

| Component | Metric | Alert Threshold |
|-----------|--------|-----------------|
| **Collectors** | `scrape_duration_seconds` | >5 seconds |
| **Collectors** | `scrape_errors_total` | >10 errors/min |
| **Kafka** | `consumer_lag` | >1M messages |
| **Kafka** | `broker_disk_usage_pct` | >80% |
| **TSDB** | `write_throughput_qps` | <50M QPS (expected 100M) |
| **TSDB** | `query_latency_p99_ms` | >5000ms |
| **TSDB** | `disk_usage_pct` | >90% |
| **Alerting Engine** | `alert_evaluation_duration_ms` | >10 seconds |
| **Query Service** | `cache_hit_rate_pct` | <50% |

---

## 11. Trade-offs Summary

| What We Gain | What We Sacrifice |
|--------------|-------------------|
| ‚úÖ **High write throughput** (100M writes/sec) | ‚ùå Expensive infrastructure (100+ nodes, 10 PB storage) |
| ‚úÖ **Real-time alerting** (<5 seconds) | ‚ùå Complex stream processing (stateful Flink jobs) |
| ‚úÖ **Long-term retention** (2+ years) | ‚ùå Aggressive rollups (lose granularity for old data) |
| ‚úÖ **Fast dashboard queries** (<1 second) | ‚ùå Eventual consistency (rollups lag behind raw data) |
| ‚úÖ **Horizontal scalability** (add nodes) | ‚ùå Operational complexity (manage 100+ nodes) |
| ‚úÖ **Multi-region deployment** (low latency) | ‚ùå 3x infrastructure cost |
| ‚úÖ **Automatic downsampling** (reduce storage) | ‚ùå Cannot reconstruct original raw data |

---

## 12. Real-World Examples

### Uber's M3 Monitoring System

**Scale:**
- **Endpoints:** 10M+ metrics/sec from 100k+ hosts
- **Storage:** 100+ PB of metrics data
- **Queries:** 1M+ queries/day
- **Alerts:** 100k+ alerts/day

**Architecture:**
- **Collection:** Pull-based (Prometheus-compatible)
- **Storage:** M3DB (custom TSDB built by Uber)
- **Alerting:** M3Query + custom alerting engine
- **Visualization:** Grafana

**Key Insights:**
- Built M3DB because existing TSDB couldn't scale to 10M writes/sec
- Aggressive compression (Gorilla + downsampling) ‚Üí 10:1 ratio
- Multi-region deployment with 5 seconds replication lag

### Datadog (SaaS Monitoring Platform)

**Scale:**
- **Customers:** 25k+ companies
- **Metrics:** 500 billion metrics/day
- **Storage:** Multi-exabyte scale
- **Queries:** 10M+ queries/day

**Architecture:**
- **Collection:** Push-based (Datadog agent)
- **Storage:** Custom TSDB (proprietary)
- **Alerting:** Real-time stream processing
- **Visualization:** Custom web UI

**Key Insights:**
- Multi-tenancy (data isolation per customer)
- Tiered storage (hot SSD, warm HDD, cold S3)
- Dynamic rollups based on query patterns (optimize for frequently queried metrics)

### Netflix's Atlas Monitoring System

**Scale:**
- **Metrics:** 2 billion metrics/minute
- **Endpoints:** 100k+ instances
- **Storage:** Cassandra (1 PB)
- **Queries:** 1M+ queries/day

**Architecture:**
- **Collection:** Push-based (in-process collection)
- **Storage:** Cassandra + custom TSDB layer
- **Alerting:** Atlas query language (AQL) with windowed evaluation
- **Visualization:** Custom dashboards

**Key Insights:**
- Built on Cassandra (not specialized TSDB) for operational simplicity
- Custom compression algorithm (better than standard Cassandra)
- In-memory aggregation (pre-aggregate before writing to Cassandra)

---

## 13. References

### Related Chapters:
- **[2.1.3 Specialized Databases](../../02-components/2.1.3-specialized-databases.md)** - Time-series databases
- **[2.3.2 Kafka Deep Dive](../../02-components/2.3.2-kafka-deep-dive.md)** - Message queue for metrics buffering
- **[2.3.5 Batch vs Stream Processing](../../02-components/2.3.5-batch-vs-stream-processing.md)** - Real-time aggregation
- **[2.4.2 Observability](../../02-components/2.4.2-observability.md)** - Monitoring and alerting best practices

### External Resources:
- **Prometheus Documentation:** https://prometheus.io/docs/
- **M3 by Uber:** https://m3db.io/
- **Gorilla Compression Paper (Facebook):** https://www.vldb.org/pvldb/vol8/p1816-teller.pdf
- **Google's Monarch Paper:** https://research.google/pubs/pub50652/
- **Datadog Architecture:** https://www.datadoghq.com/blog/engineering/timeseries-indexing-at-scale/

---

**Total:** This comprehensive design covers all aspects of building a distributed monitoring system capable of handling 100M writes/sec, storing 10 PB of data, and providing real-time alerting and visualization.

