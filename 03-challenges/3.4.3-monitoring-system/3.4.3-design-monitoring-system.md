# 3.4.3 Design a Distributed Monitoring System

> 📚 **Note on Implementation Details:**
> This document focuses on high-level design concepts and architectural decisions.
> For detailed algorithm implementations, see **[pseudocode.md](./pseudocode.md)**.

## 📊 Visual Diagrams & Resources

- **[High-Level Design Diagrams](./hld-diagram.md)** - System architecture, component design, data flow
- **[Sequence Diagrams](./sequence-diagrams.md)** - Detailed interaction flows and failure scenarios
- **[Design Decisions (This Over That)](./this-over-that.md)** - In-depth analysis of architectural choices
- **[Pseudocode Implementations](./pseudocode.md)** - Detailed algorithm implementations

---

## 1. Problem Statement

Design a distributed monitoring system that collects, stores, and analyzes metrics from millions of endpoints (servers, containers, applications) in real-time. The system must:

- **Collect** metrics (CPU, memory, disk, latency, QPS, error rates) from 1M+ endpoints
- **Store** time-series data efficiently for long-term retention (2+ years)
- **Alert** on anomalies and threshold violations in real-time (<5 seconds)
- **Visualize** metrics through dashboards for historical analysis and troubleshooting
- **Scale** horizontally to handle massive write throughput (1M writes/sec)

**Similar Systems:** Prometheus, Datadog, New Relic, Grafana Cloud, AWS CloudWatch

---

## 2. Requirements and Scale Estimation

### Functional Requirements (FRs)

1. **Metrics Collection**
   - Pull-based collection (scraping endpoints like Prometheus)
   - Push-based collection (agents push metrics)
   - Support for custom metrics and labels
   - Auto-discovery of new endpoints

2. **Time-Series Storage**
   - Store metrics with high resolution (1-second granularity)
   - Support retention policies (rollup and downsampling over time)
   - Efficient compression for long-term storage

3. **Real-Time Alerting**
   - Evaluate alert rules in real-time (e.g., `CPU > 80% for 5 minutes`)
   - Support complex queries (aggregations, percentiles)
   - Trigger notifications (email, Slack, PagerDuty)
   - Alert deduplication and grouping

4. **Visualization**
   - Dashboards with customizable graphs
   - Support for ad-hoc queries
   - Historical data analysis
   - Anomaly detection and forecasting

### Non-Functional Requirements (NFRs)

1. **High Write Throughput:** 1M writes/sec sustained, 5M writes/sec peak
2. **Low Write Latency:** <100ms p99 from metric collection to storage
3. **Fast Query Performance:** <1 second for dashboard queries (last 1 hour)
4. **High Availability:** 99.9% uptime for alerting and query services
5. **Durability:** No data loss for critical metrics (best-effort for non-critical)
6. **Scalability:** Horizontal scaling for both writes and reads

### Scale Estimation

| Metric                    | Assumption                                        | Calculation                                               | Result                                                 |
|---------------------------|---------------------------------------------------|-----------------------------------------------------------|--------------------------------------------------------|
| **Total Endpoints**       | 1M servers/containers                             | -                                                         | 1 Million endpoints                                    |
| **Metrics per Endpoint**  | 100 metrics (CPU, memory, disk, network, custom) | -                                                         | 100 metrics per endpoint                               |
| **Collection Frequency**  | 1 sample per second                               | -                                                         | 1 data point/sec per metric                            |
| **Write Throughput**      | $1\text{M}$ endpoints × $100$ metrics × $1$/sec   | -                                                         | **100M data points/sec** ($100\text{M}$ $\text{QPS}$) |
| **Storage (1 Day)**       | $100\text{M}$ points/sec × $86,400$ sec/day       | -                                                         | 8.64 trillion data points/day                          |
| **Storage (2 Years)**     | $8.64$ trillion × $365$ × $2$                     | -                                                         | **6.3 quadrillion data points** (petabyte scale)       |
| **Storage Size (Raw)**    | 16 bytes per point (timestamp + value + labels)   | $6.3$ quadrillion × $16$ bytes                            | **~100 PB** (before compression)                       |
| **Storage Size (Compressed)** | 10:1 compression ratio                        | $100$ PB / $10$                                           | **~10 PB** (after compression)                         |
| **Query Throughput**      | 100k dashboards × 10 queries/min                  | -                                                         | **~16k QPS** (read queries)                            |

**Key Insight:** The system is **write-heavy** (100M writes/sec vs 16k reads/sec). Storage must be optimized for sequential writes and time-based compression.

---

## 3. High-Level Architecture

The architecture follows a classic **time-series data pipeline** optimized for high write throughput and efficient time-based queries.

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                          MONITORED INFRASTRUCTURE                           │
│  ┌─────────┐ ┌─────────┐ ┌─────────┐ ┌─────────┐         ┌─────────┐      │
│  │ Server 1│ │ Server 2│ │Container│ │   App   │   ...   │ Server  │      │
│  │ (1M endpoints)        │         │         │         │   1M    │      │
│  └────┬────┘ └────┬────┘ └────┬────┘ └────┬────┘         └────┬────┘      │
└───────┼───────────┼───────────┼───────────┼──────────────────┼────────────┘
        │           │           │           │                  │
        │ scrape    │ scrape    │ push      │ push             │ scrape
        │ (pull)    │ (pull)    │ (agent)   │ (SDK)            │ (pull)
        ▼           ▼           ▼           ▼                  ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│                          COLLECTION LAYER                                   │
│  ┌─────────────────┐ ┌─────────────────┐ ┌─────────────────┐              │
│  │ Collector 1     │ │ Collector 2     │ │ Collector N     │              │
│  │ (Prometheus)    │ │ (Prometheus)    │ │ (100 instances) │              │
│  │ 1M metrics/sec  │ │ 1M metrics/sec  │ │                 │              │
│  └────────┬────────┘ └────────┬────────┘ └────────┬────────┘              │
└───────────┼───────────────────┼───────────────────┼─────────────────────────┘
            │                   │                   │
            │ write             │ write             │ write
            ▼                   ▼                   ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│                            BUFFER LAYER                                     │
│                         ┌─────────────────┐                                 │
│                         │  Kafka Cluster  │                                 │
│                         │  (Message Queue)│                                 │
│                         │  100M msgs/sec  │                                 │
│                         │  24 partitions  │                                 │
│                         └────────┬────────┘                                 │
└──────────────────────────────────┼──────────────────────────────────────────┘
                                   │
         ┌─────────────────────────┼─────────────────────────┐
         │                         │                         │
         ▼                         ▼                         ▼
┌────────────────┐      ┌────────────────────┐      ┌──────────────────┐
│ ALERTING       │      │ STORAGE            │      │ REAL-TIME        │
│ ENGINE         │      │ (TSDB)             │      │ AGGREGATION      │
│ (Flink/Streams)│      │                    │      │ (Stream Processor│
│                │      │ ┌────────────────┐ │      │                  │
│ - Evaluate     │      │ │ InfluxDB/M3DB  │ │      │ - Downsampling   │
│   rules        │      │ │ Cassandra+TSDB │ │      │ - Rollups        │
│ - Windowed     │      │ │                │ │      │                  │
│   aggregations │      │ │ Sharded by:    │ │      └──────┬───────────┘
│ - Trigger      │      │ │ - Time         │ │             │
│   alerts       │      │ │ - Metric name  │ │             │ write
│                │      │ │                │ │             │ (rollups)
└────────┬───────┘      │ │ 10 PB storage  │ │             │
         │              │ └────────┬───────┘ │             │
         │ notify       │          │         │◄────────────┘
         ▼              └──────────┼─────────┘
┌────────────────┐                │
│ NOTIFICATION   │                │ read queries
│ SERVICE        │                │
│ - Email        │                ▼
│ - Slack        │      ┌────────────────────┐
│ - PagerDuty    │      │ QUERY SERVICE      │
└────────────────┘      │ (API Layer)        │
                        │                    │
                        │ - Query optimizer  │
                        │ - Cache layer      │
                        │   (Redis)          │
                        │ - Query result     │
                        │   cache            │
                        └──────────┬─────────┘
                                   │
                                   │ HTTP API
                                   ▼
                        ┌────────────────────┐
                        │ VISUALIZATION      │
                        │ (Grafana)          │
                        │                    │
                        │ - Dashboards       │
                        │ - Ad-hoc queries   │
                        │ - Alerting UI      │
                        └────────────────────┘
```

**Key Components:**

1. **Collectors (Prometheus/Agents)** - Scrape or receive metrics from endpoints
2. **Kafka Buffer** - Decouples collectors from storage, handles write bursts
3. **Time-Series Database (TSDB)** - Optimized storage (InfluxDB, M3DB, Cassandra)
4. **Alerting Engine** - Real-time rule evaluation on Kafka stream
5. **Real-Time Aggregation** - Downsampling and rollups for long-term storage
6. **Query Service** - API layer with caching for dashboard queries
7. **Notification Service** - Sends alerts to external systems
8. **Grafana** - Visualization and dashboard UI

---

## 4. Data Model

### 4.1 Time-Series Data Structure

Time-series data consists of:
- **Metric name** (e.g., `cpu_usage`, `http_request_latency`)
- **Timestamp** (Unix epoch, millisecond precision)
- **Value** (numeric: float, int)
- **Labels/Tags** (key-value pairs for dimensions)

**Example:**
```
Metric: http_request_latency_ms
Timestamp: 1698765432000
Value: 234.5
Labels: {
  service: "api-gateway",
  method: "POST",
  endpoint: "/users",
  region: "us-east-1",
  status_code: "200"
}
```

**Storage Format (Prometheus-style):**
```
http_request_latency_ms{service="api-gateway",method="POST",endpoint="/users",region="us-east-1",status_code="200"} 234.5 1698765432000
```

### 4.2 TSDB Schema (Cassandra Example)

**Table 1: Raw Metrics (High-Resolution)**
```sql
CREATE TABLE metrics_raw (
    metric_name text,           -- e.g., cpu_usage
    labels map<text, text>,     -- {host: "server-1", region: "us-east-1"}
    timestamp timestamp,        -- Time of measurement
    value double,               -- Metric value
    PRIMARY KEY ((metric_name, labels), timestamp)
) WITH CLUSTERING ORDER BY (timestamp DESC)
  AND compaction = {'class': 'TimeWindowCompactionStrategy'};
```

**Partitioning Strategy:** Partition by `(metric_name, labels)` to group all data points for a specific metric+label combination. Cluster by `timestamp DESC` for efficient time-range queries.

**Table 2: Rollup Metrics (Downsampled)**
```sql
CREATE TABLE metrics_rollup_1min (
    metric_name text,
    labels map<text, text>,
    timestamp_bucket timestamp,  -- Rounded to 1-minute boundary
    count bigint,                -- Number of samples in window
    sum double,                  -- Sum of values
    min double,                  -- Min value
    max double,                  -- Max value
    avg double,                  -- Average value
    p50 double,                  -- 50th percentile
    p95 double,                  -- 95th percentile
    p99 double,                  -- 99th percentile
    PRIMARY KEY ((metric_name, labels), timestamp_bucket)
) WITH CLUSTERING ORDER BY (timestamp_bucket DESC);
```

**Rollup Hierarchy:**
- **Raw** (1-second resolution) → Retained for 1 day
- **1-minute rollup** → Retained for 7 days
- **5-minute rollup** → Retained for 30 days
- **1-hour rollup** → Retained for 1 year
- **1-day rollup** → Retained for 2+ years

---

## 5. Detailed Component Design

### 5.1 Metrics Collection

**Pull-Based Collection (Prometheus Model):**
1. **Service Discovery** - Automatically discover endpoints (Kubernetes pods, AWS instances)
2. **Scraping** - HTTP GET to `/metrics` endpoint every 1 second
3. **Parsing** - Parse Prometheus text format (metric name, labels, value, timestamp)
4. **Buffering** - Write to local buffer before sending to Kafka

**Push-Based Collection (Agent Model):**
1. **Agent Installation** - Deploy agent on each endpoint (e.g., Telegraf, Datadog agent)
2. **Metric Collection** - Agent collects system metrics (CPU, memory, disk, network)
3. **Push to Collector** - HTTP POST to collector endpoint
4. **Batching** - Agent batches metrics (100 points) before sending

**Comparison:**

| Aspect | Pull-Based (Prometheus) | Push-Based (Agent) |
|--------|-------------------------|---------------------|
| **Discovery** | Centralized (scraper knows endpoints) | Distributed (agents register) |
| **Network** | Collector initiates connection | Endpoint initiates connection |
| **Firewall** | Requires inbound access to endpoints | Requires outbound access only |
| **Failure Handling** | Scraper detects endpoint down | Endpoint can retry |
| **Scale** | Collector bottleneck (horizontal scaling) | Endpoints self-managing |

**Decision:** Use **hybrid approach** - pull for internal services, push for external/edge devices.

### 5.2 Kafka Buffer Layer

**Why Kafka?**
- **Decouples** collectors from storage (if TSDB is slow, Kafka absorbs writes)
- **Replay** capability for reprocessing (e.g., backfill rollups)
- **Durability** - Data persisted to disk before acknowledgment
- **Partitioning** - Distributes load across TSDB writers

**Kafka Topic Configuration:**
```
Topic: metrics.raw
Partitions: 24 (for 100M msgs/sec throughput)
Replication factor: 3 (durability)
Retention: 24 hours (enough for retry/replay)
Compression: snappy (reduces storage by 5x)
```

**Partitioning Strategy:** Partition by `hash(metric_name + labels)` to ensure all data points for a specific time series go to the same partition (maintains order).

**Consumer Groups:**
1. **TSDB Writers** - Consume and write to time-series database
2. **Alerting Engine** - Consume for real-time alert evaluation
3. **Real-Time Aggregation** - Consume for downsampling and rollups

### 5.3 Time-Series Database (TSDB)

**TSDB Selection Criteria:**
- High write throughput (100M writes/sec)
- Efficient time-range queries
- Built-in compression
- Support for retention policies and downsampling

**Option 1: InfluxDB (Specialized TSDB)**
- **Pros:** Purpose-built for time-series, excellent compression, fast time-range queries
- **Cons:** Horizontal scaling is limited (InfluxDB Enterprise required), expensive

**Option 2: M3DB (Uber's TSDB)**
- **Pros:** Designed for scale (handles billions of writes/sec), Prometheus-compatible
- **Cons:** Complex to operate, relatively new (smaller community)

**Option 3: Cassandra + Custom TSDB Layer**
- **Pros:** Proven scalability, flexible schema, strong consistency options
- **Cons:** Not optimized for time-series out-of-the-box (requires custom compression/indexing)

**Decision:** **M3DB** for write-heavy workloads (100M writes/sec) with Prometheus compatibility. *See [this-over-that.md](./this-over-that.md) for detailed analysis.*

**M3DB Architecture:**
- **M3DB Nodes** - Distributed storage nodes (50+ nodes)
- **M3Coordinator** - Query aggregator and downsampler
- **M3Aggregator** - Real-time aggregation and rollups
- **M3Query** - PromQL-compatible query engine

**Sharding Strategy:**
- **Shard by metric name + labels** (consistent hashing)
- **Replication factor:** 3 (for durability)
- **Shard count:** 4096 (for fine-grained distribution)

**Compression:**
- **Gorilla compression** (Facebook's time-series compression algorithm)
- **Compression ratio:** 10:1 (100 PB → 10 PB)
- **Delta-of-delta encoding** for timestamps
- **XOR encoding** for float values

### 5.4 Alerting Engine

**Requirements:**
- Evaluate alert rules in real-time (<5 seconds)
- Support complex queries (aggregations, percentiles, windows)
- Handle stateful queries (e.g., "CPU > 80% for 5 minutes")
- Deduplicate and group alerts

**Architecture:**

**Stream Processor:** Apache Flink or Kafka Streams
- **Input:** Kafka topic `metrics.raw`
- **Processing:** Stateful windowed aggregations
- **Output:** Kafka topic `alerts.triggered`

**Alert Rule Example:**
```
Alert: HighCPUUsage
Condition: avg(cpu_usage{service="api-gateway"}) > 80 for 5 minutes
Severity: warning
Notification: slack-channel=#alerts
```

**Processing Flow:**
1. **Consume metrics** from Kafka
2. **Group by alert rule** (e.g., all `cpu_usage` metrics for `api-gateway`)
3. **Windowed aggregation** (5-minute tumbling window)
4. **Evaluate condition** (avg > 80%)
5. **Trigger alert** if condition met for entire window
6. **Deduplicate** (don't send alert if already firing)
7. **Publish** to `alerts.triggered` topic

**Stateful Processing:**
- **State store:** RocksDB (embedded key-value store)
- **State:** Current window aggregations (count, sum, min, max)
- **Checkpointing:** Periodic snapshots for fault tolerance

**Alert Deduplication:**
- Track alert state (firing, resolved)
- Don't send duplicate alerts if already firing
- Send "resolved" notification when condition clears

**Alert Grouping:**
- Group alerts by labels (e.g., all alerts for `region=us-east-1`)
- Single notification for multiple related alerts
- Reduces notification spam

### 5.5 Real-Time Aggregation (Rollups)

**Why Rollups?**
- Storing 6.3 quadrillion data points at 1-second resolution is too expensive
- Dashboard queries over long time ranges (e.g., "last 30 days") would be slow
- Downsampling reduces storage and improves query performance

**Rollup Strategy:**

**Retention Policy:**
- **Raw (1-second):** 1 day (8.64 trillion points)
- **1-minute rollup:** 7 days (1.4 trillion points)
- **5-minute rollup:** 30 days (8.6 billion points)
- **1-hour rollup:** 1 year (876 million points)
- **1-day rollup:** 2+ years (730,000 points per metric)

**Rollup Computation:**
- **Stream processor** (Flink or Kafka Streams)
- **Input:** Kafka topic `metrics.raw`
- **Processing:** Windowed aggregations (1-minute, 5-minute, 1-hour, 1-day)
- **Output:** Write to TSDB rollup tables

**Aggregation Functions:**
- `count` - Number of samples in window
- `sum` - Sum of values
- `min`, `max` - Min/max values
- `avg` - Average value
- `p50`, `p95`, `p99` - Percentiles

**Example Rollup:**
```
Raw data (1-second):
cpu_usage{host="server-1"} 65.2 @ 12:00:00
cpu_usage{host="server-1"} 67.5 @ 12:00:01
cpu_usage{host="server-1"} 70.1 @ 12:00:02
...
cpu_usage{host="server-1"} 72.3 @ 12:00:59

1-minute rollup (12:00:00 - 12:00:59):
cpu_usage_1min{host="server-1"} {
  count: 60,
  sum: 4080.5,
  min: 65.2,
  max: 75.1,
  avg: 68.0,
  p50: 67.8,
  p95: 74.2,
  p99: 75.0
} @ 12:00:00
```

### 5.6 Query Service

**Requirements:**
- Fast query performance (<1 second for dashboard queries)
- Support for complex queries (aggregations, joins, percentiles)
- Caching for frequently accessed data
- Query optimization (push down filters, use rollups)

**Architecture:**

**Query API Layer:**
- **Protocol:** HTTP REST API + PromQL support
- **Query optimizer:** Analyzes query and determines optimal execution plan
- **Cache layer:** Redis for query result caching
- **Rate limiting:** Prevent expensive queries from overloading TSDB

**Query Optimization:**

1. **Automatic Rollup Selection**
   - Query: "Show CPU usage for last 30 days"
   - Optimizer: Use 1-hour rollup (not raw data)
   - Benefit: 3600x fewer data points to fetch

2. **Predicate Pushdown**
   - Push filters down to TSDB (not in application layer)
   - Example: Filter by `region=us-east-1` at storage level

3. **Query Result Caching**
   - Cache key: `query_hash + time_range`
   - TTL: 1 minute (for recent data), 1 hour (for old data)
   - Invalidation: On alert trigger (stale data)

4. **Query Parallelization**
   - Split query into sub-queries (per shard)
   - Execute in parallel
   - Merge results

**Caching Strategy:**

**Cache Layer 1: Query Result Cache (Redis)**
- Key: `hash(query + time_range)`
- Value: Serialized query result
- TTL: 1 minute (for last hour), 1 hour (for older data)
- Hit rate: 80% (popular dashboards)

**Cache Layer 2: TSDB Block Cache (In-Memory)**
- Cache recently accessed TSDB blocks (1-hour chunks)
- LRU eviction policy
- Size: 100 GB per TSDB node

---

## 6. Bottlenecks and Scaling Strategies

### 6.1 Write Path Bottlenecks

**Bottleneck 1: Kafka Write Throughput**

**Problem:** 100M writes/sec requires high Kafka throughput (sustained 200 MB/s per partition).

**Solution:**
- **Increase partitions:** 24 → 48 partitions (distribute load)
- **Batching:** Collectors batch 1000 metrics before sending to Kafka
- **Compression:** Use snappy compression (5x reduction)
- **Async producers:** Non-blocking writes

**Performance:**
- Before: 100M writes/sec, 200 MB/s per partition (at limit)
- After: 100M writes/sec, 100 MB/s per partition (comfortable headroom)

**Bottleneck 2: TSDB Write Throughput**

**Problem:** TSDB struggles with random writes (even though time-series data is mostly sequential, labels create randomness).

**Solution:**
- **Write buffer (M3DB aggregator):** Buffer writes in memory (1 minute), batch to disk
- **LSM-Tree storage engine:** Optimized for write-heavy workloads
- **Compression:** Gorilla compression reduces disk writes by 10x
- **Horizontal scaling:** Add more TSDB nodes (50+ nodes)

**Performance:**
- Single node: 1M writes/sec
- 100 nodes: 100M writes/sec (linear scaling)

### 6.2 Read Path Bottlenecks

**Bottleneck 3: Dashboard Query Latency**

**Problem:** Dashboard queries over long time ranges (e.g., "last 30 days") fetch millions of data points (slow).

**Solution:**
- **Automatic rollup selection:** Query optimizer uses 1-hour rollup (3600x fewer points)
- **Query result caching:** Redis cache with 80% hit rate
- **Downsampling at query time:** If query asks for 1000 points over 30 days, downsample to 1000 points (not millions)

**Performance:**
- Before: 30-day query = 2.6 billion points, 10 seconds
- After: 30-day query (1-hour rollup) = 720 points, 100ms

**Bottleneck 4: Ad-Hoc Query Overload**

**Problem:** Expensive ad-hoc queries (e.g., "Show p99 latency for all services over last year") can overwhelm TSDB.

**Solution:**
- **Query rate limiting:** Max 10 concurrent queries per user
- **Query timeout:** Kill queries running >30 seconds
- **Query cost estimation:** Warn users before running expensive queries
- **Dedicated query pool:** Separate TSDB replicas for ad-hoc queries (don't impact dashboards)

### 6.3 Storage Bottlenecks

**Bottleneck 5: Storage Capacity**

**Problem:** 10 PB storage (after compression) is expensive and hard to manage.

**Solution:**
- **Tiered storage:** Hot (SSD), warm (HDD), cold (S3/Glacier)
  - **Hot tier (0-7 days):** SSD, fast queries
  - **Warm tier (7-90 days):** HDD, slower queries
  - **Cold tier (90+ days):** S3/Glacier, archive only
- **Aggressive rollups:** Only store 1-day rollups for data >1 year old
- **Selective retention:** Critical metrics retained longer, non-critical metrics deleted after 30 days

**Cost Analysis:**
```
Hot tier (7 days): 800 TB × $0.10/GB-month = $80k/month
Warm tier (83 days): 8 PB × $0.02/GB-month = $160k/month
Cold tier (2 years): 1 PB × $0.004/GB-month = $4k/month

Total storage cost: $244k/month
```

---

## 7. Multi-Region Deployment

**Requirements:**
- Low-latency metric collection (collect in region where endpoint is located)
- Global view of metrics (aggregate across regions)
- Region failover (if one region fails, others continue)

**Architecture:**

**Regional Deployment:**
- Each region has full stack: Collectors → Kafka → TSDB → Query Service
- Metrics collected locally (low latency)
- Regional dashboards for local troubleshooting

**Global Aggregation:**
- **Cross-region replication:** Kafka MirrorMaker replicates metrics to central region
- **Global TSDB:** Aggregates metrics from all regions
- **Global dashboards:** View metrics across all regions

**Trade-offs:**
- **Eventual consistency:** Global metrics lag behind regional metrics by 5-30 seconds
- **Higher cost:** 3x infrastructure (3 regions)
- **Complexity:** Cross-region networking and replication

---

## 8. Common Anti-Patterns

### ❌ **Anti-Pattern 1: Using RDBMS for Time-Series Data**

**Problem:** Using PostgreSQL/MySQL to store time-series metrics.

**Why It's Bad:**
- RDBMS not optimized for high write throughput (B-Tree indexes, write amplification)
- Time-range queries slow (requires index scan)
- No built-in compression or retention policies
- Disk usage explodes (no downsampling)

**Solution:**
✅ Use specialized TSDB (InfluxDB, M3DB, TimescaleDB)

### ❌ **Anti-Pattern 2: No Rollups/Downsampling**

**Problem:** Storing all metrics at full resolution (1-second) forever.

**Why It's Bad:**
- Storage cost explodes (100 PB for 2 years)
- Dashboard queries over long time ranges are slow (fetching billions of points)
- Most queries don't need 1-second resolution for historical data

**Solution:**
✅ Implement aggressive rollups (1-minute, 1-hour, 1-day) with retention policies

### ❌ **Anti-Pattern 3: Synchronous Alert Evaluation on TSDB Queries**

**Problem:** Alerting engine queries TSDB every 10 seconds to evaluate alert rules.

**Why It's Bad:**
- High query load on TSDB (10k alert rules × 6 queries/min = 1M queries/min)
- Alert latency high (10+ seconds)
- TSDB query load competes with dashboard queries

**Solution:**
✅ Use stream processing (Flink/Kafka Streams) to evaluate alerts on Kafka stream in real-time

### ❌ **Anti-Pattern 4: No Cardinality Control**

**Problem:** Allowing unlimited unique label combinations (high cardinality).

**Why It's Bad:**
- Each unique label combination creates a new time series
- Example: `request_id` label with 1 billion unique values = 1 billion time series
- TSDB memory and storage explodes
- Queries become slow (needs to scan millions of time series)

**Solution:**
✅ Limit label cardinality:
- Don't use high-cardinality labels (e.g., `user_id`, `request_id`)
- Enforce label whitelist (only allow approved labels)
- Monitor cardinality growth and alert

### ❌ **Anti-Pattern 5: No Query Cost Control**

**Problem:** Allowing users to run expensive ad-hoc queries without limits.

**Why It's Bad:**
- A single expensive query can overload TSDB (e.g., "Show all metrics for all services over last year")
- Impacts all users (dashboard queries slow)
- TSDB nodes crash due to memory exhaustion

**Solution:**
✅ Implement query cost control:
- **Query timeout:** Kill queries running >30 seconds
- **Query rate limiting:** Max 10 concurrent queries per user
- **Query cost estimation:** Warn users before running expensive queries
- **Dedicated query pool:** Separate TSDB replicas for ad-hoc queries

---

## 9. Alternative Approaches

### Approach 1: Serverless Monitoring (AWS CloudWatch Model)

**Architecture:**
- Managed service (no infrastructure to manage)
- Push-based collection (agents push metrics to CloudWatch)
- Pay-per-use pricing (per metric, per query)

**Pros:**
- ✅ No operational overhead (fully managed)
- ✅ Scales automatically (no capacity planning)
- ✅ Integrated with cloud services (AWS, GCP, Azure)

**Cons:**
- ❌ Expensive at scale (100M metrics/sec = $500k/month)
- ❌ Vendor lock-in (can't migrate easily)
- ❌ Limited customization (fixed retention, limited query capabilities)
- ❌ Higher latency (shared infrastructure)

**When to use:** Small to medium scale (<1M metrics/sec), cloud-native apps, no ops team.

### Approach 2: Prometheus + Thanos (Prometheus Long-Term Storage)

**Architecture:**
- Prometheus for collection and short-term storage (2 weeks)
- Thanos for long-term storage (S3/GCS) and global view
- Sidecar architecture (Thanos sidecar uploads data to object storage)

**Pros:**
- ✅ Cost-effective (S3 storage is cheap: $0.023/GB-month)
- ✅ Unlimited retention (store data forever)
- ✅ Prometheus-compatible (same query language)
- ✅ Multi-cluster aggregation (global view)

**Cons:**
- ❌ Slower queries (fetching from S3 is slower than local disk)
- ❌ Complex architecture (multiple components: Prometheus, Thanos Sidecar, Thanos Query, Thanos Store)
- ❌ Eventual consistency (Thanos lags behind Prometheus by minutes)

**When to use:** Medium scale (1-10M metrics/sec), need long-term retention (>1 year), cost-sensitive.

### Approach 3: VictoriaMetrics (High-Performance TSDB)

**Architecture:**
- Single-binary TSDB (simpler than M3DB)
- Prometheus-compatible (drop-in replacement)
- High compression (10-50x better than Prometheus)

**Pros:**
- ✅ Extremely cost-effective (10x cheaper storage than Prometheus)
- ✅ Fast queries (optimized query engine)
- ✅ Simple to operate (single binary, no dependencies)
- ✅ High write throughput (1M writes/sec per node)

**Cons:**
- ❌ Smaller community (less mature than Prometheus)
- ❌ Single-vendor (no other implementations)
- ❌ Limited horizontal scaling (clustering is complex)

**When to use:** Cost-sensitive deployments, need high compression, simple architecture preferred.

---

## 10. Monitoring the Monitoring System (Meta-Monitoring)

**Challenge:** How do you monitor the monitoring system itself?

**Solution: Dual-Layered Monitoring**

**Layer 1: Self-Monitoring (Monitoring System Monitors Itself)**
- Collectors emit metrics about themselves (scrape duration, errors)
- Kafka emits metrics (consumer lag, partition rebalancing)
- TSDB emits metrics (write throughput, query latency, disk usage)
- Alerting engine emits metrics (alert evaluation time, alerts triggered)

**Layer 2: External Monitoring (Separate System)**
- Deploy lightweight external monitoring system (e.g., Prometheus on separate infrastructure)
- Monitor critical metrics of primary monitoring system
- Alert if primary monitoring system is down

**Key Metrics to Monitor:**

| Component | Metric | Alert Threshold |
|-----------|--------|-----------------|
| **Collectors** | `scrape_duration_seconds` | >5 seconds |
| **Collectors** | `scrape_errors_total` | >10 errors/min |
| **Kafka** | `consumer_lag` | >1M messages |
| **Kafka** | `broker_disk_usage_pct` | >80% |
| **TSDB** | `write_throughput_qps` | <50M QPS (expected 100M) |
| **TSDB** | `query_latency_p99_ms` | >5000ms |
| **TSDB** | `disk_usage_pct` | >90% |
| **Alerting Engine** | `alert_evaluation_duration_ms` | >10 seconds |
| **Query Service** | `cache_hit_rate_pct` | <50% |

---

## 11. Trade-offs Summary

| What We Gain | What We Sacrifice |
|--------------|-------------------|
| ✅ **High write throughput** (100M writes/sec) | ❌ Expensive infrastructure (100+ nodes, 10 PB storage) |
| ✅ **Real-time alerting** (<5 seconds) | ❌ Complex stream processing (stateful Flink jobs) |
| ✅ **Long-term retention** (2+ years) | ❌ Aggressive rollups (lose granularity for old data) |
| ✅ **Fast dashboard queries** (<1 second) | ❌ Eventual consistency (rollups lag behind raw data) |
| ✅ **Horizontal scalability** (add nodes) | ❌ Operational complexity (manage 100+ nodes) |
| ✅ **Multi-region deployment** (low latency) | ❌ 3x infrastructure cost |
| ✅ **Automatic downsampling** (reduce storage) | ❌ Cannot reconstruct original raw data |

---

## 12. Real-World Examples

### Uber's M3 Monitoring System

**Scale:**
- **Endpoints:** 10M+ metrics/sec from 100k+ hosts
- **Storage:** 100+ PB of metrics data
- **Queries:** 1M+ queries/day
- **Alerts:** 100k+ alerts/day

**Architecture:**
- **Collection:** Pull-based (Prometheus-compatible)
- **Storage:** M3DB (custom TSDB built by Uber)
- **Alerting:** M3Query + custom alerting engine
- **Visualization:** Grafana

**Key Insights:**
- Built M3DB because existing TSDB couldn't scale to 10M writes/sec
- Aggressive compression (Gorilla + downsampling) → 10:1 ratio
- Multi-region deployment with 5 seconds replication lag

### Datadog (SaaS Monitoring Platform)

**Scale:**
- **Customers:** 25k+ companies
- **Metrics:** 500 billion metrics/day
- **Storage:** Multi-exabyte scale
- **Queries:** 10M+ queries/day

**Architecture:**
- **Collection:** Push-based (Datadog agent)
- **Storage:** Custom TSDB (proprietary)
- **Alerting:** Real-time stream processing
- **Visualization:** Custom web UI

**Key Insights:**
- Multi-tenancy (data isolation per customer)
- Tiered storage (hot SSD, warm HDD, cold S3)
- Dynamic rollups based on query patterns (optimize for frequently queried metrics)

### Netflix's Atlas Monitoring System

**Scale:**
- **Metrics:** 2 billion metrics/minute
- **Endpoints:** 100k+ instances
- **Storage:** Cassandra (1 PB)
- **Queries:** 1M+ queries/day

**Architecture:**
- **Collection:** Push-based (in-process collection)
- **Storage:** Cassandra + custom TSDB layer
- **Alerting:** Atlas query language (AQL) with windowed evaluation
- **Visualization:** Custom dashboards

**Key Insights:**
- Built on Cassandra (not specialized TSDB) for operational simplicity
- Custom compression algorithm (better than standard Cassandra)
- In-memory aggregation (pre-aggregate before writing to Cassandra)

---

## 13. References

### Related Chapters:
- **[2.1.3 Specialized Databases](../../02-components/2.1.3-specialized-databases.md)** - Time-series databases
- **[2.3.2 Kafka Deep Dive](../../02-components/2.3.2-kafka-deep-dive.md)** - Message queue for metrics buffering
- **[2.3.5 Batch vs Stream Processing](../../02-components/2.3.5-batch-vs-stream-processing.md)** - Real-time aggregation
- **[2.4.2 Observability](../../02-components/2.4.2-observability.md)** - Monitoring and alerting best practices

### External Resources:
- **Prometheus Documentation:** https://prometheus.io/docs/
- **M3 by Uber:** https://m3db.io/
- **Gorilla Compression Paper (Facebook):** https://www.vldb.org/pvldb/vol8/p1816-teller.pdf
- **Google's Monarch Paper:** https://research.google/pubs/pub50652/
- **Datadog Architecture:** https://www.datadoghq.com/blog/engineering/timeseries-indexing-at-scale/

---

**Total:** This comprehensive design covers all aspects of building a distributed monitoring system capable of handling 100M writes/sec, storing 10 PB of data, and providing real-time alerting and visualization.

