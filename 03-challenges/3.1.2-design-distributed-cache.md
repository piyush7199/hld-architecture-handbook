# 3.1.2 Design a Distributed Cache (Redis/Memcached)

## Problem Statement

Design a highly available, horizontally scalable, distributed in-memory cache system similar to Redis or Memcached. The
system should support millions of operations per second with sub-millisecond latency while providing fault tolerance and
minimal data reshuffling when nodes are added or removed.

---

## 1. Requirements and Scale Estimation

### Functional Requirements (FRs)

| Requirement                | Description                                                               | Priority     |
|----------------------------|---------------------------------------------------------------------------|--------------|
| **Key-Value Operations**   | Support `GET(key)`, `PUT(key, value)`, `DELETE(key)` with O(1) complexity | Must Have    |
| **Time-to-Live (TTL)**     | Keys must expire after a specified duration automatically                 | Must Have    |
| **Horizontal Scalability** | Add/remove cache nodes dynamically without downtime                       | Must Have    |
| **Data Types**             | Support strings, lists, sets, sorted sets, hashes                         | Should Have  |
| **Atomic Operations**      | Support atomic increment/decrement, compare-and-swap                      | Should Have  |
| **Persistence (Optional)** | Option to persist data to disk for durability                             | Nice to Have |

### Non-Functional Requirements (NFRs)

| Requirement             | Target                                     | Rationale                                     |
|-------------------------|--------------------------------------------|-----------------------------------------------|
| **Low Latency**         | < 1 ms (p99)                               | In-memory operations must be fast             |
| **High Throughput**     | > 100K QPS per node                        | Support high-traffic applications             |
| **High Availability**   | 99.99% uptime                              | Single node failure should not affect service |
| **Minimal Reshuffling** | < 1% keys moved when adding/removing nodes | Preserve cache hit ratio                      |
| **Memory Efficiency**   | < 20% metadata overhead                    | Maximize data storage                         |

---

### Scale Estimation

#### Scenario: Large E-commerce Platform

| Metric                | Assumption                                 | Calculation                 | Result                 |
|-----------------------|--------------------------------------------|-----------------------------|------------------------|
| **Total Data**        | 10% of 1.8 TB database in cache            | 1.8 TB × 0.1                | **~180 GB RAM needed** |
| **Average Key Size**  | Key: 64 bytes, Value: 1 KB                 | 64 B + 1 KB                 | ~1 KB per entry        |
| **Total Keys**        | 180 GB / 1 KB                              | 180 GB / 1 KB               | **~180 million keys**  |
| **Peak Read QPS**     | 80% of DB reads (from 1157 QPS baseline)   | 1157 × 0.8 × 5 (burst)      | **~4,600 reads/sec**   |
| **Peak Write QPS**    | 20% of reads (cache invalidation)          | 4,600 × 0.2                 | **~920 writes/sec**    |
| **Network Bandwidth** | 1 KB × 4,600 QPS                           | 1 KB × 4,600                | **~4.6 MB/sec**        |
| **Nodes Required**    | 180 GB / 64 GB per node (with replication) | 180 GB / 64 GB × 3 replicas | **~9 nodes**           |

#### Back-of-the-Envelope Calculations

```
Memory per Node = 64 GB RAM
Replication Factor = 3 (1 primary + 2 replicas)
Total Nodes = (180 GB / 64 GB) × 3 = ~9 nodes

QPS per Node = 4,600 QPS / 3 primaries = ~1,533 QPS/node
This is well within Redis capacity (100K+ QPS per node)

Network: 4.6 MB/sec is negligible (1 Gbps = 125 MB/sec)
```

---

## 2. High-Level Architecture

### System Overview

```
┌─────────────────────────────────────────────────────────────────┐
│                     Application Servers                         │
│  ┌────────────────────────────────────────────────────────┐     │
│  │  Cache Client Library (Consistent Hashing Logic)       │     │
│  └────────────────────────────────────────────────────────┘     │
└───────────────────────┬─────────────────────────────────────────┘
                        │
        ┌───────────────┼───────────────┐
        │               │               │
        ▼               ▼               ▼
    ┌───────┐       ┌───────┐       ┌───────┐
    │ Node 1│       │ Node 2│       │ Node 3│  ← Primary Nodes
    │(Master)       │(Master)       │(Master)
    └───┬───┘       └───┬───┘       └───┬───┘
        │               │               │
    ┌───┴───┐       ┌───┴───┐       ┌───┴───┐
    │Replica│       │Replica│       │Replica│  ← Replica Nodes
    │  1A   │       │  2A   │       │  3A   │
    └───────┘       └───────┘       └───────┘
    ┌───────┐       ┌───────┐       ┌───────┐
    │Replica│       │Replica│       │Replica│
    │  1B   │       │  2B   │       │  3B   │
    └───────┘       └───────┘       └───────┘

Optional: Sentinel / Cluster Manager
┌────────────────────────────────────────────┐
│  Sentinel 1  │  Sentinel 2  │  Sentinel 3  │
│  (Monitors health and performs failover)   │
└────────────────────────────────────────────┘
```

### Key Components

| Component               | Responsibility                                  | Technology Options           |
|-------------------------|-------------------------------------------------|------------------------------|
| **Client Library**      | Consistent hashing, routing, connection pooling | jedis, lettuce, go-redis     |
| **Cache Node**          | In-memory key-value storage                     | Redis, Memcached, KeyDB      |
| **Replication**         | Data redundancy, failover                       | Master-Replica (async/sync)  |
| **Cluster Manager**     | Health monitoring, failover orchestration       | Redis Sentinel, Cluster mode |
| **Configuration Store** | Node discovery, cluster topology                | etcd, Consul, ZooKeeper      |

---

## 3. Detailed Component Design

### 3.1 Data Partitioning Strategy

#### Why Consistent Hashing?

**Problem with Simple Modulo Hashing:**

```
// Simple hash: node = hash(key) % N
// If N = 3 nodes:
node = hash("user:123") % 3  // Goes to node 1

// Add 1 node (N = 4):
node = hash("user:123") % 4  // Now goes to node 3! ❌

// Result: ~75% of keys remapped → cache invalidation cascade!
```

**Solution: Consistent Hashing**

```
// Consistent Hashing with Virtual Nodes
ConsistentHash:
  ring: HashMap
  sorted_keys: SortedList
  virtual_nodes: integer = 150
  
  function initialize(nodes, virtual_nodes=150):
    ring = empty_hashmap
    sorted_keys = empty_list
    this.virtual_nodes = virtual_nodes
    
    for each node in nodes:
      add_node(node)
  
  function add_node(node):
    for i from 0 to virtual_nodes:
      virtual_key = node + ":" + i
      hash_value = hash_function(virtual_key)
      ring[hash_value] = node
      sorted_keys.append(hash_value)
    
    sorted_keys.sort()
  
  function get_node(key):
    if ring is empty:
      return null
    
    hash_value = hash_function(key)
    
    // Binary search for the next node on the ring
    idx = binary_search(sorted_keys, hash_value)
    if idx == length(sorted_keys):
      idx = 0
    
    return ring[sorted_keys[idx]]
  
  function hash_function(key):
    return md5_hash(key) as integer

// Usage
cache = ConsistentHash(['node1', 'node2', 'node3'])
node = cache.get_node('user:123')  // Returns 'node2'

// Add node4:
cache.add_node('node4')
node = cache.get_node('user:123')  // Still likely 'node2'
// Only ~25% of keys remapped instead of 75%!
```

#### Virtual Nodes (VNodes) Impact

| VNodes per Node | Distribution Uniformity   | Lookup Overhead | Recommendation     |
|-----------------|---------------------------|-----------------|--------------------|
| 10-50           | Poor (±20% imbalance)     | Very Low        | Not recommended    |
| 100-200         | Good (±5% imbalance)      | Low             | **Recommended**    |
| 500-1000        | Excellent (±1% imbalance) | Medium          | For large clusters |
| 5000+           | Perfect                   | High            | Overkill           |

---

### 3.2 Replication Strategy

#### Master-Replica (Leader-Follower) Architecture

```
┌─────────────────────────────────────────────────────────┐
│                    Write Request                        │
└──────────────────────┬──────────────────────────────────┘
                       │
                       ▼
              ┌─────────────────┐
              │  Master Node    │
              │  (Primary)      │
              │  - Handles writes
              │  - Replicates   │
              └────────┬────────┘
                       │
           ┌───────────┴───────────┐
           │ Async Replication     │
           │ (non-blocking)        │
           ▼                       ▼
    ┌──────────────┐        ┌──────────────┐
    │  Replica 1   │        │  Replica 2   │
    │  (Read-only) │        │  (Read-only) │
    │  - Lags 1-5ms│        │  - Lags 1-5ms│
    └──────────────┘        └──────────────┘
           │                       │
           └───────────┬───────────┘
                       │
              Read Requests (Load Balanced)
```

#### Replication Options Comparison

| Strategy              | Latency         | Data Loss Risk      | Availability | Best For                          |
|-----------------------|-----------------|---------------------|--------------|-----------------------------------|
| **Async Replication** | Very Low (<1ms) | Low (1-5ms of data) | High         | **Recommended for cache**         |
| **Sync Replication**  | High (2-10ms)   | None                | Medium       | Financial systems (not cache)     |
| **Semi-Sync**         | Medium (1-5ms)  | Very Low            | High         | Balance consistency & performance |
| **No Replication**    | Lowest          | Complete on failure | Low          | Dev/test only                     |

**Why Async for Cache?**

- Cache is **not the source of truth** (database is)
- Sub-millisecond writes critical for performance
- 1-5ms of data loss acceptable (cache can be repopulated from DB)
- Losing 5ms of cached writes ≠ losing database writes

---

### 3.3 Write Policies

#### Cache-Aside (Lazy Loading) - Recommended for Distributed Cache

**Flow:**

```
function get_user(user_id):
  // 1. Try cache first
  cache_key = "user:" + user_id
  user = cache.get(cache_key)
  
  if user is not null:
    return user  // Cache Hit ✅
  
  // 2. Cache Miss - fetch from DB
  user = db.query("SELECT * FROM users WHERE id = ?", user_id)
  
  // 3. Populate cache for next time
  cache.set(cache_key, user, TTL=300)  // 5 minutes
  
  return user


function update_user(user_id, data):
  // 1. Update database (source of truth)
  db.execute("UPDATE users SET ... WHERE id = ?", data, user_id)
  
  // 2. Invalidate cache (not update!)
  cache_key = "user:" + user_id
  cache.delete(cache_key)
  // Next read will fetch fresh data from DB
```

**Why Cache-Aside Over Write-Through?**

| Aspect            | Cache-Aside                  | Write-Through                             |
|-------------------|------------------------------|-------------------------------------------|
| **Write Latency** | Low (DB only)                | High (DB + Cache sync)                    |
| **Cache Size**    | Smaller (only accessed data) | Larger (all written data)                 |
| **Consistency**   | Eventual                     | Strong                                    |
| **Best For**      | **Read-heavy workloads**     | Write-heavy, immediate consistency needed |

---

### 3.4 Eviction Policies

#### LRU (Least Recently Used) Implementation

```
LRUCache:
  capacity: integer
  cache: HashMap  // key → Node
  head: Node  // Dummy head (most recently used)
  tail: Node  // Dummy tail (least recently used)
  
  function initialize(capacity):
    this.capacity = capacity
    cache = empty_hashmap
    head = Node(0, 0)  // Dummy
    tail = Node(0, 0)  // Dummy
    head.next = tail
    tail.prev = head
  
  function get(key):
    if key exists in cache:
      node = cache[key]
      remove_node(node)
      add_to_head(node)
      return node.value
    return null
  
  function put(key, value):
    if key exists in cache:
      remove_node(cache[key])
    
    node = Node(key, value)
    add_to_head(node)
    cache[key] = node
    
    if size(cache) > capacity:
      // Evict LRU (tail)
      lru = tail.prev
      remove_node(lru)
      delete cache[lru.key]

Node:
  key: any
  value: any
  prev: Node
  next: Node
```

#### Eviction Policy Comparison

| Policy     | Best For               | Pros                  | Cons                       | Redis Support                    |
|------------|------------------------|-----------------------|----------------------------|----------------------------------|
| **LRU**    | General purpose        | Good hit rate, simple | Ignores frequency          | ✅ `maxmemory-policy allkeys-lru` |
| **LFU**    | Stable access patterns | Keeps hot data        | Complex, cold start issues | ✅ `allkeys-lfu`                  |
| **TTL**    | Time-sensitive data    | Predictable expiry    | May evict useful data      | ✅ `volatile-ttl`                 |
| **Random** | Low overhead           | Very simple           | Poor hit rate              | ✅ `allkeys-random`               |

---

### 3.5 TTL (Time-to-Live) Management

#### Implementation Strategies

**Option 1: Active Expiration (Lazy Delete)**

```python
def get(key):
    entry = storage.get(key)
    
    if entry is None:
        return None
    
    # Check TTL on access
    if entry.expires_at and time.now() > entry.expires_at:
        storage.delete(key)  # Lazy delete
        return None
    
    return entry.value
```

**Option 2: Passive Expiration (Background Scan)**

```python
def background_ttl_cleaner():
    while True:
        # Sample random keys
        sample = storage.random_sample(100)
        
        for key in sample:
            entry = storage.get(key)
            if entry.expires_at and time.now() > entry.expires_at:
                storage.delete(key)
        
        time.sleep(0.1)  # Run every 100ms
```

**Redis Approach: Hybrid**

- Active: Check on access
- Passive: Sample 20 keys every 100ms, delete expired
- Probabilistic: If > 25% expired, repeat immediately

---

### 3.6 Concurrency Control: Preventing Cache Stampede

#### Problem: Cache Stampede / Thundering Herd

```
Hot Key Expires
        │
        ├─ Request 1 ──┐
        ├─ Request 2 ──┼─▶ All hit DB simultaneously
        ├─ Request 3 ──┤
        ├─ ...         │
        └─ Request 10K ┘

Result: Database overwhelmed, cascading failure
```

#### Solution 1: Single Flight Request (Recommended)

```python
import threading

class SingleFlightCache:
    def __init__(self):
        self.cache = {}
        self.locks = {}
        self.lock_mutex = threading.Lock()
    
    def get_or_fetch(self, key, fetch_fn, ttl=300):
        # Try cache first
        value = self.cache.get(key)
        if value is not None:
            return value
        
        # Get or create lock for this key
        with self.lock_mutex:
            if key not in self.locks:
                self.locks[key] = threading.Lock()
            lock = self.locks[key]
        
        # Acquire lock (only first request proceeds)
        with lock:
            # Double-check cache (another thread may have populated it)
            value = self.cache.get(key)
            if value is not None:
                return value
            
            # Fetch from DB (only once!)
            value = fetch_fn()
            self.cache.set(key, value, ttl)
            return value

# Usage
cache = SingleFlightCache()

def get_user(user_id):
    return cache.get_or_fetch(
        f"user:{user_id}",
        lambda: db.query("SELECT * FROM users WHERE id = ?", user_id),
        ttl=300
    )

# 10,000 concurrent requests for same user:
# - Only 1 DB query executed
# - Other 9,999 requests wait for result
```

#### Solution 2: Probabilistic Early Expiration

```python
import random

def get_with_early_refresh(key, ttl=600):
    entry = cache.get_with_expiry(key)
    
    if entry is None:
        return fetch_and_cache(key, ttl)
    
    value, expires_at = entry
    time_to_expiry = expires_at - time.now()
    
    # Refresh probabilistically before expiry
    # Higher probability as expiration approaches
    if time_to_expiry < ttl * 0.1:  # Last 10% of TTL
        probability = 1 - (time_to_expiry / (ttl * 0.1))
        if random.random() < probability:
            # Asynchronously refresh in background
            threading.Thread(target=refresh_cache, args=(key, ttl)).start()
    
    return value
```

---

## 4. Availability and Fault Tolerance

### 4.1 Failover Strategy

#### Automatic Failover with Sentinel

```
Normal Operation:
App → Master (Healthy)
      └─ Replica 1
      └─ Replica 2

Master Fails:
Sentinel detects failure (quorum vote)
      ↓
Promotes Replica 1 to Master
      ↓
App redirected to new Master
      ↓
Old master becomes replica when it recovers
```

**Configuration:**

```conf
# Redis Sentinel configuration
sentinel monitor mymaster 127.0.0.1 6379 2
sentinel down-after-milliseconds mymaster 5000
sentinel parallel-syncs mymaster 1
sentinel failover-timeout mymaster 10000

# Quorum = 2 (need 2 sentinels to agree master is down)
```

**Failover Process:**

1. **Detection** (5s): Sentinel detects master unresponsive
2. **Quorum Vote** (1s): Sentinels vote if master is really down
3. **Leader Election** (1s): One sentinel leads the failover
4. **Promotion** (1-3s): Promote replica to master
5. **Reconfiguration** (1s): Update clients

**Total Downtime: ~9-13 seconds**

---

### 4.2 Cluster Mode (Sharding + Replication)

```
Redis Cluster:
16,384 Hash Slots distributed across nodes

Slot 0-5460  → Node 1 (Master) + Replica 1A
Slot 5461-10922 → Node 2 (Master) + Replica 2A
Slot 10923-16383 → Node 3 (Master) + Replica 3A

Client calculates slot: CRC16(key) % 16384
Node redirects if wrong slot (MOVED response)
```

**Advantages:**

- Automatic sharding
- Built-in failover
- Horizontal scalability

**Disadvantages:**

- More complex setup
- Cross-slot operations not supported
- Requires client library support

---

## 5. Bottlenecks and Optimizations

### 5.1 Performance Bottlenecks

| Bottleneck                | Symptoms                   | Solution                                       |
|---------------------------|----------------------------|------------------------------------------------|
| **Network Saturation**    | High latency, packet loss  | Use 10 Gbps network, connection pooling        |
| **Single-threaded Redis** | CPU at 100%, high latency  | Use Redis Cluster for parallel processing      |
| **Memory Fragmentation**  | Memory usage > actual data | Restart Redis periodically, use `activedefrag` |
| **Slow Commands**         | Occasional latency spikes  | Use `SLOWLOG`, avoid `KEYS *`, `FLUSHALL`      |
| **Large Values**          | High bandwidth usage       | Compress values, split large objects           |

### 5.2 Optimization Techniques

#### Connection Pooling

```python
import redis

# ❌ Bad: New connection per request
def get_user_bad(user_id):
    r = redis.Redis(host='localhost', port=6379)
    user = r.get(f'user:{user_id}')
    r.close()
    return user

# ✅ Good: Connection pool (reuse connections)
pool = redis.ConnectionPool(
    host='localhost',
    port=6379,
    max_connections=50,
    socket_timeout=1,
    socket_connect_timeout=1
)
r = redis.Redis(connection_pool=pool)

def get_user_good(user_id):
    user = r.get(f'user:{user_id}')
    return user
```

#### Pipelining (Batch Operations)

```python
# ❌ Bad: Round-trip for each operation
for i in range(1000):
    r.set(f'key:{i}', i)
# 1000 network round-trips (~500ms at 0.5ms each)

# ✅ Good: Pipeline (batch)
pipe = r.pipeline()
for i in range(1000):
    pipe.set(f'key:{i}', i)
pipe.execute()
# 1 network round-trip (~0.5ms)
```

#### Compression for Large Values

```python
import zlib
import json

def set_compressed(key, value, ttl=None):
    json_data = json.dumps(value)
    compressed = zlib.compress(json_data.encode())
    r.set(key, compressed, ex=ttl)

def get_compressed(key):
    compressed = r.get(key)
    if compressed:
        json_data = zlib.decompress(compressed).decode()
        return json.loads(json_data)
    return None

# 10 KB JSON → 1 KB compressed (10x savings)
```

---

## 6. Monitoring and Observability

### Key Metrics to Monitor

| Metric            | Target         | Alert Threshold | Why Important               |
|-------------------|----------------|-----------------|-----------------------------|
| **Hit Rate**      | > 80%          | < 70%           | Low hit rate = wasted cache |
| **Latency (p99)** | < 1ms          | > 5ms           | User experience impact      |
| **Memory Usage**  | < 80%          | > 90%           | Prevent eviction storms     |
| **Evicted Keys**  | Minimal        | > 1000/sec      | Indicates undersized cache  |
| **Expired Keys**  | Normal         | > 10000/sec     | Check TTL settings          |
| **Network I/O**   | < 70% capacity | > 80%           | Bottleneck indicator        |
| **Commands/sec**  | Baseline ±20%  | > 2x baseline   | Unusual traffic pattern     |

### Redis Monitoring Commands

```bash
# Get stats
redis-cli INFO stats
redis-cli INFO memory

# Monitor real-time commands
redis-cli MONITOR

# Check slow queries
redis-cli SLOWLOG GET 10

# Get memory usage per key
redis-cli --bigkeys

# Check latency
redis-cli --latency
```

---

## 7. Alternative Approaches

### Comparison: Redis vs Memcached vs KeyDB

| Feature             | Redis                                   | Memcached             | KeyDB                  | Best For                      |
|---------------------|-----------------------------------------|-----------------------|------------------------|-------------------------------|
| **Data Structures** | Rich (lists, sets, sorted sets, hashes) | Key-value only        | Same as Redis          | Complex data: Redis           |
| **Persistence**     | RDB, AOF                                | None                  | RDB, AOF               | Durability: Redis             |
| **Threading**       | Single-threaded                         | Multi-threaded        | Multi-threaded         | High CPU: KeyDB               |
| **Replication**     | Built-in                                | None (requires proxy) | Built-in               | HA: Redis/KeyDB               |
| **Clustering**      | Built-in                                | Client-side           | Built-in               | Scalability: Redis            |
| **Performance**     | 100K+ QPS                               | 300K+ QPS             | 200K+ QPS              | Pure speed: Memcached         |
| **Memory Overhead** | Higher (~20-30%)                        | Lower (~10%)          | Higher (~20-30%)       | Memory constrained: Memcached |
| **Use Case**        | General purpose                         | Pure caching          | High-performance Redis | See "Best For"                |

---

## 8. Trade-offs and Design Decisions Summary

| Decision         | Choice             | Alternative     | Why Chosen                          | Trade-off                            |
|------------------|--------------------|-----------------|-------------------------------------|--------------------------------------|
| **Partitioning** | Consistent Hashing | Modulo Hashing  | Minimal reshuffling                 | Slightly more complex                |
| **Replication**  | Master-Replica     | Masterless (AP) | Simple failover, strong consistency | Write bottleneck at master           |
| **Write Policy** | Cache-Aside        | Write-Through   | Low write latency                   | Temporary inconsistency              |
| **Eviction**     | LRU                | LFU             | Good balance, simple                | May evict hot data accessed long ago |
| **Persistence**  | None (optional)    | RDB/AOF         | Maximum performance                 | Data loss on crash                   |
| **Consistency**  | Eventual           | Strong          | Sub-ms latency                      | Stale reads possible                 |

---

## 9. Common Pitfalls and Anti-Patterns

### Anti-Pattern 1: Not Setting TTL

```python
# ❌ Bad: No TTL, cache grows forever
cache.set('user:123', user_data)

# ✅ Good: Always set TTL
cache.set('user:123', user_data, ttl=300)  # 5 minutes
```

### Anti-Pattern 2: Cache Stampede

Already covered in Section 3.6 with solutions.

### Anti-Pattern 3: Storing Large Objects

```python
# ❌ Bad: 10 MB object
cache.set('users:all', all_users_list)  # 10 MB!

# ✅ Good: Paginate or split
for i in range(0, len(all_users), 100):
    chunk = all_users[i:i+100]
    cache.set(f'users:page:{i//100}', chunk, ttl=300)
```

### Anti-Pattern 4: Using Cache as Primary Storage

```python
# ❌ Bad: Only in cache
cache.set('session:abc', session_data, ttl=3600)
# If cache fails, data is lost!

# ✅ Good: Cache as secondary storage
db.save_session(session_data)  # Primary
cache.set('session:abc', session_data, ttl=3600)  # Secondary
```

---

## 10. Final Architecture Diagram

```
┌─────────────────────────────────────────────────────────────────┐
│                      Application Layer                           │
│  ┌──────────────────────────────────────────────────────────┐   │
│  │  Cache Client Library                                    │   │
│  │  - Consistent Hashing (150 VNodes/node)                  │   │
│  │  - Connection Pooling (50 connections)                   │   │
│  │  - Retry Logic + Circuit Breaker                         │   │
│  └──────────────────────────────────────────────────────────┘   │
└───────────────────────┬─────────────────────────────────────────┘
                        │
        ┌───────────────┼───────────────┐
        │               │               │
        ▼               ▼               ▼
┌──────────────┐  ┌──────────────┐  ┌──────────────┐
│   Master 1   │  │   Master 2   │  │   Master 3   │
│   64 GB RAM  │  │   64 GB RAM  │  │   64 GB RAM  │
│ 100K QPS     │  │ 100K QPS     │  │ 100K QPS     │
│ Slots 0-5460 │  │ Slots 5461-  │  │ Slots 10923- │
└──────┬───────┘  └──────┬───────┘  └──────┬───────┘
       │                 │                 │
       │ Async           │ Async           │ Async
       │ Replication     │ Replication     │ Replication
       │ (1-5ms lag)     │ (1-5ms lag)     │ (1-5ms lag)
       │                 │                 │
   ┌───┴────┐        ┌───┴────┐        ┌───┴────┐
   │Replica │        │Replica │        │Replica │
   │  1A    │        │  2A    │        │  3A    │
   │ 64 GB  │        │ 64 GB  │        │ 64 GB  │
   └────────┘        └────────┘        └────────┘
   ┌────────┐        ┌────────┐        ┌────────┐
   │Replica │        │Replica │        │Replica │
   │  1B    │        │  2B    │        │  3B    │
   │ 64 GB  │        │ 64 GB  │        │ 64 GB  │
   └────────┘        └────────┘        └────────┘

┌─────────────────────────────────────────────────────────────┐
│              Sentinel Cluster (Monitoring)                   │
│  ┌──────────┐     ┌──────────┐     ┌──────────┐            │
│  │Sentinel 1│     │Sentinel 2│     │Sentinel 3│            │
│  │ Quorum=2 │     │ Quorum=2 │     │ Quorum=2 │            │
│  └──────────┘     └──────────┘     └──────────┘            │
└─────────────────────────────────────────────────────────────┘

Total Capacity:
- 9 nodes × 64 GB = 576 GB total RAM
- 3 master nodes = 192 GB effective capacity (with 3x replication)
- 300K+ QPS total throughput
- < 13 seconds failover time
- ~1% keys reshuffled on node add/remove
```

---

## 11. Capacity Planning Guidelines

### Scaling Strategy

| Metric      | When to Scale      | How to Scale                             |
|-------------|--------------------|------------------------------------------|
| **Memory**  | > 80% usage        | Add more nodes (horizontal)              |
| **CPU**     | > 70% usage        | Use KeyDB (multi-threaded) or add nodes  |
| **Network** | > 70% bandwidth    | Upgrade NIC, add nodes                   |
| **QPS**     | > 80K QPS per node | Add more master nodes                    |
| **Latency** | p99 > 5ms          | Optimize queries, add replicas for reads |

### Cost Estimation (AWS Example)

```
Instance Type: r6g.2xlarge (64 GB RAM, 8 vCPU)
Cost: $0.504/hour

9 nodes × $0.504/hour × 730 hours/month = $3,311/month

With 3-year reserved instance:
9 nodes × $0.303/hour × 730 hours/month = $1,991/month

Data Transfer: ~$0.01/GB (within AZ, negligible)

Total: ~$2,000-3,300/month for 180 GB cache capacity
```

---

## 12. Further Optimizations

### Advanced Techniques

1. **Read-Through Cache** (optional for specific use cases)
2. **Cache Warming** on startup
3. **Predictive Caching** (ML-based)
4. **Geo-distributed Caching** (CDN-like)
5. **Client-side Caching** (L1 cache)

### Redis Modules

- **RediSearch**: Full-text search
- **RedisJSON**: Native JSON support
- **RedisGraph**: Graph database
- **RedisTimeSeries**: Time-series data
- **RedisBloom**: Bloom filters, HyperLogLog

---

## Summary

A distributed cache system requires careful design decisions balancing:

- **Performance** (sub-millisecond latency)
- **Availability** (failover, replication)
- **Scalability** (consistent hashing, sharding)
- **Consistency** (eventual vs strong)

**Key Takeaways:**

1. Use **Consistent Hashing** with virtual nodes for minimal reshuffling
2. **Async replication** provides best performance for cache use cases
3. **Cache-Aside** pattern ideal for read-heavy workloads
4. **Single Flight Request** prevents cache stampede
5. **Monitor hit rate** and latency continuously
6. **Always set TTL** to prevent unbounded growth
7. Cache is **secondary storage** - database is source of truth

**Recommended Stack:**

- **Redis** for general purpose (rich features, good performance)
- **KeyDB** for extreme performance (multi-threaded)
- **Memcached** for pure key-value with minimal overhead
- **Redis Sentinel** for automatic failover
- **Redis Cluster** for horizontal scalability

