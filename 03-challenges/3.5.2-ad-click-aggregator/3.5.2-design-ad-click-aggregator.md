# 3.5.2 Design Ad Click Aggregator (Low-Latency Analytics)

> üìö **Note on Implementation Details:**
> This document focuses on high-level design concepts and architectural decisions.
> For detailed algorithm implementations, see **[pseudocode.md](./pseudocode.md)**.

## üìä Visual Diagrams & Resources

- **[High-Level Design Diagrams](./hld-diagram.md)** - System architecture, component design, data flow
- **[Sequence Diagrams](./sequence-diagrams.md)** - Detailed interaction flows and failure scenarios
- **[Design Decisions (This Over That)](./this-over-that.md)** - In-depth analysis of architectural choices
- **[Pseudocode Implementations](./pseudocode.md)** - Detailed algorithm implementations

---

## 1. Problem Statement

Design a highly scalable, low-latency ad click aggregation system that can ingest billions of click events per day,
provide real-time analytics for campaign dashboards, filter fraudulent clicks, and generate accurate billing reports.
The system must handle massive write throughput (500k events/sec), maintain sub-second dashboard latency, ensure no data
loss for billing, and operate cost-efficiently at petabyte scale.

**Core Challenges:**

- Handle 500k writes/sec with no data loss
- Provide real-time aggregations (< 5 second lag)
- Filter fraud before counting
- Generate accurate billing from historical data
- Store 15 PB of raw logs cost-effectively

---

## 2. Requirements and Scale Estimation

### Functional Requirements (FRs)

| Requirement                  | Description                                                          | Priority    |
|------------------------------|----------------------------------------------------------------------|-------------|
| **Click Ingestion**          | Ingest billions of ad click events per day with minimal latency      | Must Have   |
| **Real-Time Counting**       | Provide near real-time (seconds lag) counts for active campaigns     | Must Have   |
| **Fraud Filtering**          | Filter out bot traffic and fraudulent clicks before counting         | Must Have   |
| **Final Reporting**          | Provide accurate, eventual counts for billing and long-term analysis | Must Have   |
| **Time-Window Aggregations** | Support queries like "clicks in last 5 minutes"                      | Must Have   |
| **Campaign Analytics**       | Breakdown by campaign, ad group, creative, geo, device               | Must Have   |
| **Late Arrival Handling**    | Handle events that arrive late due to network delays                 | Should Have |

### Non-Functional Requirements (NFRs)

| Requirement                 | Target                                    | Rationale                                        |
|-----------------------------|-------------------------------------------|--------------------------------------------------|
| **High Write Throughput**   | 500k events/sec peak                      | Ad clicks are extremely write-heavy              |
| **Durability**              | 99.999% (no data loss)                    | Critical for billing reconciliation              |
| **Cost Efficiency**         | < $0.01 per 1M events                     | Massive volume requires cheap storage/processing |
| **Low Latency (Real-Time)** | < 5 seconds lag                           | Dashboard must feel real-time                    |
| **Query Latency**           | < 100ms (real-time), < 5 sec (historical) | Fast dashboard and reporting                     |
| **Availability**            | 99.99% uptime                             | Downtime = lost revenue data                     |

### Scale Estimation

| Metric                   | Assumption                                       | Calculation               | Result                                      |
|--------------------------|--------------------------------------------------|---------------------------|---------------------------------------------|
| **Peak Clicks (Writes)** | 500,000 events/sec                               | -                         | $500 \text{k}$ $\text{QPS}$ streaming input |
| **Total Daily Events**   | $500 \text{k}$ $\text{QPS}$ $\times 86400$ s/day | -                         | $43.2$ Billion events/day                   |
| **Average Throughput**   | Assume 20% peak                                  | $500 \text{k} \times 0.2$ | $100 \text{k}$ $\text{QPS}$ average         |
| **Event Size**           | Typical ad click event                           | JSON payload              | $\sim 1$ KB per event                       |
| **Daily Ingestion**      | $43.2 \text{B} \times 1$ KB                      | -                         | $43.2$ TB/day raw                           |
| **Storage (1 Year)**     | $43.2 \text{TB} \times 365$ days                 | -                         | $\sim 15$ PB/year (raw logs)                |
| **Kafka Throughput**     | Compressed events ($3:1$)                        | $43.2 \text{TB} / 3$      | $14.4$ TB/day ingested                      |
| **Active Campaigns**     | Typical ad network                               | -                         | $\sim 100,000$ active campaigns             |
| **Aggregation State**    | 100k campaigns $\times$ 100 dimensions           | -                         | $10$ million counters in memory             |

**Back-of-Envelope Calculations:**

```
Write Bandwidth:
500k events/sec √ó 1 KB = 500 MB/sec = 4 Gbps

Kafka Cluster Size:
- Replication factor: 3
- Retention: 7 days
- Raw: 43.2 TB/day √ó 7 √ó 3 = 907 TB
- With compression: 302 TB storage

Stream Processing:
- Flink workers: 50 instances (10k events/sec each)
- Memory per worker: 16 GB (state + heap)
- Total: 800 GB for stateful processing

Real-Time Storage (Redis):
- 10M counters √ó 100 bytes = 1 GB
- With metadata: ~5 GB total
```

---

## 3. High-Level Architecture

The system uses a **Kappa Architecture** (stream-only processing) with a **two-speed pipeline**: fast path for real-time
dashboards and slow path for accurate billing.

### 3.1 Architecture Diagram (ASCII)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         CLIENT LAYER                                 ‚îÇ
‚îÇ  Ad Networks, Publishers, Advertisers (500k clicks/sec)             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
                      ‚îÇ HTTP POST
                      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      INGESTION LAYER                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê             ‚îÇ
‚îÇ  ‚îÇ   NGINX     ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   API GW    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Validation ‚îÇ             ‚îÇ
‚îÇ  ‚îÇ (L4 Proxy)  ‚îÇ    ‚îÇ  (Stateless)‚îÇ    ‚îÇ   Service   ‚îÇ             ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                  ‚îÇ
                                                  ‚îÇ Kafka Producer
                                                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    STREAMING BACKBONE                                ‚îÇ
‚îÇ            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                      ‚îÇ
‚îÇ            ‚îÇ      Kafka Cluster               ‚îÇ                      ‚îÇ
‚îÇ            ‚îÇ  Topic: raw_clicks               ‚îÇ                      ‚îÇ
‚îÇ            ‚îÇ  Partitions: 100                 ‚îÇ                      ‚îÇ
‚îÇ            ‚îÇ  Replication: 3                  ‚îÇ                      ‚îÇ
‚îÇ            ‚îÇ  Retention: 7 days               ‚îÇ                      ‚îÇ
‚îÇ            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ                  ‚îÇ
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ FAST PATH                   SLOW PATH  ‚îÇ
       ‚ñº                                        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  REAL-TIME PROCESSING   ‚îÇ      ‚îÇ   BATCH PROCESSING          ‚îÇ
‚îÇ                         ‚îÇ      ‚îÇ                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ      ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ
‚îÇ  ‚îÇ Flink Cluster  ‚îÇ     ‚îÇ      ‚îÇ  ‚îÇ Spark Batch    ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ (50 workers)   ‚îÇ     ‚îÇ      ‚îÇ  ‚îÇ (Nightly runs) ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ                ‚îÇ     ‚îÇ      ‚îÇ  ‚îÇ                ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ - Fraud Filter ‚îÇ     ‚îÇ      ‚îÇ  ‚îÇ - Full scan    ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ - Aggregation  ‚îÇ     ‚îÇ      ‚îÇ  ‚îÇ - Deduplication‚îÇ         ‚îÇ
‚îÇ  ‚îÇ - Windowing    ‚îÇ     ‚îÇ      ‚îÇ  ‚îÇ - Joins        ‚îÇ         ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ      ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ
‚îÇ          ‚îÇ              ‚îÇ      ‚îÇ           ‚îÇ                 ‚îÇ
‚îÇ          ‚ñº              ‚îÇ      ‚îÇ           ‚ñº                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ      ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ
‚îÇ  ‚îÇ Redis Cluster  ‚îÇ     ‚îÇ      ‚îÇ  ‚îÇ  PostgreSQL    ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ (Real-time     ‚îÇ     ‚îÇ      ‚îÇ  ‚îÇ  (Billing DB)  ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ  counters)     ‚îÇ     ‚îÇ      ‚îÇ  ‚îÇ                ‚îÇ         ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ      ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                                    ‚îÇ
         ‚îÇ                                    ‚îÇ
         ‚ñº                                    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        STORAGE LAYER                                 ‚îÇ
‚îÇ                                                                      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ  ‚îÇ   S3 / Data Lake (Cold Storage)                           ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ   - Raw events (Parquet, compressed)                      ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ   - Retention: 3 years                                    ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ   - 15 PB total                                           ‚îÇ     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚îÇ Query
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                       QUERY LAYER                                    ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                         ‚îÇ
‚îÇ  ‚îÇ  Presto/Athena  ‚îÇ    ‚îÇ   Dashboard API ‚îÇ                         ‚îÇ
‚îÇ  ‚îÇ  (Ad-hoc query) ‚îÇ    ‚îÇ   (Real-time)   ‚îÇ                         ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 3.2 Data Flow

**Write Path (Real-Time):**

1. Client ‚Üí NGINX ‚Üí API Gateway ‚Üí Validation Service
2. Validation ‚Üí Kafka Producer ‚Üí raw_clicks topic
3. Flink consumes ‚Üí Fraud filter ‚Üí Aggregate ‚Üí Redis
4. Dashboard queries Redis for real-time counts

**Write Path (Batch):**

1. Kafka ‚Üí S3 sink (hourly dumps)
2. Spark batch job (nightly) reads S3
3. Full deduplication + fraud removal + joins
4. Write to PostgreSQL (billing DB)

---

## 4. Detailed Component Design

### 4.1 Ingestion Layer

**NGINX (L4 Load Balancer):**

- Handles 500k connections/sec
- Round-robin to API Gateway instances
- Connection pooling: 100k concurrent connections
- TCP keep-alive: 60 seconds

**API Gateway (Stateless):**

- Lightweight HTTP server (Go/Rust)
- Validates event schema (JSON)
- Rate limiting: 1000 events/sec per IP
- Adds server timestamp
- Returns 202 Accepted immediately (async)

**Validation Service:**

- Schema validation (required fields present)
- Range checks (timestamp within 5 minutes)
- Duplicate detection (bloom filter for last 1 hour)
- Enrichment: Add IP geolocation, user-agent parsing

**Event Schema:**

```json
{
  "event_id": "uuid",
  "timestamp": "2025-10-31T12:34:56.789Z",
  "campaign_id": "campaign_123",
  "ad_id": "ad_456",
  "publisher_id": "pub_789",
  "user_id": "user_hash",
  "ip_address": "1.2.3.4",
  "user_agent": "Mozilla/5.0...",
  "referer": "https://example.com",
  "device_type": "mobile",
  "os": "iOS",
  "country": "US",
  "city": "San Francisco",
  "cost_per_click": 0.50
}
```

### 4.2 Streaming Backbone (Kafka)

**Configuration:**

- Topic: `raw_clicks`
- Partitions: 100 (partitioned by `campaign_id` hash)
- Replication factor: 3
- Retention: 7 days (168 hours)
- Compression: LZ4 (3:1 ratio)
- Acks: all (ensure durability)
- Min in-sync replicas: 2

**Producer Settings:**

- Batch size: 16 KB
- Linger time: 10 ms (trade latency for throughput)
- Compression: LZ4
- Retries: 3
- Idempotence: enabled

**Why Partition by Campaign ID:**

- All events for one campaign go to same partition
- Enables sequential processing per campaign
- Simplifies state management in Flink
- Maintains event ordering per campaign

*See this-over-that.md for Kafka vs Kinesis vs Pulsar comparison*

### 4.3 Stream Processing (Flink)

**Flink Cluster:**

- 50 task managers (workers)
- 4 vCPU, 16 GB RAM each
- RocksDB state backend
- Checkpointing: every 60 seconds
- Parallelism: 100 (matches Kafka partitions)

**Processing Pipeline:**

```
Kafka Source
  ‚Üì
Fraud Detection Filter
  ‚Üì
Windowed Aggregation (5-minute tumbling windows)
  ‚Üì
Redis Sink (real-time counters)
  ‚Üì
S3 Sink (archival)
```

**Fraud Detection:**

1. **Bloom Filter Check:** Known bot IPs/user-agents
2. **Click Pattern Analysis:** Same IP > 100 clicks/min = suspicious
3. **Honeypot Detection:** Clicks on hidden ads
4. **Machine Learning Model:** Real-time scoring (risk 0-1)

*See pseudocode.md::fraud_detection_filter() for implementation*

**Aggregation Logic:**

- Tumbling windows: 5 minutes, 1 hour, 1 day
- Sliding windows: Last 5 minutes (updated every 10 seconds)
- Group by: campaign_id, ad_id, country, device_type
- Metrics: click_count, unique_users, total_cost

**State Management:**

- Use RocksDB for large state (10M counters)
- Incremental checkpoints to S3
- State TTL: 7 days (cleanup old campaigns)

*See pseudocode.md::windowed_aggregation() for implementation*

### 4.4 Real-Time Storage (Redis)

**Redis Cluster:**

- 6 nodes (3 masters + 3 replicas)
- Sharding: by campaign_id hash
- Memory: 64 GB per node = 384 GB total
- Eviction: allkeys-lru (least recently used)
- Persistence: RDB snapshots every 5 minutes

**Data Structures:**

```
Key Pattern: counter:{campaign_id}:{time_window}:{dimension}
Example: counter:campaign_123:5min:US:mobile

Value: Redis Hash
{
  "clicks": 15234,
  "unique_users": 8901,
  "cost": 7617.00,
  "last_updated": 1730383200
}

TTL: 24 hours (old windows expire)
```

**Query Pattern:**

```
GET counter:campaign_123:5min:US:mobile
‚Üí Returns: {"clicks": 15234, "unique_users": 8901, ...}

MGET counter:campaign_123:1hour:* (scan pattern)
‚Üí Returns: All hourly breakdowns for campaign_123
```

**Write Performance:**

- Pipeline writes: 100 commands per batch
- Throughput: 100k writes/sec per node
- Latency: < 1ms per write

### 4.5 Cold Storage (S3 Data Lake)

**S3 Structure:**

```
s3://ad-clicks-data-lake/
‚îú‚îÄ‚îÄ raw/
‚îÇ   ‚îú‚îÄ‚îÄ year=2025/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ month=10/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ day=31/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hour=12/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ clicks-12-00.parquet.snappy
```

**File Format:**

- Parquet columnar format (10x compression vs JSON)
- Snappy compression (fast read/write)
- File size: 128 MB (optimal for Spark)
- Schema evolution supported

**Partitioning:**

- Partition by: year, month, day, hour
- Enables efficient time-based queries
- S3 Select for column pruning

**Lifecycle Policy:**

- Hot (last 30 days): S3 Standard
- Warm (30-180 days): S3 Infrequent Access
- Cold (180+ days): S3 Glacier
- Delete after 3 years

**Cost:**

- S3 Standard: $0.023/GB/month
- 15 PB √ó $0.023 = $345k/month (without lifecycle)
- With lifecycle: ~$100k/month

### 4.6 Batch Processing (Spark)

**Spark Cluster:**

- 100 executors
- 8 vCPU, 32 GB RAM each
- Runs nightly (off-peak hours)
- EMR Spot Instances (80% cost savings)

**Batch Pipeline:**

```
S3 (Parquet files)
  ‚Üì
Spark Read (distributed)
  ‚Üì
Deduplication (by event_id)
  ‚Üì
Fraud Removal (ML model inference)
  ‚Üì
Join with Advertiser Data (campaign metadata)
  ‚Üì
Aggregate by Campaign/Day
  ‚Üì
Write to PostgreSQL (billing DB)
```

**Deduplication:**

- Group by event_id
- Keep first occurrence (by timestamp)
- Handle late arrivals (events up to 7 days late)

**Accuracy Priority:**

- Batch = source of truth for billing
- Real-time = approximate, fast
- Reconciliation: Compare batch vs real-time, flag discrepancies > 5%

*See pseudocode.md::batch_deduplication() for implementation*

### 4.7 Billing Database (PostgreSQL)

**Schema:**

```sql
CREATE TABLE campaign_daily_stats (
    campaign_id BIGINT,
    date DATE,
    clicks BIGINT,
    unique_users BIGINT,
    total_cost DECIMAL(12,2),
    fraudulent_clicks BIGINT,
    country VARCHAR(2),
    device_type VARCHAR(20),
    PRIMARY KEY (campaign_id, date, country, device_type)
);

CREATE INDEX idx_campaign_date ON campaign_daily_stats(campaign_id, date);
```

**Partitioning:**

- Range partition by date (monthly partitions)
- Keeps query fast for recent data
- Archive old partitions to cold storage

**Replication:**

- 1 primary + 2 replicas
- Async replication (billing queries can be slightly stale)
- Read replicas for reporting queries

---

## 5. Fraud Detection and Prevention

### 5.1 Real-Time Fraud Detection (Stream Processing)

**Bloom Filter (Known Bad Actors):**

- Size: 100 million entries
- False positive rate: 0.1%
- Memory: ~150 MB
- Updated hourly from fraud DB

**Click Pattern Analysis:**

| Pattern                | Threshold                       | Action             |
|------------------------|---------------------------------|--------------------|
| **High Frequency**     | > 100 clicks/min from same IP   | Flag as bot        |
| **Sequential Clicks**  | > 10 clicks in < 1 second       | Flag as automated  |
| **Impossible Geo**     | IP location != claimed location | Flag as suspicious |
| **User-Agent Anomaly** | Known bot user-agent            | Drop immediately   |

**Machine Learning Model:**

- Random Forest classifier (trained offline)
- Features: IP, user-agent, click_rate, time_of_day, referer
- Inference latency: < 5ms
- Model updated weekly

**Risk Score:**

- 0.0-0.3: Clean (count normally)
- 0.3-0.7: Suspicious (count but flag for review)
- 0.7-1.0: Fraud (drop immediately)

*See pseudocode.md::ml_fraud_scoring() for implementation*

### 5.2 Batch Fraud Detection (Offline)

**Advanced Analysis:**

- Click farms: Clusters of IPs with similar patterns
- Click injection: Clicks without corresponding impressions
- Attribution fraud: Clicks from unrelated referrers
- Time-based analysis: Unusual traffic spikes

**Post-Processing:**

- Retrospective fraud removal
- Chargeback fraudulent clicks
- Update advertiser invoices
- Refine ML model with new fraud patterns

---

## 6. Availability and Fault Tolerance

### 6.1 Single Points of Failure

| Component         | Impact if Failed        | Mitigation                                      |
|-------------------|-------------------------|-------------------------------------------------|
| **Kafka Cluster** | No event ingestion      | 3-broker replication, cross-AZ deployment       |
| **Flink Cluster** | Real-time lag increases | Checkpointing, auto-restart, horizontal scaling |
| **Redis Cluster** | Dashboard unavailable   | Master-replica, Redis Sentinel failover         |
| **API Gateway**   | No event acceptance     | Stateless, auto-scaling, load balanced          |

### 6.2 Disaster Recovery

**Kafka:**

- Mirror Maker 2: Replicate to backup region
- Retention: 7 days (time to recover)
- Backup to S3: Every 6 hours

**Flink:**

- Checkpoints to S3: Every 60 seconds
- Recovery time: < 5 minutes
- Exactly-once semantics (no duplicate counting)

**Redis:**

- RDB snapshots: Every 5 minutes
- AOF (Append-Only File): Sync every second
- Recovery: Rebuild from Kafka (7 days history)

**S3:**

- Cross-region replication
- Versioning enabled
- 99.999999999% durability

### 6.3 Backpressure Handling

**Kafka Full:**

- Producers buffer in memory (up to 128 MB)
- Exponential backoff retries
- Circuit breaker: Reject writes after 10 seconds

**Flink Lag:**

- Monitor consumer lag metric
- Auto-scale workers if lag > 5 minutes
- Alert if lag > 15 minutes

**Redis Full:**

- LRU eviction (least recently used)
- Acceptable: Old campaigns evicted
- Dashboard shows "data unavailable" for evicted keys

---

## 7. Bottlenecks and Optimizations

### 7.1 Ingestion Bottleneck

**Problem:** NGINX cannot handle 500k connections/sec.

**Solution:**

- Use L4 load balancer (AWS NLB) instead of L7
- Connection pooling: Reuse TCP connections
- HTTP/2: Multiplexing (multiple requests per connection)

### 7.2 Kafka Write Throughput

**Problem:** Kafka brokers saturate at 300k writes/sec.

**Solution:**

- Add more brokers (horizontal scaling)
- Increase partitions: 100 ‚Üí 200
- Use faster disks (NVMe SSDs)
- Batch producer writes (16 KB batches)

### 7.3 Flink State Size

**Problem:** 10M counters √ó 100 bytes = 1 GB state per worker.

**Solution:**

- Use RocksDB (disk-backed state)
- Incremental checkpoints (only changed state)
- State TTL: Remove inactive campaigns after 7 days
- Compress state with Snappy

### 7.4 Query Performance

**Problem:** Dashboard queries slow (> 1 second).

**Solution:**

- Pre-aggregate in Flink (don't query raw Redis keys)
- Use Redis pipelining (batch 100 GET commands)
- Cache popular queries in CDN (1-minute TTL)
- Use materialized views for common breakdowns

---

## 8. Common Anti-Patterns

### ‚ùå Anti-Pattern 1: Synchronous API Response

**Problem:**
API waits for Kafka write confirmation before responding to client.

**Bad:**

```
POST /click ‚Üí Write to Kafka (50ms) ‚Üí Return 200 OK
Total latency: 50ms+ per request
```

**Good:**

```
POST /click ‚Üí Buffer in memory ‚Üí Return 202 Accepted (< 1ms)
Async: Flush buffer to Kafka every 10ms
```

**Why:** At 500k QPS, synchronous writes cause API saturation.

### ‚ùå Anti-Pattern 2: Real-Time Billing

**Problem:**
Using real-time counts for billing invoices.

**Why It's Wrong:**

- Real-time has ~2% error rate (duplicates, fraud)
- Late arrivals not included
- Redis evictions cause missing data

**Solution:**
Use batch processing for billing (source of truth).

### ‚ùå Anti-Pattern 3: No Fraud Filtering

**Problem:**
Count all clicks blindly.

**Impact:**

- 10-30% of clicks are fraud
- Advertisers overpay
- Legal liability

**Solution:**
Multi-stage fraud detection (Bloom filter + ML + batch analysis).

### ‚ùå Anti-Pattern 4: Lambda Architecture

**Problem:**
Separate batch and stream codebases.

**Why It's Wrong:**

- Code duplication
- Hard to maintain consistency
- Bugs in one path not in other

**Solution:**
Kappa architecture (unified stream processing).

---

## 9. Alternative Approaches

### 9.1 Lambda Architecture

**What:**
Separate batch layer and stream layer, merge results.

**Pros:**

- Historical reprocessing easy
- Fault tolerance via batch

**Cons:**

- Code duplication
- Complex merge logic
- Eventual consistency issues

**When to Use:**
If batch requirements differ significantly from real-time.

### 9.2 Kinesis Instead of Kafka

**Pros:**

- Managed service (less ops)
- Auto-scaling
- AWS integration

**Cons:**

- Higher cost (10x vs Kafka)
- Shard limit (1 MB/sec per shard)
- Vendor lock-in

**Decision:** Kafka chosen for cost and throughput.

*See this-over-that.md::kafka-vs-kinesis for detailed comparison*

### 9.3 ClickHouse for Real-Time Storage

**Pros:**

- Columnar OLAP database
- Fast aggregations
- SQL queries

**Cons:**

- Higher write latency (> 1 second)
- Complex cluster management
- Memory overhead

**Decision:** Redis chosen for sub-millisecond latency.

---

## 10. Monitoring and Observability

### 10.1 Key Metrics

| Metric                 | Target          | Alert Threshold                        |
|------------------------|-----------------|----------------------------------------|
| **Ingestion Rate**     | 500k events/sec | < 400k/sec (capacity issue)            |
| **Kafka Lag**          | < 1 minute      | > 5 minutes (consumer slow)            |
| **Flink Backpressure** | 0%              | > 50% (needs scaling)                  |
| **Redis Memory**       | < 80%           | > 90% (eviction risk)                  |
| **Fraud Rate**         | 10-20%          | > 30% (attack) or < 5% (filter broken) |
| **Batch Job Duration** | < 2 hours       | > 3 hours (data accumulation)          |
| **Real-Time Accuracy** | 98%+ vs batch   | < 95% (reconciliation needed)          |

### 10.2 Distributed Tracing

**Trace Path:**

```
API Gateway ‚Üí Kafka Producer ‚Üí Flink Consumer ‚Üí Redis
```

**Span Tags:**

- campaign_id
- event_id
- partition_id
- processing_time_ms

**Tool:** Jaeger or OpenTelemetry

### 10.3 Alerting

**Critical Alerts:**

- Kafka cluster down
- Flink job failed
- S3 sink stopped
- Fraud rate spike

**Warning Alerts:**

- Kafka lag > 5 minutes
- Redis memory > 80%
- Batch job slow

---

## 11. Trade-offs Summary

| Decision                           | What We Gain                  | What We Sacrifice                 |
|------------------------------------|-------------------------------|-----------------------------------|
| **Kappa Architecture**             | ‚úÖ Unified codebase            | ‚ùå Harder batch reprocessing       |
| **Kafka Partitioning by Campaign** | ‚úÖ Event ordering per campaign | ‚ùå Uneven partition sizes          |
| **Redis for Real-Time**            | ‚úÖ Sub-ms query latency        | ‚ùå Eventual consistency, evictions |
| **S3 for Cold Storage**            | ‚úÖ Cheap, durable              | ‚ùå Slow query (seconds)            |
| **Bloom Filter Fraud**             | ‚úÖ Fast lookup (O(1))          | ‚ùå 0.1% false positives            |
| **Async API Response**             | ‚úÖ High throughput             | ‚ùå No immediate confirmation       |
| **Batch for Billing**              | ‚úÖ 100% accuracy               | ‚ùå 24-hour lag                     |
| **Flink Checkpointing**            | ‚úÖ Exactly-once semantics      | ‚ùå 60-second recovery window       |

---

## 12. Real-World Examples

### Google Ads

**Architecture:**

- Dremel (Presto-like) for ad-hoc queries
- Flume for log ingestion
- Bigtable for real-time counters
- MapReduce for batch billing

**Scale:**

- 100+ billion clicks/day
- 10 PB/day ingestion

### Facebook Ads

**Architecture:**

- Scribe for event ingestion
- Puma for real-time aggregation
- Hive for batch processing
- Scuba for exploratory analytics

**Key Innovation:**

- Custom ML models for fraud detection
- 3-tier storage (hot/warm/cold)

### Amazon Advertising

**Architecture:**

- Kinesis for event streaming
- EMR (Spark) for batch processing
- DynamoDB for real-time counters
- S3 + Athena for queries

**Scale:**

- 50+ billion events/day
- Sub-second dashboard latency

---

## 13. References

### Related System Design Components

- **[2.3.2 Apache Kafka Deep Dive](../../02-components/2.3-messaging-streaming/2.3.2-apache-kafka-deep-dive.md)** -
  Event streaming
- **[2.3.7 Apache Spark Deep Dive](../../02-components/2.3-messaging-streaming/2.3.7-apache-spark-deep-dive.md)** -
  Batch processing
- **[2.3.8 Apache Flink Deep Dive](../../02-components/2.3-messaging-streaming/2.3.8-apache-flink-deep-dive.md)** -
  Stream processing
- **[2.2.1 Redis Deep Dive](../../02-components/2.2-caching/2.2.1-redis-deep-dive.md)** - Real-time storage
- **[2.5.4 Bloom Filter](../../02-components/2.5-algorithms/2.5.4-bloom-filter.md)** - Fraud detection

### Related Design Challenges

- **[3.4.3 Monitoring System](../3.4.3-monitoring-system/)** - Metrics aggregation patterns
- **[3.4.2 News Feed](../3.4.2-news-feed/)** - Real-time aggregation
- **[3.3.3 Flash Sale](../3.3.3-flash-sale/)** - High write throughput
- **[3.5.1 Payment Gateway](../3.5.1-payment-gateway/)** - Fraud detection, accuracy

### External Resources

- **Kappa Architecture:** [Jay Kreps' Blog](https://www.oreilly.com/radar/questioning-the-lambda-architecture/)
- **Flink State Management:
  ** [Apache Flink Docs](https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/fault-tolerance/state/)
- **Kafka at Scale:** [LinkedIn Engineering Blog](https://engineering.linkedin.com/kafka/running-kafka-scale)
- **Real-Time Analytics:
  ** [Netflix Tech Blog](https://netflixtechblog.com/keystone-real-time-stream-processing-platform-a3ee651812a)

### Books

- *Streaming Systems* by Tyler Akidau - Windowing, watermarks, late data
- *Designing Data-Intensive Applications* by Martin Kleppmann - Stream vs batch
- *Kafka: The Definitive Guide* by Neha Narkhede - Kafka architecture

---

## 14. Deployment and Infrastructure

### 14.1 Kubernetes Deployment

**API Gateway:**

```yaml
Deployment:
  - Replicas: 20-100 (autoscaling)
  - Resources: 2 vCPU, 4 GB RAM
  - HPA target: CPU 70%
  - Liveness probe: /health every 10s
```

**Flink:**

- JobManager: 1 replica (4 vCPU, 8 GB RAM)
- TaskManager: 50 replicas (4 vCPU, 16 GB RAM each)
- Checkpoints to S3
- Restart strategy: Exponential backoff

**Redis:**

- StatefulSet: 6 pods (3 masters + 3 replicas)
- PVC: 100 GB SSD per pod
- Redis Sentinel for failover

### 14.2 AWS Infrastructure

**VPC:**

- 3 Availability Zones
- Private subnets for Kafka, Flink, Redis
- Public subnets for API Gateway
- VPC peering to S3

**Kafka (MSK):**

- kafka.m5.4xlarge: 10 brokers
- 2 TB storage per broker
- Multi-AZ deployment

**Flink (EMR):**

- m5.2xlarge: 50 task nodes
- Spot instances for cost savings
- Auto-scaling based on backlog

**Redis (ElastiCache):**

- cache.r6g.2xlarge: 6 nodes
- 54 GB memory per node
- Multi-AZ with automatic failover

**S3:**

- Bucket: ad-clicks-data-lake
- Lifecycle: Standard ‚Üí IA ‚Üí Glacier
- Cross-region replication

### 14.3 Multi-Region Strategy

**Primary Region:** US-East-1 (80% traffic)
**Secondary Region:** EU-West-1 (20% traffic)

**Data Strategy:**

- Regional ingestion (low latency)
- Kafka MirrorMaker for replication
- S3 cross-region replication (async)
- Batch processing in primary region only

---

## 15. Advanced Features

### 15.1 Real-Time Anomaly Detection

**Use Case:** Detect unusual traffic patterns in real-time.

**Implementation:**

- Sliding window: Track clicks/min for each campaign
- Baseline: Average of last 7 days
- Alert if current > 3x baseline

**Benefits:**

- Catch DDoS attacks
- Detect viral campaigns
- Early fraud detection

### 15.2 Cost Attribution

**Use Case:** Break down costs by advertiser, campaign, creative.

**Implementation:**

- Store cost_per_click in event
- Aggregate: SUM(cost_per_click) GROUP BY dimension
- Store in separate Redis keys

**Example:**

```
cost:advertiser:123:daily ‚Üí $12,345.67
cost:campaign:456:hourly ‚Üí $234.56
```

### 15.3 A/B Testing Support

**Use Case:** Test ad variants.

**Implementation:**

- Add variant_id to event
- Aggregate clicks by variant
- Calculate conversion rate per variant

**Schema:**

```
ab_test_stats: {
  "test_id": "test_789",
  "variant_a_clicks": 5000,
  "variant_b_clicks": 5200,
  "variant_a_conversions": 250,
  "variant_b_conversions": 280
}
```

### 15.4 Click Heatmaps

**Use Case:** Visualize where users click on ads.

**Implementation:**

- Add click_x, click_y coordinates to event
- Aggregate: Group by (x, y) buckets
- Generate heatmap image

**Storage:**

```
heatmap:ad_123:coordinates ‚Üí [{x: 100, y: 50, count: 45}, ...]
```

---

## 16. Performance Optimization

### 16.1 Kafka Tuning

**Producer:**

- batch.size: 32 KB
- linger.ms: 50 ms
- compression.type: lz4
- acks: 1 (performance over durability for real-time)

**Consumer:**

- fetch.min.bytes: 1 MB
- max.poll.records: 10000
- session.timeout.ms: 30000

### 16.2 Flink Tuning

**Memory:**

- taskmanager.memory.managed.fraction: 0.4
- taskmanager.memory.network.fraction: 0.1

**State:**

- state.backend: rocksdb
- state.backend.incremental: true
- state.backend.rocksdb.block.cache-size: 256 MB

**Checkpointing:**

- execution.checkpointing.interval: 60s
- execution.checkpointing.mode: EXACTLY_ONCE
- execution.checkpointing.timeout: 10min

### 16.3 Redis Optimization

**Memory:**

- maxmemory-policy: allkeys-lru
- maxmemory: 50 GB (leave 20% headroom)

**Persistence:**

- save 300 1 (save if 1 key changed in 5 min)
- appendonly: no (RDB only for speed)

**Pipelining:**

- Batch 100 commands per roundtrip
- Use MGET for bulk reads

### 16.4 S3 Performance

**Writes:**

- Use S3 Transfer Acceleration
- Multipart upload for files > 100 MB
- Parallel uploads (10 threads)

**Reads:**

- S3 Select for column pruning
- Parquet predicate pushdown
- Cache frequently accessed files in EBS

---

## 17. Interview Discussion Points

### 17.1 Common Interview Questions

**Q1: Why Kafka instead of SQS?**

**Answer:**

- **Throughput:** Kafka handles 500k writes/sec, SQS limited to 3000/sec per queue
- **Retention:** Kafka retains 7 days for replay, SQS max 14 days
- **Ordering:** Kafka guarantees partition ordering, SQS FIFO has throughput limits
- **Cost:** Kafka $0.01/GB, SQS $0.40/million requests

**Q2: How do you handle late-arriving events?**

**Answer:**

- **Real-time:** Use watermarks (Flink), allow 5-minute grace period
- **Batch:** Reprocess last 7 days nightly to catch late arrivals
- **Trade-off:** Real-time slightly inaccurate, batch is source of truth

**Q3: How do you prevent data loss?**

**Answer:**

- **Kafka:** Replication factor 3, acks=all, min.insync.replicas=2
- **Flink:** Checkpointing to S3 every 60 seconds, exactly-once semantics
- **S3:** Cross-region replication, 99.999999999% durability

**Q4: What if Flink crashes?**

**Answer:**

- **Recovery:** Restart from last checkpoint (60-second lag max)
- **State:** RocksDB persisted to S3, restore on restart
- **Idempotence:** Kafka consumer offsets stored in checkpoint

### 17.2 Scalability Scenarios

**Scenario 1: Traffic doubles to 1M events/sec**

**Actions:**

1. Double Kafka partitions: 100 ‚Üí 200
2. Double Flink workers: 50 ‚Üí 100
3. Add Kafka brokers: 10 ‚Üí 15
4. Increase Redis memory: 384 GB ‚Üí 768 GB

**Cost:** ~$50k/month additional

**Scenario 2: Need to support 1 million active campaigns**

**Actions:**

1. Increase Flink state: Use Flink SQL for stateless processing
2. Redis: Shard by campaign_id hash (add 6 more nodes)
3. Batch processing: Partition PostgreSQL by campaign_id

### 17.3 Trade-Off Discussions

**Q: Real-time vs Accuracy?**

**Answer:**

- **Real-time:** 2% error acceptable for dashboards
- **Batch:** 100% accurate for billing
- **Decision:** Two-speed pipeline, batch as source of truth

**Q: Cost vs Latency?**

**Answer:**

- **Low latency:** Use Flink + Redis (~$30k/month)
- **Low cost:** Use batch only (~$5k/month)
- **Decision:** Hybrid approach, real-time for UX, batch for billing