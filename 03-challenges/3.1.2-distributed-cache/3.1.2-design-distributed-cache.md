# 3.1.2 Design a Distributed Cache (Redis/Memcached)

> 📚 **Note on Implementation Details:**
> This document focuses on high-level design concepts and architectural decisions. 
> For detailed algorithm implementations, see **[pseudocode.md](./pseudocode.md)**.

## 📊 Visual Diagrams & Resources

- **[High-Level Design Diagrams](./hld-diagram.md)** - System architecture, component design, data flow
- **[Sequence Diagrams](./sequence-diagrams.md)** - Detailed interaction flows and failure scenarios
- **[Design Decisions (This Over That)](./this-over-that.md)** - In-depth analysis of architectural choices
- **[Pseudocode Implementations](./pseudocode.md)** - Detailed algorithm implementations

---

## 1. Problem Statement

Design a highly available, horizontally scalable, distributed in-memory cache system similar to Redis or Memcached. The
system should support millions of operations per second with sub-millisecond latency while providing fault tolerance and
minimal data reshuffling when nodes are added or removed.

---

## 2. Requirements and Scale Estimation

### Functional Requirements (FRs)

| Requirement                | Description                                                               | Priority     |
|----------------------------|---------------------------------------------------------------------------|--------------|
| **Key-Value Operations**   | Support `GET(key)`, `PUT(key, value)`, `DELETE(key)` with O(1) complexity | Must Have    |
| **Time-to-Live (TTL)**     | Keys must expire after a specified duration automatically                 | Must Have    |
| **Horizontal Scalability** | Add/remove cache nodes dynamically without downtime                       | Must Have    |
| **Data Types**             | Support strings, lists, sets, sorted sets, hashes                         | Should Have  |
| **Atomic Operations**      | Support atomic increment/decrement, compare-and-swap                      | Should Have  |
| **Persistence (Optional)** | Option to persist data to disk for durability                             | Nice to Have |

### Non-Functional Requirements (NFRs)

| Requirement             | Target                                     | Rationale                                     |
|-------------------------|--------------------------------------------|-----------------------------------------------|
| **Low Latency**         | < 1 ms (p99)                               | In-memory operations must be fast             |
| **High Throughput**     | > 100K QPS per node                        | Support high-traffic applications             |
| **High Availability**   | 99.99% uptime                              | Single node failure should not affect service |
| **Minimal Reshuffling** | < 1% keys moved when adding/removing nodes | Preserve cache hit ratio                      |
| **Memory Efficiency**   | < 20% metadata overhead                    | Maximize data storage                         |

---

### Scale Estimation

#### Scenario: Large E-commerce Platform

| Metric                | Assumption                                 | Calculation                 | Result                 |
|-----------------------|--------------------------------------------|-----------------------------|------------------------|
| **Total Data**        | 10% of 1.8 TB database in cache            | 1.8 TB × 0.1                | **~180 GB RAM needed** |
| **Average Key Size**  | Key: 64 bytes, Value: 1 KB                 | 64 B + 1 KB                 | ~1 KB per entry        |
| **Total Keys**        | 180 GB / 1 KB                              | 180 GB / 1 KB               | **~180 million keys**  |
| **Peak Read QPS**     | 80% of DB reads (from 1157 QPS baseline)   | 1157 × 0.8 × 5 (burst)      | **~4,600 reads/sec**   |
| **Peak Write QPS**    | 20% of reads (cache invalidation)          | 4,600 × 0.2                 | **~920 writes/sec**    |
| **Network Bandwidth** | 1 KB × 4,600 QPS                           | 1 KB × 4,600                | **~4.6 MB/sec**        |
| **Nodes Required**    | 180 GB / 64 GB per node (with replication) | 180 GB / 64 GB × 3 replicas | **~9 nodes**           |

#### Back-of-the-Envelope Calculations

```
Memory per Node = 64 GB RAM
Replication Factor = 3 (1 primary + 2 replicas)
Total Nodes = (180 GB / 64 GB) × 3 = ~9 nodes

QPS per Node = 4,600 QPS / 3 primaries = ~1,533 QPS/node
This is well within Redis capacity (100K+ QPS per node)

Network: 4.6 MB/sec is negligible (1 Gbps = 125 MB/sec)
```

---

## 3. High-Level Architecture

### System Overview

```
┌─────────────────────────────────────────────────────────────────┐
│                     Application Servers                         │
│  ┌────────────────────────────────────────────────────────┐     │
│  │  Cache Client Library (Consistent Hashing Logic)       │     │
│  └────────────────────────────────────────────────────────┘     │
└───────────────────────┬─────────────────────────────────────────┘
                        │
        ┌───────────────┼───────────────┐
        │               │               │
        ▼               ▼               ▼
    ┌───────┐       ┌───────┐       ┌───────┐
    │ Node 1│       │ Node 2│       │ Node 3│  ← Primary Nodes
    │(Master)       │(Master)       │(Master)
    └───┬───┘       └───┬───┘       └───┬───┘
        │               │               │
    ┌───┴───┐       ┌───┴───┐       ┌───┴───┐
    │Replica│       │Replica│       │Replica│  ← Replica Nodes
    │  1A   │       │  2A   │       │  3A   │
    └───────┘       └───────┘       └───────┘
    ┌───────┐       ┌───────┐       ┌───────┐
    │Replica│       │Replica│       │Replica│
    │  1B   │       │  2B   │       │  3B   │
    └───────┘       └───────┘       └───────┘

Optional: Sentinel / Cluster Manager
┌────────────────────────────────────────────┐
│  Sentinel 1  │  Sentinel 2  │  Sentinel 3  │
│  (Monitors health and performs failover)   │
└────────────────────────────────────────────┘
```

### Key Components

| Component               | Responsibility                                  | Technology Options           |
|-------------------------|-------------------------------------------------|------------------------------|
| **Client Library**      | Consistent hashing, routing, connection pooling | jedis, lettuce, go-redis     |
| **Cache Node**          | In-memory key-value storage                     | Redis, Memcached, KeyDB      |
| **Replication**         | Data redundancy, failover                       | Master-Replica (async/sync)  |
| **Cluster Manager**     | Health monitoring, failover orchestration       | Redis Sentinel, Cluster mode |
| **Configuration Store** | Node discovery, cluster topology                | etcd, Consul, ZooKeeper      |

---

## 4. Detailed Component Design

### 3.1 Data Partitioning Strategy

#### Why Consistent Hashing?

**Problem with Simple Modulo Hashing:**

```
// Simple hash: node = hash(key) % N
// If N = 3 nodes:
node = hash("user:123") % 3  // Goes to node 1

// Add 1 node (N = 4):
node = hash("user:123") % 4  // Now goes to node 3! ❌

// Result: ~75% of keys remapped → cache invalidation cascade!
```

**Solution: Consistent Hashing**

```
**Consistent Hashing Implementation:**

**Data Structures:**
- `ring`: HashMap mapping hash values to node identifiers
- `sorted_keys`: Sorted list of all hash values for binary search
- `virtual_nodes`: 150 virtual nodes per physical node

**Key Operations:**
- `initialize(nodes)`: Creates ring with virtual nodes for each physical node
- `add_node(node)`: Adds 150 virtual nodes to ring (O(V log N))
- `get_node(key)`: Finds responsible node via binary search (O(log N))
- `hash_function(key)`: MD5/MurmurHash for uniform distribution

**Example:**
- Setup: 3 nodes (node1, node2, node3)
- Query: `get_node('user:123')` → Returns 'node2'
- Add node4: Only ~25% of keys remapped (vs. 75% with modulo)

*See `pseudocode.md::ConsistentHash` for detailed implementation*
```

#### Virtual Nodes (VNodes) Impact

| VNodes per Node | Distribution Uniformity   | Lookup Overhead | Recommendation     |
|-----------------|---------------------------|-----------------|--------------------|
| 10-50           | Poor (±20% imbalance)     | Very Low        | Not recommended    |
| 100-200         | Good (±5% imbalance)      | Low             | **Recommended**    |
| 500-1000        | Excellent (±1% imbalance) | Medium          | For large clusters |
| 5000+           | Perfect                   | High            | Overkill           |

---

### 3.2 Replication Strategy

#### Master-Replica (Leader-Follower) Architecture

```
┌─────────────────────────────────────────────────────────┐
│                    Write Request                        │
└──────────────────────┬──────────────────────────────────┘
                       │
                       ▼
              ┌─────────────────┐
              │  Master Node    │
              │  (Primary)      │
              │  - Handles writes
              │  - Replicates   │
              └────────┬────────┘
                       │
           ┌───────────┴───────────┐
           │ Async Replication     │
           │ (non-blocking)        │
           ▼                       ▼
    ┌──────────────┐        ┌──────────────┐
    │  Replica 1   │        │  Replica 2   │
    │  (Read-only) │        │  (Read-only) │
    │  - Lags 1-5ms│        │  - Lags 1-5ms│
    └──────────────┘        └──────────────┘
           │                       │
           └───────────┬───────────┘
                       │
              Read Requests (Load Balanced)
```

#### Replication Options Comparison

| Strategy              | Latency         | Data Loss Risk      | Availability | Best For                          |
|-----------------------|-----------------|---------------------|--------------|-----------------------------------|
| **Async Replication** | Very Low (<1ms) | Low (1-5ms of data) | High         | **Recommended for cache**         |
| **Sync Replication**  | High (2-10ms)   | None                | Medium       | Financial systems (not cache)     |
| **Semi-Sync**         | Medium (1-5ms)  | Very Low            | High         | Balance consistency & performance |
| **No Replication**    | Lowest          | Complete on failure | Low          | Dev/test only                     |

**Why Async for Cache?**

- Cache is **not the source of truth** (database is)
- Sub-millisecond writes critical for performance
- 1-5ms of data loss acceptable (cache can be repopulated from DB)
- Losing 5ms of cached writes ≠ losing database writes

---

### 3.3 Write Policies

#### Cache-Aside (Lazy Loading) - Recommended for Distributed Cache

**How It Works:**

**Read Flow:**
1. Application checks cache first for the data
2. **Cache Hit:** Return data immediately (fast path)
3. **Cache Miss:** 
   - Fetch from database (slow path)
   - Populate cache with fetched data (TTL = 5 minutes)
   - Return data to application
4. Subsequent reads hit cache (fast)

**Write Flow:**
1. Update database first (source of truth)
2. **Invalidate** cache entry (don't update it!)
3. Next read will fetch fresh data and populate cache

**Why Invalidate Instead of Update:**
- **Simpler:** No serialization issues
- **Safer:** Avoids inconsistency if DB update succeeds but cache update fails
- **Lazy:** Don't waste effort on data that might not be read again

*See `pseudocode.md::get_from_cache_aside()` and `pseudocode.md::update_with_cache_aside()` for detailed implementation*

**Why Cache-Aside Over Write-Through?**

| Aspect            | Cache-Aside                  | Write-Through                             |
|-------------------|------------------------------|-------------------------------------------|
| **Write Latency** | Low (DB only)                | High (DB + Cache sync)                    |
| **Cache Size**    | Smaller (only accessed data) | Larger (all written data)                 |
| **Consistency**   | Eventual                     | Strong                                    |
| **Best For**      | **Read-heavy workloads**     | Write-heavy, immediate consistency needed |

---

### 3.4 Eviction Policies

#### LRU (Least Recently Used) Implementation

**LRU Cache Implementation:**

**Data Structures:**
- **HashMap:** Maps keys to nodes for O(1) lookup
- **Doubly-Linked List:** Maintains access order (most recent at head, least recent at tail)

**Operations:**

1. **GET(key):**
   - Lookup key in HashMap (O(1))
   - If found: Move node to head (mark as recently used)
   - Return value

2. **PUT(key, value):**
   - If key exists: Remove old node
   - Create new node and add to head
   - Update HashMap
   - If capacity exceeded: Evict tail node (LRU)

**Key Properties:**
- All operations are O(1)
- Space complexity: O(capacity)
- Thread-safe with locking

*See `pseudocode.md::LRUCache` for detailed implementation with node manipulation*

#### Eviction Policy Comparison

| Policy     | Best For               | Pros                  | Cons                       | Redis Support                    |
|------------|------------------------|-----------------------|----------------------------|----------------------------------|
| **LRU**    | General purpose        | Good hit rate, simple | Ignores frequency          | ✅ `maxmemory-policy allkeys-lru` |
| **LFU**    | Stable access patterns | Keeps hot data        | Complex, cold start issues | ✅ `allkeys-lfu`                  |
| **TTL**    | Time-sensitive data    | Predictable expiry    | May evict useful data      | ✅ `volatile-ttl`                 |
| **Random** | Low overhead           | Very simple           | Poor hit rate              | ✅ `allkeys-random`               |

---

### 3.5 TTL (Time-to-Live) Management

#### Implementation Strategies

**Option 1: Active Expiration (Lazy Delete)**

Checks TTL when a key is accessed:
- On GET request, check if key has expired
- If expired: Delete immediately and return null
- If not expired: Return value

**Pros:** Simple, no background process needed  
**Cons:** Expired keys not accessed remain in memory

**Option 2: Passive Expiration (Background Scan)**

Background process periodically scans for expired keys:
- Sample 100 random keys every 100ms
- Delete any expired keys found
- Repeat continuously

**Pros:** Cleans up unused keys  
**Cons:** Uses CPU, may miss some expired keys

**Redis Approach: Hybrid (Best Practice)**

Combines both strategies:
- **Active:** Check on every access
- **Passive:** Sample 20 keys every 100ms, delete expired
- **Probabilistic:** If > 25% expired, scan again immediately (aggressive mode)

*See `pseudocode.md::TTL Management` for detailed implementations*

---

### 3.6 Concurrency Control: Preventing Cache Stampede

#### Problem: Cache Stampede / Thundering Herd

```
Hot Key Expires
        │
        ├─ Request 1 ──┐
        ├─ Request 2 ──┼─▶ All hit DB simultaneously
        ├─ Request 3 ──┤
        ├─ ...         │
        └─ Request 10K ┘

Result: Database overwhelmed, cascading failure
```

#### Solution 1: Single Flight Request (Recommended)

**How It Works:**

Uses distributed locking to ensure only one request fetches from the database when cache expires:

1. **Request arrives:** Check cache first
2. **Cache miss:** Acquire lock for this specific key
3. **Lock acquired (first request):**
   - Double-check cache (another thread may have populated it)
   - Fetch from database
   - Populate cache
   - Release lock
4. **Lock waiting (other requests):**
   - Wait for first request to complete
   - Read from cache (now populated)
   - No database query needed

**Example: 10,000 Concurrent Requests for Same User**
- Without this pattern: 10,000 DB queries
- With this pattern: **1 DB query** (other 9,999 wait and read from cache)

**Benefits:**
- Prevents database overload
- Transparent to application code
- Works across multiple servers (distributed lock)

**Trade-offs:**
- Adds slight latency (lock acquisition overhead ~1-5ms)
- Requires distributed lock infrastructure (Redis, etcd)

*See `pseudocode.md::SingleFlightCache` for detailed implementation*

#### Solution 2: Probabilistic Early Expiration

**How it works:**
- Get entry with expiry time
- If TTL < 10% remaining, calculate refresh probability
- Probability increases as expiration approaches
- Trigger async background refresh
- Cache never goes completely cold

*See `pseudocode.md::get_with_early_refresh()` for detailed implementation*

---

## 5. Availability and Fault Tolerance

### 4.1 Failover Strategy

#### Automatic Failover with Sentinel

```
Normal Operation:
App → Master (Healthy)
      └─ Replica 1
      └─ Replica 2

Master Fails:
Sentinel detects failure (quorum vote)
      ↓
Promotes Replica 1 to Master
      ↓
App redirected to new Master
      ↓
Old master becomes replica when it recovers
```

**Configuration:**

```conf
# Redis Sentinel configuration
sentinel monitor mymaster 127.0.0.1 6379 2
sentinel down-after-milliseconds mymaster 5000
sentinel parallel-syncs mymaster 1
sentinel failover-timeout mymaster 10000

# Quorum = 2 (need 2 sentinels to agree master is down)
```

**Failover Process:**

1. **Detection** (5s): Sentinel detects master unresponsive
2. **Quorum Vote** (1s): Sentinels vote if master is really down
3. **Leader Election** (1s): One sentinel leads the failover
4. **Promotion** (1-3s): Promote replica to master
5. **Reconfiguration** (1s): Update clients

**Total Downtime: ~9-13 seconds**

---

### 4.2 Cluster Mode (Sharding + Replication)

```
Redis Cluster:
16,384 Hash Slots distributed across nodes

Slot 0-5460  → Node 1 (Master) + Replica 1A
Slot 5461-10922 → Node 2 (Master) + Replica 2A
Slot 10923-16383 → Node 3 (Master) + Replica 3A

Client calculates slot: CRC16(key) % 16384
Node redirects if wrong slot (MOVED response)
```

**Advantages:**

- Automatic sharding
- Built-in failover
- Horizontal scalability

**Disadvantages:**

- More complex setup
- Cross-slot operations not supported
- Requires client library support

---

## 6. Bottlenecks and Optimizations

### 5.1 Performance Bottlenecks

| Bottleneck                | Symptoms                   | Solution                                       |
|---------------------------|----------------------------|------------------------------------------------|
| **Network Saturation**    | High latency, packet loss  | Use 10 Gbps network, connection pooling        |
| **Single-threaded Redis** | CPU at 100%, high latency  | Use Redis Cluster for parallel processing      |
| **Memory Fragmentation**  | Memory usage > actual data | Restart Redis periodically, use `activedefrag` |
| **Slow Commands**         | Occasional latency spikes  | Use `SLOWLOG`, avoid `KEYS *`, `FLUSHALL`      |
| **Large Values**          | High bandwidth usage       | Compress values, split large objects           |

### 5.2 Optimization Techniques

#### Connection Pooling

❌ **Bad:** New connection per request → TCP handshake ~3ms overhead each

✅ **Good:** Connection pool (max 50 connections) → Reuse connections → No handshake → 3x faster

*See `pseudocode.md::ConnectionPool` for detailed implementation*

#### Pipelining (Batch Operations)

```
// ❌ Bad: Round-trip for each operation
for i from 0 to 999:
  cache.set("key:" + i, i)
// 1000 network round-trips (~500ms at 0.5ms each)

// ✅ Good: Pipeline (batch)
pipeline = cache.create_pipeline()
for i from 0 to 999:
  pipeline.set("key:" + i, i)
pipeline.execute()
// 1 network round-trip (~0.5ms)
```

#### Compression for Large Values

For large values (> 1 KB):
- Serialize to JSON
- Compress (gzip/snappy)
- Store compressed
- On retrieval: decompress and deserialize

**Example:** 10 KB JSON → 1 KB compressed (10x memory savings)

*See `pseudocode.md::set_compressed()` and `pseudocode.md::get_compressed()` for implementation*

---

## 7. Monitoring and Observability

### Key Metrics to Monitor

| Metric            | Target         | Alert Threshold | Why Important               |
|-------------------|----------------|-----------------|-----------------------------|
| **Hit Rate**      | > 80%          | < 70%           | Low hit rate = wasted cache |
| **Latency (p99)** | < 1ms          | > 5ms           | User experience impact      |
| **Memory Usage**  | < 80%          | > 90%           | Prevent eviction storms     |
| **Evicted Keys**  | Minimal        | > 1000/sec      | Indicates undersized cache  |
| **Expired Keys**  | Normal         | > 10000/sec     | Check TTL settings          |
| **Network I/O**   | < 70% capacity | > 80%           | Bottleneck indicator        |
| **Commands/sec**  | Baseline ±20%  | > 2x baseline   | Unusual traffic pattern     |

### Redis Monitoring Commands

```bash
# Get stats
redis-cli INFO stats
redis-cli INFO memory

# Monitor real-time commands
redis-cli MONITOR

# Check slow queries
redis-cli SLOWLOG GET 10

# Get memory usage per key
redis-cli --bigkeys

# Check latency
redis-cli --latency
```

---

## 8. Alternative Approaches

### Comparison: Redis vs Memcached vs KeyDB

| Feature             | Redis                                   | Memcached             | KeyDB                  | Best For                      |
|---------------------|-----------------------------------------|-----------------------|------------------------|-------------------------------|
| **Data Structures** | Rich (lists, sets, sorted sets, hashes) | Key-value only        | Same as Redis          | Complex data: Redis           |
| **Persistence**     | RDB, AOF                                | None                  | RDB, AOF               | Durability: Redis             |
| **Threading**       | Single-threaded                         | Multi-threaded        | Multi-threaded         | High CPU: KeyDB               |
| **Replication**     | Built-in                                | None (requires proxy) | Built-in               | HA: Redis/KeyDB               |
| **Clustering**      | Built-in                                | Client-side           | Built-in               | Scalability: Redis            |
| **Performance**     | 100K+ QPS                               | 300K+ QPS             | 200K+ QPS              | Pure speed: Memcached         |
| **Memory Overhead** | Higher (~20-30%)                        | Lower (~10%)          | Higher (~20-30%)       | Memory constrained: Memcached |
| **Use Case**        | General purpose                         | Pure caching          | High-performance Redis | See "Best For"                |

---

## 9. Trade-offs and Design Decisions Summary

| Decision         | Choice             | Alternative     | Why Chosen                          | Trade-off                            |
|------------------|--------------------|-----------------|-------------------------------------|--------------------------------------|
| **Partitioning** | Consistent Hashing | Modulo Hashing  | Minimal reshuffling                 | Slightly more complex                |
| **Replication**  | Master-Replica     | Masterless (AP) | Simple failover, strong consistency | Write bottleneck at master           |
| **Write Policy** | Cache-Aside        | Write-Through   | Low write latency                   | Temporary inconsistency              |
| **Eviction**     | LRU                | LFU             | Good balance, simple                | May evict hot data accessed long ago |
| **Persistence**  | None (optional)    | RDB/AOF         | Maximum performance                 | Data loss on crash                   |
| **Consistency**  | Eventual           | Strong          | Sub-ms latency                      | Stale reads possible                 |

---

## 10. Common Pitfalls and Anti-Patterns

### Anti-Pattern 1: Not Setting TTL

```
// ❌ Bad: No TTL, cache grows forever
cache.set('user:123', user_data)

// ✅ Good: Always set TTL
cache.set('user:123', user_data, TTL=300)  // 5 minutes
```

### Anti-Pattern 2: Cache Stampede

Already covered in Section 3.6 with solutions.

### Anti-Pattern 3: Storing Large Objects

```
// ❌ Bad: 10 MB object
cache.set('users:all', all_users_list)  // 10 MB!

// ✅ Good: Paginate or split
for i from 0 to length(all_users) step 100:
  chunk = all_users[i : i+100]
  cache.set("users:page:" + (i/100), chunk, TTL=300)
```

### Anti-Pattern 4: Using Cache as Primary Storage

```
// ❌ Bad: Only in cache
cache.set('session:abc', session_data, TTL=3600)
// If cache fails, data is lost!

// ✅ Good: Cache as secondary storage
db.save_session(session_data)  // Primary
cache.set('session:abc', session_data, TTL=3600)  // Secondary
```

---

## 11. Final Architecture Diagram

```
┌─────────────────────────────────────────────────────────────────┐
│                      Application Layer                           │
│  ┌──────────────────────────────────────────────────────────┐   │
│  │  Cache Client Library                                    │   │
│  │  - Consistent Hashing (150 VNodes/node)                  │   │
│  │  - Connection Pooling (50 connections)                   │   │
│  │  - Retry Logic + Circuit Breaker                         │   │
│  └──────────────────────────────────────────────────────────┘   │
└───────────────────────┬─────────────────────────────────────────┘
                        │
        ┌───────────────┼───────────────┐
        │               │               │
        ▼               ▼               ▼
┌──────────────┐  ┌──────────────┐  ┌──────────────┐
│   Master 1   │  │   Master 2   │  │   Master 3   │
│   64 GB RAM  │  │   64 GB RAM  │  │   64 GB RAM  │
│ 100K QPS     │  │ 100K QPS     │  │ 100K QPS     │
│ Slots 0-5460 │  │ Slots 5461-  │  │ Slots 10923- │
└──────┬───────┘  └──────┬───────┘  └──────┬───────┘
       │                 │                 │
       │ Async           │ Async           │ Async
       │ Replication     │ Replication     │ Replication
       │ (1-5ms lag)     │ (1-5ms lag)     │ (1-5ms lag)
       │                 │                 │
   ┌───┴────┐        ┌───┴────┐        ┌───┴────┐
   │Replica │        │Replica │        │Replica │
   │  1A    │        │  2A    │        │  3A    │
   │ 64 GB  │        │ 64 GB  │        │ 64 GB  │
   └────────┘        └────────┘        └────────┘
   ┌────────┐        ┌────────┐        ┌────────┐
   │Replica │        │Replica │        │Replica │
   │  1B    │        │  2B    │        │  3B    │
   │ 64 GB  │        │ 64 GB  │        │ 64 GB  │
   └────────┘        └────────┘        └────────┘

┌─────────────────────────────────────────────────────────────┐
│              Sentinel Cluster (Monitoring)                   │
│  ┌──────────┐     ┌──────────┐     ┌──────────┐            │
│  │Sentinel 1│     │Sentinel 2│     │Sentinel 3│            │
│  │ Quorum=2 │     │ Quorum=2 │     │ Quorum=2 │            │
│  └──────────┘     └──────────┘     └──────────┘            │
└─────────────────────────────────────────────────────────────┘

Total Capacity:
- 9 nodes × 64 GB = 576 GB total RAM
- 3 master nodes = 192 GB effective capacity (with 3x replication)
- 300K+ QPS total throughput
- < 13 seconds failover time
- ~1% keys reshuffled on node add/remove
```

---

## 12. Capacity Planning Guidelines

### Scaling Strategy

| Metric      | When to Scale      | How to Scale                             |
|-------------|--------------------|------------------------------------------|
| **Memory**  | > 80% usage        | Add more nodes (horizontal)              |
| **CPU**     | > 70% usage        | Use KeyDB (multi-threaded) or add nodes  |
| **Network** | > 70% bandwidth    | Upgrade NIC, add nodes                   |
| **QPS**     | > 80K QPS per node | Add more master nodes                    |
| **Latency** | p99 > 5ms          | Optimize queries, add replicas for reads |

### Cost Estimation (AWS Example)

```
Instance Type: r6g.2xlarge (64 GB RAM, 8 vCPU)
Cost: $0.504/hour

9 nodes × $0.504/hour × 730 hours/month = $3,311/month

With 3-year reserved instance:
9 nodes × $0.303/hour × 730 hours/month = $1,991/month

Data Transfer: ~$0.01/GB (within AZ, negligible)

Total: ~$2,000-3,300/month for 180 GB cache capacity
```

---

## 13. Further Optimizations

### Advanced Techniques

1. **Read-Through Cache** (optional for specific use cases)
2. **Cache Warming** on startup
3. **Predictive Caching** (ML-based)
4. **Geo-distributed Caching** (CDN-like)
5. **Client-side Caching** (L1 cache)

### Redis Modules

- **RediSearch**: Full-text search
- **RedisJSON**: Native JSON support
- **RedisGraph**: Graph database
- **RedisTimeSeries**: Time-series data
- **RedisBloom**: Bloom filters, HyperLogLog

---

## Summary

A distributed cache system requires careful design decisions balancing:

- **Performance** (sub-millisecond latency)
- **Availability** (failover, replication)
- **Scalability** (consistent hashing, sharding)
- **Consistency** (eventual vs strong)

**Key Takeaways:**

1. Use **Consistent Hashing** with virtual nodes for minimal reshuffling
2. **Async replication** provides best performance for cache use cases
3. **Cache-Aside** pattern ideal for read-heavy workloads
4. **Single Flight Request** prevents cache stampede
5. **Monitor hit rate** and latency continuously
6. **Always set TTL** to prevent unbounded growth
7. Cache is **secondary storage** - database is source of truth

**Recommended Stack:**

- **Redis** for general purpose (rich features, good performance)
- **KeyDB** for extreme performance (multi-threaded)
- **Memcached** for pure key-value with minimal overhead
- **Redis Sentinel** for automatic failover
- **Redis Cluster** for horizontal scalability

