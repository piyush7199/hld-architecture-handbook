# 3.1.1 Design URL Shortener (TinyURL/Bitly)

## 1. Requirements and Scale Estimation

### Functional Requirements (FRs)

1. **URL Redirection:** Given a short URL, the system must redirect the user to the original long URL.
2. **URL Shortening:** Given a long URL, the system must generate a unique, short URL (alias).
3. **Custom Alias:** Users should be able to specify a custom short alias (if available).
4. **Expiry:** Links should optionally expire after a set time.

### Non-Functional Requirements (NFRs)

1. **High Availability ($\text{HA}$):** The redirection service must be highly available ($99.99\%+$). Downtime is
   unacceptable.
2. **Low Latency (Redirect):** Redirection must be fast ($\sim 50$ $\text{ms}$).
3. **High Read Capacity:** Read traffic (redirects) is significantly higher than Write traffic (creation).

### Scale Estimation

| Metric                | Assumption                                                                       | Calculation                                               | Result                                      |
|-----------------------|----------------------------------------------------------------------------------|-----------------------------------------------------------|---------------------------------------------|
| Total Users           | 500 Million ($\text{MAU}$)                                                       | -                                                         | -                                           |
| URL Creates (Writes)  | 1 Million per day ($\text{QPS}_{w}$)                                             | $\frac{1 \text{M}}{24 \text{h} \times 3600 \text{s/h}}$   | $\sim 12$ Writes per second ($\text{QPS}$)  |
| URL Redirects (Reads) | 100 Million per day ($\text{QPS}_{r}$)                                           | $\frac{100 \text{M}}{24 \text{h} \times 3600 \text{s/h}}$ | $\sim 1157$ Reads per second ($\text{QPS}$) |
| Storage (5 Years)     | $1 \text{M} \text{URLs}/\text{day} \times 365 \times 5 = 1.825 \text{B}$ records | $1.825 \text{B} \times 1 \text{kB}/\text{record}$         | $\sim 1.8 \text{TB}$ total storage          |

---

## 2. High-Level Design (HLD)

The system is split into two primary flows: the **Write Path** (creation) and the **Read Path** (redirection).

### Components

1. **API Gateway / Load Balancer:** Routes traffic, enforces Rate Limits (for the Write Path), and performs
   authentication checks.
2. **Shortening Service (Write Path):** Generates the unique alias and stores the mapping in the database.
3. **Redirection Service (Read Path):** The high-speed component that retrieves the long URL from the Cache or DB.
4. **Cache (Redis):** Stores the most frequently accessed short_url $\rightarrow$ long_url mappings to ensure low
   redirection latency.
5. **Database (Sharded PostgreSQL):** The persistent, authoritative store for all URL mappings.

---

## 3. Detailed Component Design

### 3.1 Data Model and Storage

Since we require **ACID** properties (we cannot afford to lose the mapping or have conflicting keys) and the total data
volume is manageable ($\sim 1.8 \text{TB}$), we start with a **Relational Database (PostgreSQL)**.

#### Schema ($\text{URLs}$ Table)

| Field           | Data Type                     | Notes                                                     |
|-----------------|-------------------------------|-----------------------------------------------------------|
| **short_alias** | $\text{VARCHAR}(\text{8})$    | Primary Key (Clustered Index), unique.                    |
| **long_url**    | $\text{VARCHAR}(\text{2048})$ | The destination URL.                                      |
| **created_at**  | $\text{TIMESTAMP}$            | Record creation time.                                     |
| **user_id**     | $\text{BIGINT}$               | Foreign key to the User service (for custom links/stats). |
| **expires_at**  | $\text{TIMESTAMP}$            | Optional expiration time.                                 |
| **status**      | $\text{ENUM}$                 | $\text{ACTIVE}$, $\text{EXPIRED}$, $\text{BLOCKED}$.      |

#### Database Scaling (Sharding)

The table must be **sharded** horizontally by the `short_alias` to distribute the read/write load across multiple
database instances. This is because lookups are based on the primary key, making lookups efficient across shards.

### 3.2 Alias Generation Strategy (The Heart of the Write Path)

The alias must be short (e.g., 7-8 characters) and unique.

| Architectural Choice    | Decision Made                                                                                                          | Why This Over That (Tradeoffs)                                                                                                                                                                                                                                                                                                        |
|-------------------------|------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **Alias Generation**    | **Base62 Encoding (or similar)**                                                                                       | **Why Base62 over MD5/Hashing?** Hashing ($\text{MD5}$, $\text{SHA}$) is fast but leads to collisions, which requires complex resolution logic and multiple database checks. **Base62** (using letters $\text{A-Z}$, $\text{a-z}$, and numbers $\text{0-9}$) is a deterministic encoding of a sequential ID, guaranteeing uniqueness. |
| **Source of Unique ID** | Dedicated $\text{ID}$ $\text{Generator}$ $\text{Service}$ ($\text{e.g.}$, Snowflake, or $\text{RDBMS}$ Auto-Increment) | **Why a dedicated service?** $\text{RDBMS}$ auto-increment IDs are centralized and become a **Single Point of Failure** and write bottleneck at high scale. A distributed $\text{ID}$ service guarantees globally unique, highly available sequential $\text{IDs}$. **Tradeoff**: Adds complexity and another service to maintain.    |

#### Write Path Flow (High-Level Steps)

1. Client submits $\text{Long}$ $\text{URL}$.
2. $\text{Shortening}$ $\text{Service}$ asks $\text{ID}$ $\text{Generator}$ $\text{Service}$ for the next
   unique $\text{BIGINT}$ $\text{ID}$.
3. $\text{Service}$ converts the $\text{BIGINT}$ $\text{ID}$ into a
   short, $7$-$\text{character}$ $\text{Base62}$ $\text{Alias}$.
4. $\text{Service}$ writes the `short_alias` $\rightarrow$ `long_url` mapping to
   the $\text{Sharded}$ $\text{PostgreSQL}$ DB.
5. **IMPORTANT:** The mapping is immediately written to the **Cache (Redis)** with a $\text{TTL}$.

### 3.3 Redirection Strategy (The Read Path)

The Read Path must prioritize speed and offload the database.

| Architectural Choice  | Decision Made                            | Why This Over That (Tradeoffs)                                                                                                                                                                                                                                                       |
|-----------------------|------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **Database**          | Sharded **PostgreSQL** (Source of Truth) | **Why PostgreSQL over NoSQL** (like Cassandra)? The core data is highly relational (User $\rightarrow$ URL $\rightarrow$ Stats) and requires $\text{ACID}$ consistency for the crucial `short_alias` $\text{Primary}$ $\text{Key}$ constraint. **Tradeoff**: Harder to scale writes. |
| **Caching Strategy**  | **Cache-Aside** with a $\text{TTL}$      | **Why Cache-Aside over Write-Through?** The redirection path is extremely read-heavy (100:1 read-to-write ratio). We prioritize low write latency and accept that a new link might have a tiny risk of a $\text{Cache}$ $\text{Miss}$ on the first few reads.                        |
| **Redirection Logic** | **HTTP 301/302**                         | **Why 301/302?** Using standard $\text{HTTP}$ redirects is the fastest way to offload the process to the client's browser.                                                                                                                                                           |

#### Read Path Flow (High-Level Steps)

1. Client sends request for `tinyl.co/abcXYZ`.
2. $\text{Redirection}$ $\text{Service}$ extracts `abcXYZ`.
3. **Cache Check ($\text{Redis}$):** Look up `abcXYZ` in $\text{Redis}$.
4. **Cache Hit:** Redirect client to $\text{Long}$ $\text{URL}$ ($\text{HTTP}$ $301$/$302$). $\sim 5$ $\text{ms}$.
5. **Cache Miss:** Look up abcXYZ in $\text{Sharded}$ $\text{PostgreSQL}$.
6. **DB Hit**: Store mapping in $\text{Redis}$ (Cache-Aside) and redirect the client. $\sim 50$ $\text{ms}$.

---

## 4. Bottlenecks and Future Scaling

1. **SPOF in Cache:** If the Redis Cluster fails, all traffic hits the $\text{DB}$, causing a
   potential $\text{Cache}$ $\text{Stampede}$ and $\text{DB}$ overload.
    - **Mitigation**: Implement a multi-master/active-active $\text{Redis}$ setup with automatic failover. Use
      a $\text{Circuit}$ $\text{Breaker}$ on the $\text{DB}$ calls in the $\text{Redirection}$ $\text{Service}$ to drop
      excess traffic gracefully.
2. **Write Scaling:** If the daily link creation rate grows beyond $\sim 100$ $\text{QPS}$, the $\text{RDBMS}$ will
   struggle with contention even with sharding.
    - **Future Scaling:** Decouple the creation process using a **Message Queue (Kafka).** The user gets a confirmation
      immediately, and the $\text{URL}$ $\text{Shortening}$ $\text{Service}$ processes the actual $\text{DB}$ write
      asynchronously.
3. **Rate Limiting:** Abusive users could $\text{DDoS}$ the $\text{Write}$ $\text{Path}$ by repeatedly submitting new
   URLs.
    - **Mitigation**: Enforce $\text{Rate}$ $\text{Limiting}$ at the $\text{API}$ $\text{Gateway}$ using the **Token
      Bucket Algorithm** (2.5.1), limited by $\text{User}$ $\text{ID}$ or $\text{IP}$ $\text{Address}$.

---

## 5. Alternative Approaches (Not Chosen)

### Approach A: NoSQL-First Design (DynamoDB/Cassandra)

**Architecture:**

- Use DynamoDB or Cassandra as primary database
- Leverage NoSQL's horizontal scalability
- Store mappings as simple key-value pairs

**Pros:**

- ✅ Excellent horizontal scalability
- ✅ Simple data model (key-value)
- ✅ Very high write throughput
- ✅ Built-in replication and high availability

**Cons:**

- ❌ More complex to enforce uniqueness constraints
- ❌ No ACID guarantees for custom alias conflicts
- ❌ Eventual consistency may cause issues during alias creation
- ❌ More expensive at small scale

**Why Not Chosen:**

- The requirement for **unique alias constraints** is better handled by RDBMS
- At 12 writes/sec, we don't need NoSQL-level write throughput yet
- ACID guarantees important for preventing duplicate aliases
- Better to start with proven relational model and scale later if needed

**When to Reconsider:**

- Write QPS exceeds 1,000 per second
- Data volume exceeds multiple terabytes
- Need multi-region active-active writes

---

### Approach B: Hash-Based Alias Generation (MD5/SHA)

**Architecture:**

- Use MD5 or SHA-256 hash of the long URL
- Take first 7 characters as the short alias
- Handle collisions with iteration (append counter)

**Pros:**

- ✅ Deterministic (same URL → same alias)
- ✅ No need for ID generator service
- ✅ Simpler architecture

**Cons:**

- ❌ **Collision handling is complex** (requires multiple DB checks)
- ❌ Aliases are not sequential or predictable
- ❌ Cannot support custom aliases easily
- ❌ Hash collisions increase with scale
- ❌ Same URL submitted twice creates duplicate entries

**Why Not Chosen:**

- Collision resolution adds latency and complexity
- Custom alias feature is harder to implement
- Base62 encoding of sequential ID is more elegant
- Deterministic hashing conflicts with custom aliases

**Example Collision Problem:**

```
URL1: https://example.com/page1 → MD5 → abc1234 (first 7 chars)
URL2: https://example.com/page2 → MD5 → abc1234 (collision!)
System must rehash with counter: hash(URL2 + "1") → xyz5678
```

---

### Approach C: Client-Side ID Generation

**Architecture:**

- Client generates UUID/GUID for each short URL
- No centralized ID generator needed

**Pros:**

- ✅ No single point of failure
- ✅ Infinite scalability
- ✅ No coordination needed

**Cons:**

- ❌ **UUIDs are 128-bit (too long for short URLs)**
- ❌ Not human-readable or memorable
- ❌ Base62 encoding of UUID still results in long strings (~22 chars)
- ❌ Cannot guarantee short length

**Why Not Chosen:**

- The entire point of URL shortener is **short** URLs
- UUID encoding defeats the purpose
- "tinyl.co/a5c9e1b2f8d4" is not short

---

## 6. Deep Dive: Handling Edge Cases

### Edge Case 1: Expired Links

**Problem:** Links with expiry need automatic cleanup

**Solution:**

```python
# Background job (runs every hour)
def cleanup_expired_links():
    expired_links = db.query(
        "SELECT short_alias FROM urls WHERE expires_at < NOW() AND status = 'ACTIVE'"
    )
    
    for alias in expired_links:
        db.update("UPDATE urls SET status = 'EXPIRED' WHERE short_alias = ?", alias)
        cache.delete(alias)  # Remove from cache
    
    log.info(f"Cleaned up {len(expired_links)} expired links")
```

**Alternative: TTL in Database**

- Some databases (DynamoDB, Cassandra) support automatic TTL-based deletion
- More efficient than batch cleanup jobs

---

### Edge Case 2: Malicious/Inappropriate URLs

**Problem:** Users might create links to harmful content

**Solution:**

```python
def create_short_url(long_url: str) -> str:
    # 1. Validate URL format
    if not is_valid_url(long_url):
        raise ValidationError("Invalid URL format")
    
    # 2. Check against blacklist
    if is_blacklisted_domain(long_url):
        raise ForbiddenError("Domain is blacklisted")
    
    # 3. Optional: Scan with external service
    if url_safety_scanner.is_malicious(long_url):
        raise ForbiddenError("URL flagged as malicious")
    
    # 4. Create short URL
    short_alias = generate_alias()
    save_to_db(short_alias, long_url)
    return short_alias
```

---

### Edge Case 3: Analytics and Click Tracking

**Problem:** Users want to know how many times their link was clicked

**Extended Schema:**

```sql
CREATE TABLE url_analytics (
    short_alias VARCHAR(8) PRIMARY KEY,
    total_clicks BIGINT DEFAULT 0,
    last_clicked_at TIMESTAMP,
    INDEX(short_alias)
);

CREATE TABLE click_events (
    event_id BIGINT PRIMARY KEY AUTO_INCREMENT,
    short_alias VARCHAR(8),
    clicked_at TIMESTAMP,
    user_agent VARCHAR(255),
    ip_address VARCHAR(45),
    referer VARCHAR(2048),
    country VARCHAR(2),
    INDEX(short_alias, clicked_at)
);
```

**Architecture:**

```
User clicks short URL
    ↓
Redirection Service
    ↓
┌─────────────────────────────┐
│ 1. Increment counter (Redis)│
│ 2. Publish ClickEvent       │
│ 3. Redirect immediately     │
└─────────────────────────────┘
         ↓
    Kafka Queue
         ↓
Analytics Consumer
    ↓
Store in Cassandra/ClickHouse
(for long-term analytics)
```

**Key Decision:**

- **Asynchronous tracking** to avoid slowing down redirects
- Use **Redis counters** for real-time counts
- Use **batch inserts** to analytics DB for historical data

---

## 7. Monitoring and Observability

### Key Metrics to Track

| Metric                     | Type      | Threshold         | Alert Action                          |
|----------------------------|-----------|-------------------|---------------------------------------|
| **Redirect Latency (P99)** | Histogram | < 100ms           | If > 200ms, check cache hit rate      |
| **Cache Hit Rate**         | Gauge     | > 90%             | If < 80%, increase cache size or TTL  |
| **DB Connection Pool**     | Gauge     | < 80% utilization | If > 90%, scale DB connections        |
| **URL Creation QPS**       | Counter   | Monitor trends    | If spike, check for abuse             |
| **Error Rate (404s)**      | Counter   | < 1%              | If > 2%, investigate data consistency |
| **Redis Availability**     | Gauge     | 100%              | If down, trigger circuit breaker      |

### Distributed Tracing

**Trace Example:**

```
Request: GET /abc123
  ├─ Load Balancer [2ms]
  ├─ API Gateway [5ms]
  ├─ Redirection Service [45ms]
  │   ├─ Redis Cache Lookup [3ms] ← Cache hit!
  │   └─ HTTP 301 Redirect [1ms]
  └─ Total: 55ms
```

**Trace Example (Cache Miss):**

```
Request: GET /xyz789
  ├─ Load Balancer [2ms]
  ├─ API Gateway [5ms]
  ├─ Redirection Service [93ms]
  │   ├─ Redis Cache Lookup [3ms] ← Cache miss!
  │   ├─ Database Query [80ms] ← Slow!
  │   ├─ Cache Write [5ms]
  │   └─ HTTP 301 Redirect [2ms]
  └─ Total: 100ms
```

---

## 8. Interview Discussion Points

### Question 1: How would you handle 100× growth?

**Answer:**

- **Reads (Redirects):** Already horizontally scalable
    - Add more cache nodes (Redis Cluster)
    - Add more API servers behind load balancer
    - Implement CDN for popular links

- **Writes (Creation):** Bottleneck at ID generator
    - Use distributed ID generator (Snowflake-style)
    - Each server generates IDs independently
    - No single point of coordination

- **Database:** Shard by hash of short_alias
    - 10 shards → 100 shards
    - Use consistent hashing for shard assignment

---

### Question 2: What if custom aliases become 50% of traffic?

**Answer:**

- **Challenge:** Custom aliases can't use sequential ID generation
- **Solution:**
    1. Check Redis for alias availability (fast path)
    2. If available, atomically reserve in DB:
       ```sql
       INSERT INTO urls (short_alias, long_url, ...)
       VALUES (?, ?, ...)
       ON DUPLICATE KEY UPDATE short_alias = short_alias;
       -- If insert fails, alias is taken
       ```
    3. Use optimistic locking to handle race conditions

- **Performance Impact:**
    - Higher DB load (must check uniqueness)
    - May need stronger consistency (SERIALIZABLE isolation)
    - Consider pre-reserving popular aliases

---

### Question 3: How do you handle GDPR deletion requests?

**Answer:**

- **Requirements:**
    - Delete user's URLs within 30 days
    - Anonymize analytics data

- **Implementation:**
    1. Mark URLs as `status = 'DELETED'`
    2. Keep alias reserved (prevent reuse) but show 410 Gone
    3. Asynchronously purge from:
        - Primary database
        - Cache (immediate)
        - Analytics logs (anonymize user_id)
        - Backups (within retention window)

- **Compliance:**
  ```sql
  -- Soft delete
  UPDATE urls SET status = 'DELETED', long_url = NULL, user_id = NULL
  WHERE user_id = ? AND status = 'ACTIVE';
  
  -- Background job for hard delete
  DELETE FROM urls WHERE status = 'DELETED' AND deleted_at < NOW() - INTERVAL 30 DAY;
  ```

---

### Question 4: How would you prevent abuse (spam, phishing)?

**Answer:**

- **Rate Limiting:**
    - 10 URLs per hour for anonymous users
    - 100 URLs per hour for authenticated users
    - Use Token Bucket algorithm at API Gateway

- **Domain Blacklisting:**
    - Maintain list of known malicious domains
    - Check against Google Safe Browsing API

- **URL Validation:**
    - Verify URL is reachable (HTTP HEAD request)
    - Check SSL certificate validity
    - Scan content with VirusTotal API

- **User Reputation:**
    - Track abuse reports per user
    - Automatically ban users with high spam ratio
    - Require CAPTCHA for suspicious activity

---

### Question 5: How do you ensure 99.99% availability?

**Answer:**

- **Eliminate Single Points of Failure:**
    - Multi-AZ deployment (at least 3 zones)
    - Redundant load balancers
    - Redis Cluster with automatic failover
    - Database primary-replica setup with automatic promotion

- **Circuit Breakers:**
    - Protect against cascading failures
    - Fail fast when cache/DB is slow

- **Health Checks:**
    - Load balancer removes unhealthy nodes
    - Kubernetes auto-restarts failed pods

- **Graceful Degradation:**
    - If Redis fails, serve from DB (slower but functional)
    - If analytics fails, still serve redirects

- **Disaster Recovery:**
    - Multi-region deployment for critical services
    - Regular database backups
    - Tested failover procedures

---

## 9. Comparison with Real-World Systems

| Feature            | Our Design         | Bitly       | TinyURL   | Short.io  |
|--------------------|--------------------|-------------|-----------|-----------|
| **Alias Length**   | 7 chars (Base62)   | 7 chars     | 6-7 chars | 5-8 chars |
| **Custom Aliases** | Yes                | Yes (Pro)   | No        | Yes (Pro) |
| **Analytics**      | Basic (optional)   | Advanced    | Basic     | Advanced  |
| **Expiry**         | Yes                | Yes         | No        | Yes       |
| **Caching**        | Redis              | Redis + CDN | Unknown   | CDN       |
| **Database**       | Postgres (sharded) | Cassandra   | Unknown   | MongoDB   |
| **Scale**          | 1B links           | 25B links   | Unknown   | 1B+ links |

---

## 10. Cost Analysis (AWS Example)

**Assumptions:**

- 1M URL creations/day = 12 QPS write
- 100M redirects/day = 1,157 QPS read
- 1.8 TB storage (5 years)

| Component                     | Specification           | Monthly Cost      |
|-------------------------------|-------------------------|-------------------|
| **EC2 (API Servers)**         | 10× t3.medium           | $400              |
| **RDS PostgreSQL**            | db.r5.2xlarge           | $800              |
| **ElastiCache Redis**         | cache.r5.xlarge cluster | $300              |
| **Application Load Balancer** | Standard                | $20               |
| **Data Transfer**             | 10 TB/month egress      | $900              |
| **CloudWatch Monitoring**     | Logs + Metrics          | $50               |
| **Route 53 (DNS)**            | 1B queries/month        | $400              |
| **S3 (Backups)**              | 2 TB                    | $50               |
| **Total**                     |                         | **~$2,920/month** |

**Cost Optimization:**

- Use CDN (CloudFront) to cache redirects → Reduce data transfer by 80%
- Reserved instances → Save 40% on EC2/RDS
- Spot instances for non-critical services
- **Optimized Cost: ~$1,500/month**

---

This enhanced design provides a production-ready blueprint for building a URL shortener at scale!
