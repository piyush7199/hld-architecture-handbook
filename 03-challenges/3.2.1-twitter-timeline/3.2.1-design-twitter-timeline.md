# 3.2.1 Design a Twitter/X Timeline (Microblogging Feed)

## Problem Statement

Design a scalable microblogging timeline system (like Twitter/X) that allows users to post short messages (tweets) and
view a personalized feed containing tweets from people they follow. The system must handle 500 million monthly active
users, process 200 million tweets per day, serve 10 billion timeline loads per day with sub-200ms latency, and
efficiently solve the "Fanout Problem" — distributing one tweet to millions of followers' timelines in near-real-time.

---

> 📚 **Note on Implementation Details:**
> This document focuses on high-level design concepts and architectural decisions. For detailed algorithm
> implementations (fanout strategies, timeline merging, activity-based fanout, etc.), see *
*[pseudocode.md](./pseudocode.md)**.

---

## 1. Requirements and Scale Estimation

### Functional Requirements (FRs)

| Requirement            | Description                                                                        | Priority     |
|------------------------|------------------------------------------------------------------------------------|--------------|
| **Post Tweet**         | Users can publish new, short text messages (tweets) up to 280 characters           | Must Have    |
| **Timeline Retrieval** | Users can view a feed (timeline) containing tweets from people they follow         | Must Have    |
| **Real-Time Feed**     | Timeline must be nearly real-time (eventual consistency acceptable within seconds) | Must Have    |
| **Follow/Unfollow**    | Users can follow and unfollow other users                                          | Must Have    |
| **Search & Mentions**  | Users can search tweets and tag others (@mentions)                                 | Nice to Have |

### Non-Functional Requirements (NFRs)

| Requirement              | Target                    | Rationale                                         |
|--------------------------|---------------------------|---------------------------------------------------|
| **High Availability**    | 99.99% uptime (Read Path) | Timeline unavailability affects millions of users |
| **Low Read Latency**     | < 200 ms (p99)            | Users expect instant timeline loads               |
| **Write Resilience**     | Handle 20K+ QPS spikes    | Viral tweets cause massive write spikes           |
| **Scalability**          | Handle 500M MAU, 100M DAU | Long-term growth                                  |
| **Eventual Consistency** | Acceptable (BASE model)   | A few seconds delay in timeline is tolerable      |

### Scale Estimation

| Metric                  | Assumption                                      | Calculation                                  | Result                                  |
|-------------------------|-------------------------------------------------|----------------------------------------------|-----------------------------------------|
| **Total Users**         | 500 Million $\text{MAU}$ (Monthly Active Users) | -                                            | -                                       |
| **Active Users**        | 100 Million $\text{DAU}$ (Daily Active Users)   | -                                            | -                                       |
| **Posts (Writes)**      | 200 Million tweets per day                      | $\frac{200 \text{M}}{86400 \text{ s/day}}$   | $\sim 2,300$ Writes/sec ($\text{QPS}$)  |
| **Feed Views (Reads)**  | 10 Billion timeline loads per day               | $\frac{10 \text{B}}{86400 \text{ s/day}}$    | $\sim 115,740$ Reads/sec ($\text{QPS}$) |
| **Read:Write Ratio**    | Reads outweigh writes                           | $\frac{115,740}{2,300}$                      | $\sim 50:1$ (read-heavy)                |
| **Avg Followers**       | 200 followers per user (median)                 | -                                            | -                                       |
| **Fanout Operations**   | Per tweet, deliver to 200 followers             | $200 \text{M tweets} \times 200$             | $\sim 40$ billion fanout ops/day        |
| **Storage (Tweets)**    | 200M tweets/day × 1 KB average                  | $200 \text{M} \times 1 \text{ KB}$           | $\sim 200$ GB/day, $\sim 73$ TB/year    |
| **Storage (Timelines)** | 100M users × 2000 cached tweets × 100 bytes     | $100\text{M} \times 2000 \times 100\text{B}$ | $\sim 20$ TB in cache                   |

**Key Insight:** The system is **read-heavy** (50:1 ratio). The critical challenge is the **Fanout Problem** —
efficiently distributing one tweet to millions of followers' timelines.

---

## 2. High-Level Architecture

### System Overview

```
                           ┌─────────────────────────────────┐
                           │         Users/Clients           │
                           │   (Web, iOS, Android, Mobile)   │
                           └──────────────┬──────────────────┘
                                          │
                    ┌─────────────────────┼─────────────────────┐
                    │                     │                     │
                    ▼                     ▼                     ▼
          ┌─────────────────┐   ┌─────────────────┐   ┌─────────────────┐
          │  Load Balancer  │   │  Load Balancer  │   │  Load Balancer  │
          │   (US-East-1)   │   │   (EU-West-1)   │   │   (AP-South-1)  │
          └────────┬────────┘   └────────┬────────┘   └────────┬────────┘
                   │                     │                     │
                   └─────────────────────┼─────────────────────┘
                                         │
                         ┌───────────────┴───────────────┐
                         │         API Gateway           │
                         │    (Auth, Rate Limiting)      │
                         └───────────────┬───────────────┘
                                         │
                    ┌────────────────────┴────────────────────┐
                    │                                         │
          Write Path (2,300 QPS)                   Read Path (115,740 QPS)
                    │                                         │
                    ▼                                         ▼
        ┌────────────────────────┐              ┌────────────────────────┐
        │   Post Service         │              │   Timeline Service     │
        │   (Stateless)          │              │   (Stateless)          │
        │                        │              │                        │
        │  1. Validate tweet     │              │  1. Check cache        │
        │  2. Get Snowflake ID   │              │  2. Merge tweets       │
        │  3. Save to DB         │              │  3. Rank & filter      │
        │  4. Publish to Kafka   │              │  4. Return to client   │
        └───────────┬────────────┘              └───────────┬────────────┘
                    │                                       │
                    ▼                                       ▼
        ┌────────────────────────┐              ┌────────────────────────┐
        │   Kafka Message Queue  │              │   Timeline Cache       │
        │                        │              │   (Redis Cluster)      │
        │  Topic: new_tweets     │              │                        │
        │  Partitions: 64        │              │  user_id → tweets[]    │
        │  Retention: 7 days     │              │  TTL: 1 hour           │
        │  Absorbs write spikes  │              │  Hit Rate: 90%+        │
        └───────────┬────────────┘              └────────────────────────┘
                    │                                       ▲
                    ▼                                       │
        ┌────────────────────────┐                          │
        │  Fanout Service        │                          │
        │  (Consumer Workers)    │                          │
        │                        │                          │
        │  1. Consume from Kafka │                          │
        │  2. Fetch followers    │                          │
        │  3. Push to timelines  │──────────────────────────┘
        │  4. Update cache       │
        └───────────┬────────────┘
                    │
                    ├──────────────┬──────────────┬──────────────┐
                    ▼              ▼              ▼              ▼
        ┌──────────────────┐  ┌──────────────────┐  ┌──────────────────┐
        │ Follower Graph   │  │ UserTimeline DB  │  │  Tweet Storage   │
        │  (Neo4j / RDBMS) │  │  (Cassandra)     │  │  (PostgreSQL)    │
        │                  │  │                  │  │  Sharded by      │
        │ user_id →        │  │ user_id →        │  │  tweet_id        │
        │   [followers]    │  │   [tweet_ids]    │  │  ACID guarantee  │
        └──────────────────┘  └──────────────────┘  └──────────────────┘
```

### Key Components

| Component            | Responsibility                                   | Technology Options                | Scalability                     |
|----------------------|--------------------------------------------------|-----------------------------------|---------------------------------|
| **Load Balancer**    | Distribute traffic, SSL termination, geo-routing | NGINX, HAProxy, AWS ALB           | Horizontal (multi-region)       |
| **API Gateway**      | Authentication, rate limiting, routing           | Kong, AWS API Gateway             | Horizontal                      |
| **Post Service**     | Tweet validation, ID generation, persistence     | Go, Java, Kotlin (stateless)      | Horizontal                      |
| **Timeline Service** | Timeline retrieval, merging, ranking             | Go, Rust, Java (stateless)        | Horizontal                      |
| **Kafka**            | Message queue for async fanout                   | Apache Kafka                      | Horizontal (partitioned)        |
| **Fanout Workers**   | Distribute tweets to followers' timelines        | Go, Java (consumer groups)        | Horizontal (scale workers)      |
| **Timeline Cache**   | Hot timelines for active users                   | Redis Cluster, KeyDB              | Horizontal (consistent hashing) |
| **UserTimeline DB**  | Persistent timeline storage (denormalized)       | Cassandra, ScyllaDB               | Horizontal (sharding)           |
| **Tweet Storage**    | Source of truth for tweets                       | PostgreSQL (sharded), CockroachDB | Horizontal (sharding)           |
| **Follower Graph**   | Social graph (who follows whom)                  | Neo4j, PostgreSQL (sharded)       | Horizontal (graph partitioning) |

---

## 3. Detailed Component Design

### 3.1 Data Model and Storage

Since timelines are **read-heavy** and follow the $\text{BASE}$ principle (eventual consistency is acceptable), we use
different storage systems optimized for different access patterns.

#### Tweets Table (PostgreSQL - Source of Truth)

| Field                   | Data Type                    | Notes                                     |
|-------------------------|------------------------------|-------------------------------------------|
| **tweet_id**            | $\text{BIGINT}$              | Primary Key (Snowflake ID, time-sortable) |
| **user_id**             | $\text{BIGINT}$              | Author of the tweet, indexed              |
| **content**             | $\text{VARCHAR}(\text{280})$ | Tweet text                                |
| **created_at**          | $\text{TIMESTAMP}$           | Creation timestamp                        |
| **reply_to_tweet_id**   | $\text{BIGINT}$              | Nullable, for reply threads               |
| **retweet_of_tweet_id** | $\text{BIGINT}$              | Nullable, for retweets                    |
| **like_count**          | $\text{INT}$                 | Denormalized counter (updated async)      |
| **retweet_count**       | $\text{INT}$                 | Denormalized counter (updated async)      |
| **status**              | $\text{ENUM}$                | ACTIVE, DELETED, HIDDEN                   |

**Indexing:**

```sql
CREATE INDEX idx_user_created ON tweets(user_id, created_at DESC);
CREATE INDEX idx_created ON tweets(created_at DESC);  -- For trending
```

**Sharding Strategy:** Shard by `tweet_id` (time-based, enables range queries for recent tweets).

**Why PostgreSQL?** Strong consistency for source of truth. Tweets must never be lost or duplicated. ACID guarantees are
critical.

#### UserTimeline Table (Cassandra - Denormalized Timeline)

| Field          | Data Type          | Notes                              |
|----------------|--------------------|------------------------------------|
| **user_id**    | $\text{BIGINT}$    | Partition Key                      |
| **tweet_id**   | $\text{BIGINT}$    | Clustering Key (sorted descending) |
| **tweet_data** | $\text{BLOB}$      | Denormalized tweet content         |
| **created_at** | $\text{TIMESTAMP}$ | For sorting and filtering          |

**Cassandra Schema:**

```sql
CREATE TABLE user_timeline (
    user_id BIGINT,
    tweet_id BIGINT,
    tweet_data BLOB,
    created_at TIMESTAMP,
    PRIMARY KEY (user_id, tweet_id)
) WITH CLUSTERING ORDER BY (tweet_id DESC);
```

**Why Denormalize?** Avoids expensive joins on read. Timeline query becomes simple key lookup:

```sql
SELECT * FROM user_timeline WHERE user_id = ? LIMIT 50;
```

**Why Cassandra?**

- Optimized for high-throughput writes (fanout writes billions of timeline entries/day)
- Cheap storage compared to Redis ($\sim \$50$/TB vs $\sim \$500$/TB)
- Naturally scales horizontally via consistent hashing
- Query pattern is simple: `user_id` → list of `tweet_ids`

#### Follower Graph

**Option 1: Graph Database (Neo4j)**

```
(:User {user_id: 12345})-[:FOLLOWS]->(:User {user_id: 67890})
```

**Query (Find all followers):**

```cypher
MATCH (u:User {user_id: 12345})<-[:FOLLOWS]-(follower)
RETURN follower.user_id
```

**Pros:** Optimized for graph traversals, sub-ms queries for social graph operations.

**Option 2: RDBMS (Sharded PostgreSQL)**

```sql
CREATE TABLE followers (
    follower_id BIGINT,  -- User who is following
    followee_id BIGINT,  -- User being followed
    followed_at TIMESTAMP,
    PRIMARY KEY (follower_id, followee_id)
);

CREATE INDEX idx_followee ON followers(followee_id);  -- For reverse lookup
```

**Sharding:** Partition by `follower_id` for write path, replicate for read path fanout queries.

**Pros:** Simpler ops if team lacks graph DB expertise, works well with proper indexing.

*See [this-over-that.md](this-over-that.md) for detailed comparison.*

---

### 3.2 Alias Generation for Tweets: Snowflake IDs

Tweets need globally unique, time-sortable identifiers for chronological ordering.

**Snowflake ID Structure (64 bits):**

```
┌─────────────────────────────────────────────────────────────────┐
│ 1 bit │  41 bits (timestamp)  │ 10 bits (worker) │ 12 bits (seq) │
│ Sign  │  Milliseconds since   │   Worker ID      │   Sequence    │
│  (0)  │  custom epoch         │   0-1023         │   0-4095      │
└─────────────────────────────────────────────────────────────────┘
```

**Benefits:**

- **Time-sortable:** Tweet IDs naturally sorted by creation time
- **Distributed:** Each worker generates IDs independently
- **High throughput:** 4096 IDs per millisecond per worker
- **No coordination:** No central bottleneck

*See [3.1.3 Distributed ID Generator](../../03-challenges/3.1.3-distributed-id-generator/) for detailed design.*

---

### 3.3 Write Path: Posting a Tweet

**Sequence:**

1. **Client** → POST `/tweet` (content, media URLs)
2. **API Gateway** → Authenticate user, check rate limit
3. **Post Service**:
    - Validate content (length, spam detection)
    - Generate Snowflake `tweet_id`
    - Save tweet to **PostgreSQL** (source of truth)
    - Publish event to **Kafka** (`new_tweets` topic)
    - Return success to client (immediately, without waiting for fanout)
4. **Fanout Workers** (async):
    - Consume tweet event from Kafka
    - Fetch author's followers from **Follower Graph**
    - For each follower: Insert tweet into **UserTimeline** (Cassandra)
    - Update **Timeline Cache** (Redis) for active followers

**Latency:**

- Post API response: $< 100$ ms (write to PostgreSQL + Kafka publish)
- Fanout completion: $< 10$ seconds (async, user doesn't wait)

**Why Kafka?** Decouples Post Service from Fanout Workers. During viral events (millions of tweets/minute), Kafka
absorbs the spike. Fanout workers process at their own pace (backpressure handling).

*See [sequence-diagrams.md](sequence-diagrams.md) for detailed flow visualization.*

---

### 3.4 Read Path: Loading Timeline

**Sequence:**

1. **Client** → GET `/timeline`
2. **Timeline Service**:
    - Check **Redis Cache** for `user_id` → timeline
    - **Cache Hit** → Return immediately ($< 10$ ms)
    - **Cache Miss** → Query **Cassandra** UserTimeline table
    - Load most recent $N$ tweets (e.g., 50)
    - Populate Redis cache (TTL: 1 hour)
    - Return to client
3. **Client** → Display timeline

**Performance:**

- Cache hit: $< 10$ ms (P99)
- Cache miss (Cassandra): $< 50$ ms (P99)
- Target: $< 200$ ms end-to-end (including network latency)

**Optimization: Pagination**

Load timelines in chunks (50 tweets at a time). As user scrolls, fetch next page:

```
GET /timeline?cursor=<last_tweet_id>&limit=50
```

---

### 3.5 Fanout Strategies: The Core Challenge

The **Fanout Problem** is the central challenge. When @Cristiano (500M followers) posts, how do we efficiently deliver
it to all followers?

#### Strategy 1: Fanout-on-Write (Push Model)

**How It Works:**

When a tweet is posted, immediately push it to all followers' timelines (pre-compute).

**Implementation:**

When user posts tweet:

1. Save tweet to database
2. Fetch all followers from Follower Graph
3. For each follower: INSERT into their UserTimeline (Cassandra)
4. Update Timeline Cache for active followers

**Advantages:**

- ✅ **Fast Reads:** Timeline is pre-built, reading is just a key lookup ($\text{O}(1)$)
- ✅ **Low Read Latency:** Users see updates instantly when they refresh
- ✅ **Simple Read Logic:** No complex merge/sort on read path

**Disadvantages:**

- ❌ **Expensive Writes:** One tweet = $N$ writes (where $N$ = number of followers)
- ❌ **Celebrity Problem:** When celebrity posts, system must perform millions of writes
- ❌ **Wasted Work:** Many followers might be inactive (never read the tweet)

**When to Use:** For normal users with $< 10,000$ followers.

*See [pseudocode.md::fanout_on_write()](pseudocode.md) for implementation.*

---

#### Strategy 2: Fanout-on-Read (Pull Model)

**How It Works:**

When a user requests their timeline, fetch tweets from all accounts they follow and merge on-the-fly.

**Implementation:**

When user requests timeline:

1. Fetch all followees from Follower Graph
2. For each followee: Fetch their $N$ most recent tweets
3. Merge all tweets, sort by timestamp, return top 50

**Advantages:**

- ✅ **Cheap Writes:** One tweet = $1$ write (just save the tweet)
- ✅ **No Celebrity Problem:** Doesn't matter how many followers someone has
- ✅ **No Wasted Work:** Only compute timelines for active users

**Disadvantages:**

- ❌ **Slow Reads:** Must query $N$ accounts (where $N$ = number of followees), merge, sort
- ❌ **Complex Read Logic:** Merging $N$ sorted lists is $\text{O}(N \log N)$
- ❌ **High Read Latency:** Can take seconds for users following thousands

**When to Use:** For celebrity accounts with millions of followers.

*See [pseudocode.md::fanout_on_read()](pseudocode.md) for implementation.*

---

#### Strategy 3: Hybrid Fanout (Twitter's Approach) ✅ Recommended

**How It Works:**

- **Normal users** ($< 10,000$ followers): Use Fanout-on-Write (push)
- **Celebrity users** ($> 10,000$ followers): Use Fanout-on-Read (pull)
- **Timeline Reads:** Merge pre-computed timeline (push) with on-demand celebrity tweets (pull)

**Implementation:**

**Write Path:**

```
If author has < 10,000 followers:
    Fanout to all followers (push model)
Else:
    Mark as celebrity, skip fanout
```

**Read Path:**

```
1. Fetch pre-computed timeline from cache/Cassandra (covers ~90% of feed)
2. For each celebrity the user follows:
    Fetch their N most recent tweets
3. Merge both result sets, sort by timestamp
4. Return top 50 tweets
```

**Advantages:**

- ✅ Balances write cost and read latency
- ✅ Handles celebrity problem gracefully
- ✅ Optimizes for the common case (95% of users)

**Disadvantages:**

- ❌ Increased system complexity
- ❌ More challenging to debug and monitor
- ❌ Celebrity tweets might appear slightly slower (pull on read)

*See [pseudocode.md::hybrid_fanout()](pseudocode.md) for detailed implementation.*

---

### 3.6 Handling Celebrity Problem: Optimizations

**Problem:** When @Cristiano (500M followers) posts, hybrid model still requires fetching his tweet for every timeline
load.

**Optimization 1: Celebrity Tweet Cache**

Cache celebrity tweets separately in Redis:

```
celebrity:<user_id>:recent_tweets → [tweet_id1, tweet_id2, ...]
TTL: 1 hour
```

When loading timeline, batch-fetch celebrity tweets from this cache.

**Optimization 2: Tiered Fanout**

Even for celebrities, fanout to "super fans" (highly active followers):

```
If follower logged in last 24 hours:
    Fanout tweet to their timeline (push)
Else:
    Skip (they'll pull on next login)
```

**Optimization 3: Dedicated Celebrity Pipeline**

- Separate Kafka topic: `celebrity_tweets`
- Higher-priority fanout workers
- Separate monitoring and alerting

---

## 4. Bottlenecks and Future Scaling

### 4.1 Bottleneck: Celebrity Problem (High Fanout)

**Problem:** When @BarackObama (130M followers) posts, system must handle 130M timeline inserts.

**Current Mitigation:**

- Hybrid fanout: Use pull model for celebrities
- Tiered fanout: Only push to active followers

**Future Enhancements:**

- **Sampling:** Only deliver to subset of followers, others fetch on-demand
- **Bloom Filter:** Skip followers who recently saw a tweet from this celebrity
- **Geographically Tiered:** Fanout to users in same region first (locality)

---

### 4.2 Bottleneck: Cache Hotspots

**Problem:** Timelines of popular users (followed by millions) requested millions of times/sec.

**Mitigation:**

- **Consistent Hashing:** Distribute popular `user_id` keys across Redis cluster (
  see [2.2.2](../../02-components/2.2.2-consistent-hashing.md))
- **Multi-Master Redis:** High availability, automatic failover
- **CDN Caching:** For public profiles/timelines (read-only)
- **Connection Pooling:** Reuse connections to Redis (avoid TCP overhead)

---

### 4.3 Bottleneck: Storage Growth

**Problem:** $200$ million tweets/day = $73$ TB/year. Timeline storage grows even faster (denormalized).

**Mitigation:**

- **Archive Old Tweets:** Move tweets $> 6$ months to cold storage (S3, Glacier)
- **Timeline Pruning:** Limit timeline history to 10,000 most recent tweets per user
- **Compression:** Use columnar storage and compression in Cassandra
- **Sharding:** Partition Cassandra by `user_id` across hundreds of nodes

---

### 4.4 Bottleneck: Kafka Throughput

**Problem:** At peak (viral event), Kafka might receive $50,000$ messages/sec.

**Mitigation:**

- **Partition Kafka Topic:** Use `user_id` as partition key → spread load across brokers
- **Batch Writes:** Fanout workers batch timeline inserts (100 writes/batch) to reduce overhead
- **Scale Kafka Cluster:** Add more brokers as traffic grows
- **Monitor Consumer Lag:** Alert when consumer lag exceeds threshold (slow fanout)

---

## 5. Common Anti-Patterns

### Anti-Pattern 1: Fanout to All Followers (Including Inactive)

**Problem:**

❌ **Naive Fanout:**

When a celebrity posts, fanning out to all 500M followers wastes writes. 90% of those followers might be inactive (
haven't logged in for months).

**Why It's Bad:**

- Wastes database writes and storage
- Increases fanout lag
- Higher infrastructure cost

**Better:**

✅ **Activity-Based Fanout:**

Only fanout to followers who logged in recently:

*Description:* Check if follower is active (logged in last 30 days). If active, insert tweet into their timeline.
Otherwise, skip (they'll fetch on next login using pull model).

*See [pseudocode.md::activity_based_fanout()](pseudocode.md) for implementation.*

---

### Anti-Pattern 2: Synchronous Fanout (Blocking Write)

**Problem:**

❌ **Blocking Fanout:**

If Post Service waits for fanout to complete before returning to client, users with many followers experience slow post
times (several seconds).

**Why It's Bad:**

- Poor user experience (slow API response)
- Post Service becomes bottleneck
- Can't absorb write spikes

**Better:**

✅ **Async Fanout:**

Post Service publishes to Kafka and returns immediately. Fanout Workers process asynchronously.

*Description:* Save tweet to database, publish event to Kafka, and return success to client without waiting. Fanout
workers consume from Kafka asynchronously and insert into timelines.

*See [sequence-diagrams.md](sequence-diagrams.md) for async fanout flow visualization.*

---

### Anti-Pattern 3: No Cache Invalidation Strategy

**Problem:**

❌ **Stale Cache:**

Timeline cache never expires. When a tweet is deleted or edited, users still see the old version in their timeline.

**Why It's Bad:**

- Inconsistent user experience
- Deleted content remains visible
- Violates content policy

**Better:**

✅ **Cache Invalidation:**

- **TTL:** Set 1-hour TTL on timeline cache entries (automatic expiration)
- **Active Invalidation:** When tweet is deleted, publish invalidation event to clear affected timelines
- **Eventual Consistency:** Accept that users might see deleted tweets for a few minutes

---

### Anti-Pattern 4: Storing Entire Tweet in Timeline

**Problem:**

❌ **Full Denormalization:**

Storing complete tweet object (including user profile, media URLs) in each follower's timeline wastes space.

**Why It's Bad:**

- Duplicates user profile data millions of times
- Expensive storage cost
- Complex updates (when user changes profile)

**Better:**

✅ **Store Tweet ID + Minimal Metadata:**

*Description:* In UserTimeline, store only the tweet_id, timestamp, and author_user_id. Client makes a second call to
fetch full tweet details (can be batched: 50 tweet IDs in single query to PostgreSQL or cache).

*See [pseudocode.md::load_timeline_with_details()](pseudocode.md) for implementation.*

---

## 6. Alternative Approaches (Not Chosen)

### Approach A: Pure Fanout-on-Read (Pull Model)

**Architecture:**

- No pre-computed timelines
- When user requests timeline, fetch tweets from all followees and merge on-the-fly

**Pros:**

- ✅ Simple write path (just save tweet)
- ✅ No storage waste on inactive users
- ✅ Always up-to-date (no cache consistency issues)

**Cons:**

- ❌ Very slow reads ($> 1$ second for users following thousands)
- ❌ Complex merge logic ($N$ sorted lists)
- ❌ High CPU cost on read path

**Why Not Chosen:**

Twitter's use case is **read-heavy** (50:1 ratio). Optimizing read latency is critical. Users expect instant timeline
loads. Pull model sacrifices read performance.

---

### Approach B: NoSQL-Only (No RDBMS)

**Architecture:**

- Use Cassandra or DynamoDB for everything (tweets, timelines, follower graph)
- No PostgreSQL

**Pros:**

- ✅ Horizontal scaling is easier
- ✅ Lower operational complexity (one DB type)
- ✅ Built-in replication and high availability

**Cons:**

- ❌ Weaker consistency guarantees (eventual consistency everywhere)
- ❌ Complex queries (no joins, limited secondary indexes)
- ❌ Harder to model relational data (replies, likes, retweets)

**Why Not Chosen:**

Tweets are the **source of truth** and require strong consistency. We cannot afford duplicate tweets, lost tweets, or
conflicting IDs. PostgreSQL provides ACID guarantees. Using PostgreSQL for tweets + Cassandra for timelines gives us the
best of both worlds.

---

### Approach C: Pure Fanout-on-Write (No Hybrid)

**Architecture:**

- Fanout to all followers for every user (including celebrities)

**Pros:**

- ✅ Simplest implementation
- ✅ Fastest reads (always pre-computed)

**Cons:**

- ❌ Crushes system when celebrities post
- ❌ Extremely expensive infrastructure cost
- ❌ Wastes resources on inactive followers

**Why Not Chosen:**

Does not scale for celebrity users. When @Cristiano posts, fanning out to 500M followers would:

- Take minutes to complete (slow fanout)
- Require massive database cluster ($\sim 10$× larger)
- Waste writes on inactive users ($\sim 90$% waste)

---

## 7. Monitoring and Observability

### Key Metrics to Monitor

| Metric                           | Target              | Alert Threshold       | Description                                              |
|----------------------------------|---------------------|-----------------------|----------------------------------------------------------|
| **Timeline Read Latency (P99)**  | $< 200$ ms          | $> 500$ ms            | Time to load timeline (cache hit + miss)                 |
| **Tweet Write Latency (P99)**    | $< 100$ ms          | $> 300$ ms            | Time to save tweet and publish to Kafka                  |
| **Fanout Lag**                   | $< 10$ seconds      | $> 60$ seconds        | Time between tweet posted and delivered to all timelines |
| **Kafka Consumer Lag**           | $< 1,000$ messages  | $> 10,000$ messages   | How far behind are fanout workers?                       |
| **Cache Hit Rate**               | $> 90\%$            | $< 70\%$              | Percentage of timeline requests served from cache        |
| **Cassandra Write Throughput**   | $50,000$ writes/sec | $< 20,000$ writes/sec | Timeline insertion rate                                  |
| **Follower Graph Query Latency** | $< 50$ ms           | $> 200$ ms            | Time to fetch all followers for fanout                   |

### Dashboards and Alerts

**Dashboard 1: Write Path Health**

- Tweet creation rate (QPS)
- Kafka publish rate and broker lag
- Fanout worker throughput
- Cassandra write latency

**Dashboard 2: Read Path Health**

- Timeline request rate (QPS)
- Cache hit/miss ratio
- Read latency (P50, P99, P999)
- Error rate (4xx, 5xx)

**Dashboard 3: System Health**

- Redis cluster CPU/memory
- Cassandra cluster CPU/disk
- PostgreSQL replication lag
- Kafka consumer lag

**Critical Alerts:**

- Fanout lag $> 60$ seconds → Scale up fanout workers
- Cache hit rate $< 70\%$ → Investigate cache eviction or TTL issues
- Kafka consumer lag $> 10,000$ → Scale up consumers or increase partition count
- PostgreSQL replication lag $> 10$ seconds → Check replica health

---

## 8. Trade-offs Summary

### What We Gained

✅ **Low Read Latency:** Pre-computed timelines enable $< 10$ ms cache hits
✅ **High Write Throughput:** Kafka absorbs spikes, async fanout scales independently
✅ **Handles Celebrity Problem:** Hybrid fanout model balances write cost and read performance
✅ **Horizontal Scaling:** Cassandra and Kafka scale to petabytes and millions of QPS
✅ **Cost Efficiency:** Cassandra cheaper than Redis for full timeline storage

### What We Sacrificed

❌ **Eventual Consistency:** Timelines might lag behind reality by a few seconds
❌ **Storage Cost:** Denormalized timelines consume $\sim 20$ TB (expensive)
❌ **System Complexity:** Hybrid fanout model is harder to implement and debug
❌ **Celebrity Timeline Lag:** Celebrities' tweets might appear slower (pull on read)
❌ **Operational Overhead:** Multiple database types (PostgreSQL, Cassandra, Redis, Neo4j)

---

## 9. Real-World Implementations

### Twitter (2012-2020)

- **Fanout Model:** Hybrid (push for normal users, pull for celebrities)
- **Timeline Storage:** Redis (cache) + MySQL (persistent) + Manhattan (distributed DB)
- **Message Queue:** Kestrel (early), later migrated to Kafka
- **Key Optimization:** Pre-computation of timelines for active users only

### Facebook News Feed

- **Fanout Model:** Mostly pull (fanout-on-read) with ML ranking
- **Storage:** TAO (distributed graph store) + Memcache
- **Key Difference:** Heavy ML ranking (not just chronological), so pre-computing is harder

### Instagram

- **Fanout Model:** Hybrid, with emphasis on media-optimized storage
- **Storage:** Cassandra + PostgreSQL + CDN for images/videos
- **Key Optimization:** Lazy-load images, aggressive CDN caching

---

## 10. References and Further Reading

- [CAP Theorem](../../01-principles/1.1.1-cap-theorem.md) - Understanding consistency trade-offs
- [Data Consistency Models](../../01-principles/1.1.4-data-consistency-models.md) - ACID vs BASE
- [Asynchronous Communication](../../02-components/2.3.1-asynchronous-communication.md) - Kafka, queues, pub/sub
  patterns
- [Kafka Deep Dive](../../02-components/2.3.2-kafka-deep-dive.md) - Partitions, consumer groups, offset management
- [NoSQL Deep Dive](../../02-components/2.1.2-no-sql-deep-dive.md) - Cassandra, wide-column stores, BASE principle
- [Consistent Hashing](../../02-components/2.2.2-consistent-hashing.md) - Distributing cache keys across Redis cluster
- [Database Scaling](../../02-components/2.1.4-database-scaling.md) - Sharding, replication strategies

---

**Next Steps:**

- See **[hld-diagram.md](hld-diagram.md)** for visual system architecture diagrams
- See **[sequence-diagrams.md](sequence-diagrams.md)** for detailed interaction flows and failure scenarios
- See **[this-over-that.md](this-over-that.md)** for in-depth analysis of design decisions and trade-offs
- See **[pseudocode.md](pseudocode.md)** for detailed algorithm implementations

