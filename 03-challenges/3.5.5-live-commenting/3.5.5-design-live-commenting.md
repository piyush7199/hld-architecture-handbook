# 3.5.5 Design Live Commenting (Facebook Live/Twitch)

> üìö **Note on Implementation Details:**
> This document focuses on high-level design concepts and architectural decisions.
> For detailed algorithm implementations, see **[pseudocode.md](./pseudocode.md)**.

## üìä Visual Diagrams & Resources

- **[High-Level Design Diagrams](./hld-diagram.md)** - System architecture, component design, data flow
- **[Sequence Diagrams](./sequence-diagrams.md)** - Detailed interaction flows and failure scenarios
- **[Design Decisions (This Over That)](./this-over-that.md)** - In-depth analysis of architectural choices
- **[Pseudocode Implementations](./pseudocode.md)** - Detailed algorithm implementations

---

## 1. Problem Statement

Design a real-time commenting system for live streaming platforms like **Facebook Live** or **Twitch** that delivers
comments to millions of concurrent viewers with sub-second latency. The system must handle massive fanout (1 comment ‚Üí 5M
viewers), extreme write bursts (10k comments/sec), real-time moderation, and maintain persistent connections for instant
delivery.

**Key Challenges:**

- **Massive Fanout**: Single comment must reach millions of viewers simultaneously
- **Real-Time Delivery**: Comments must appear within 500ms
- **Connection Management**: Maintain millions of persistent WebSocket connections
- **Bursty Traffic**: Handle sudden spikes during viral moments
- **Moderation**: Filter offensive content in real-time without blocking delivery
- **Horizontal Scaling**: Scale WebSocket servers while maintaining connection affinity

Unlike traditional chat systems, live commenting requires:

- **Broadcast-First Architecture**: One-to-many delivery (not peer-to-peer)
- **Ephemeral Priority**: Real-time delivery more critical than durability
- **Massive Concurrency**: Handle millions of simultaneous connections per stream
- **Adaptive Throttling**: Sample comments during extreme traffic to prevent overload

---

## 2. Requirements and Scale Estimation

### 2.1 Functional Requirements

| Requirement | Description | Priority |
|------------|-------------|----------|
| **Real-Time Delivery** | Comments broadcast to all viewers within 500ms | Critical |
| **High Fanout** | Single comment delivered to millions of viewers simultaneously | Critical |
| **Persistent Connections** | Maintain WebSocket connections for instant push | Critical |
| **Moderation** | Real-time spam/offensive content filtering | High |
| **Comment History** | Store comments for replay/analytics | Medium |
| **User Presence** | Show active viewer count | High |
| **Emoji Reactions** | Support quick reactions (like, heart, etc.) | Medium |
| **Pinned Comments** | Allow broadcasters to pin important comments | Low |

### 2.2 Non-Functional Requirements

| Requirement | Target | Justification |
|------------|--------|---------------|
| **Comment Latency** | $< 500$ ms (p95) | Sub-second delivery for real-time feel |
| **Connection Stability** | 99.9% uptime | Viewers expect uninterrupted streams |
| **Write Throughput** | 10k comments/sec per stream | Handle viral moments |
| **Fanout Capacity** | 50B pushes per event | 10k comments √ó 5M viewers |
| **Connection Limit** | 1M connections per server | Efficient resource utilization |
| **Data Consistency** | Eventual consistency acceptable | Real-time delivery prioritized over ordering |

### 2.3 Scale Estimation

#### Traffic Metrics

| Metric | Assumption | Calculation | Result |
|--------|-----------|-------------|---------|
| **Concurrent Viewers** | 5M viewers per major event | - | 5M concurrent connections |
| **Comment Ingestion** | 2% of viewers comment actively | $5M \times 0.02 = 100k$ active commenters | 100k potential writers |
| **Peak Comment Rate** | 10% of active commenters/sec | $100k \times 0.1$ | 10k comments/sec |
| **Fanout Operations** | 10k comments √ó 5M viewers | $10k \times 5M$ | 50B pushes/sec peak |
| **Average Comment Rate** | 1k comments/sec sustained | - | 1k comments/sec avg |
| **Typical Event Duration** | 2 hours | - | 2 hours streaming |

#### Storage Estimation

| Component | Calculation | Result |
|-----------|-------------|---------|
| **Comment Size** | 200 bytes avg (text + metadata) | 200 bytes |
| **Comments per Event** | 1k comments/sec √ó 7200 sec | 7.2M comments |
| **Storage per Event** | $7.2M \times 200 \text{ bytes}$ | 1.44 GB/event |
| **Daily Events** | 10k simultaneous streams | 10k events |
| **Daily Storage** | $10k \times 1.44 \text{ GB}$ | 14.4 TB/day |
| **Annual Storage** | $14.4 \text{ TB} \times 365$ | 5.26 PB/year |
| **With Compression** | $5.26 \text{ PB} \times 0.3$ | 1.58 PB/year |

#### Bandwidth Estimation

| Metric | Calculation | Result |
|--------|-------------|---------|
| **Comment Payload** | 200 bytes per comment | 200 bytes |
| **Peak Fanout Bandwidth** | $10k \times 5M \times 200 \text{ bytes}$ | 10 TB/sec peak |
| **Average Fanout Bandwidth** | $1k \times 5M \times 200 \text{ bytes}$ | 1 TB/sec avg |
| **Daily Bandwidth** | $1 \text{ TB/sec} \times 86400 \text{ sec}$ | 86 PB/day |
| **Monthly Bandwidth** | $86 \text{ PB} \times 30$ | 2.58 EB/month |

**Key Insight**: Bandwidth cost is the dominant expense. Optimization strategies include compression, adaptive sampling,
and efficient serialization.

#### Infrastructure Estimation

| Component | QPS | CPU per Request | Total Cores |
|-----------|-----|-----------------|-------------|
| **Ingestion Service** | 10k peak writes | 10ms CPU | $\sim 100$ cores |
| **Moderation Service** | 10k peak | 50ms CPU | $\sim 500$ cores |
| **WebSocket Servers** | 5M connections | 0.1ms per message | $\sim 5,000$ servers |
| **Broadcast Coordinator** | 10k fanout ops | 20ms CPU | $\sim 200$ cores |

**Total Infrastructure**: 5,000+ servers primarily for WebSocket connection management.

---

## 3. High-Level Architecture

The system is a specialized **Pub/Sub architecture** optimized for **massive fanout-on-write** using **WebSockets** for
real-time bidirectional communication.

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                          Live Commenting System                              ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                                                ‚îÇ
‚îÇ  ‚îÇ Viewers  ‚îÇ                                                                ‚îÇ
‚îÇ  ‚îÇ (5M)     ‚îÇ                                                                ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                                                ‚îÇ
‚îÇ       ‚îÇ WebSocket                                                            ‚îÇ
‚îÇ       ‚Üì                                                                      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                   ‚îÇ
‚îÇ  ‚îÇ   WebSocket Cluster (5k servers)     ‚îÇ                                   ‚îÇ
‚îÇ  ‚îÇ   - Persistent connections            ‚îÇ                                   ‚îÇ
‚îÇ  ‚îÇ   - Push comments to viewers          ‚îÇ                                   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                   ‚îÇ
‚îÇ            ‚îÇ Redis Pub/Sub                                                   ‚îÇ
‚îÇ            ‚Üì                                                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                   ‚îÇ
‚îÇ  ‚îÇ   Broadcast Coordinator               ‚îÇ                                   ‚îÇ
‚îÇ  ‚îÇ   - Fanout orchestration              ‚îÇ                                   ‚îÇ
‚îÇ  ‚îÇ   - Connection routing                ‚îÇ                                   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                   ‚îÇ
‚îÇ            ‚îÇ Kafka Stream                                                    ‚îÇ
‚îÇ            ‚Üì                                                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                   ‚îÇ
‚îÇ  ‚îÇ   Kafka (Comment Stream)              ‚îÇ                                   ‚îÇ
‚îÇ  ‚îÇ   - Buffer comments                   ‚îÇ                                   ‚îÇ
‚îÇ  ‚îÇ   - Event sourcing                    ‚îÇ                                   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                  ‚îÇ
‚îÇ        ‚îÇ           ‚îÇ                                                         ‚îÇ
‚îÇ        ‚îÇ           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                           ‚îÇ
‚îÇ        ‚Üì                         ‚Üì                                           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                  ‚îÇ
‚îÇ  ‚îÇ Moderation  ‚îÇ          ‚îÇ  Persistence ‚îÇ                                  ‚îÇ
‚îÇ  ‚îÇ Service     ‚îÇ          ‚îÇ  (Cassandra) ‚îÇ                                  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                  ‚îÇ
‚îÇ                                                                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 3.1 Core Components

| Component | Technology | Purpose | Scalability |
|-----------|-----------|---------|-------------|
| **API Gateway** | Kong/NGINX | Rate limiting, auth, routing | Horizontal (stateless) |
| **Ingestion Service** | Go/Java | Accept comments, validate | Horizontal (stateless) |
| **Kafka** | Kafka Cluster | Buffer comment stream, event sourcing | Partitioned by stream_id |
| **Moderation Service** | Python/ML | Real-time spam/hate speech detection | Horizontal (async) |
| **Broadcast Coordinator** | Go | Fanout orchestration, routing | Horizontal (sharded) |
| **WebSocket Cluster** | Go/Elixir | Maintain persistent connections | Horizontal (connection sharding) |
| **Redis Pub/Sub** | Redis Cluster | Internal message routing | Sharded by stream_id |
| **Cassandra** | Cassandra | Store comment history | Partitioned by stream_id |
| **Connection Registry** | Redis | Track which server holds which connection | Sharded by user_id |

---

## 4. Data Models

### 4.1 PostgreSQL (Metadata)

#### Streams Table

```sql
CREATE TABLE streams (
    stream_id BIGINT PRIMARY KEY,
    broadcaster_id BIGINT NOT NULL,
    title VARCHAR(500),
    status VARCHAR(20), -- 'live', 'ended', 'scheduled'
    started_at TIMESTAMP,
    ended_at TIMESTAMP,
    viewer_count INT DEFAULT 0,
    comment_count INT DEFAULT 0,
    created_at TIMESTAMP DEFAULT NOW()
);

CREATE INDEX idx_streams_broadcaster ON streams(broadcaster_id);
CREATE INDEX idx_streams_status ON streams(status);
```

#### Users Table

```sql
CREATE TABLE users (
    user_id BIGINT PRIMARY KEY,
    username VARCHAR(50) UNIQUE NOT NULL,
    display_name VARCHAR(100),
    avatar_url VARCHAR(500),
    is_banned BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMP DEFAULT NOW()
);
```

### 4.2 Cassandra (Comment Storage)

#### Comments Table

```sql
CREATE TABLE comments (
    stream_id BIGINT,
    comment_id TIMEUUID,
    user_id BIGINT,
    message TEXT,
    is_deleted BOOLEAN,
    created_at TIMESTAMP,
    PRIMARY KEY (stream_id, comment_id)
) WITH CLUSTERING ORDER BY (comment_id DESC);

-- Time-series optimized for recent comments
```

#### Moderation Actions Table

```sql
CREATE TABLE moderation_actions (
    stream_id BIGINT,
    comment_id TIMEUUID,
    action VARCHAR(20), -- 'flagged', 'deleted', 'shadow_ban'
    reason TEXT,
    moderator_id BIGINT,
    created_at TIMESTAMP,
    PRIMARY KEY (stream_id, created_at, comment_id)
) WITH CLUSTERING ORDER BY (created_at DESC);
```

### 4.3 Redis (Connection Registry)

#### Connection Mapping

```
Key: conn:user:{user_id}
Type: Hash
Fields:
  - stream_id: BIGINT
  - ws_server: "ws-server-42.example.com"
  - connected_at: TIMESTAMP
TTL: 2 hours (auto-cleanup stale connections)

Operations:
HSET conn:user:123 stream_id 789 ws_server "ws-42" connected_at 1699123456
HGET conn:user:123 ws_server  // Find which server holds connection
HDEL conn:user:123  // Remove on disconnect
```

#### Stream Presence (Viewer Count)

```
Key: stream:viewers:{stream_id}
Type: HyperLogLog

Operations:
PFADD stream:viewers:789 user_123  // Add viewer (unique count)
PFCOUNT stream:viewers:789  // Get unique viewer count
```

#### Active Streams Cache

```
Key: stream:{stream_id}
Type: Hash
Fields:
  - broadcaster_id: BIGINT
  - title: VARCHAR
  - viewer_count: INT
  - status: VARCHAR
TTL: 1 hour

Operations:
HGETALL stream:789  // Get stream metadata
HINCRBY stream:789 viewer_count 1  // Increment viewer count
```

---

## 5. Detailed Component Design

### 5.1 WebSocket Connection Management

**Challenge**: Maintain 5M persistent WebSocket connections efficiently.

**Solution**: Connection sharding across 5,000 servers with connection registry.

#### Connection Establishment Flow

*See pseudocode.md::establish_websocket_connection() for implementation*

1. **Client Request**: User opens stream page
2. **API Gateway**: Routes to available WebSocket server (via load balancer)
3. **Authentication**: Validate JWT token
4. **Connection Registry**: Store mapping in Redis (`conn:user:{user_id} ‚Üí ws-server-42`)
5. **Subscribe**: WebSocket server subscribes to Redis Pub/Sub channel `stream:{stream_id}`
6. **Heartbeat**: Client sends ping every 30 seconds

**Why Connection Registry?**

- Allows broadcast coordinator to find which server holds a specific user's connection
- Enables targeted disconnect/ban operations
- Supports connection migration during server scaling

#### Connection Scaling

| Metric | Value | Notes |
|--------|-------|-------|
| **Connections per Server** | 1M connections | Optimized Go/Elixir with epoll/kqueue |
| **Memory per Connection** | 4 KB | Minimal buffer, shared read/write queues |
| **Total Memory (5M connections)** | 20 GB | 5M √ó 4 KB across 5 servers |
| **CPU per Message** | 0.1ms | Efficient serialization (MessagePack) |

**Optimization Techniques**:

- **Zero-Copy IO**: Use `sendfile()` syscall to avoid kernel-user space copying
- **Connection Pooling**: Reuse HTTP/2 connections to Redis
- **Batch Writes**: Buffer multiple comments, flush every 10ms
- **Message Compression**: Enable WebSocket compression (permessage-deflate)

### 5.2 Comment Ingestion and Moderation

**Flow**:

1. **Client Sends Comment**: WebSocket message to ingestion server
2. **Rate Limiting**: Per-user limit (10 comments/min)
3. **Validation**: Check message length, format
4. **Publish to Kafka**: Write to `comments` topic (partition by `stream_id`)
5. **Async Moderation**: Moderation service consumes Kafka, runs ML model
6. **Broadcast**: Immediately broadcast to viewers (don't wait for moderation)
7. **Delete if Flagged**: If moderation flags comment, publish `COMMENT_DELETED` event

**Why Async Moderation?**

- **Write Speed Priority**: Comments published instantly (<50ms)
- **Eventual Consistency Acceptable**: Offensive comment visible for 1-2 seconds before removal
- **User Experience**: No blocking wait for AI model (100ms latency)

**Moderation Pipeline**:

*See pseudocode.md::moderate_comment() for implementation*

```
function moderate_comment(comment):
  // Stage 1: Rule-Based Filter (fast, 5ms)
  if contains_blacklisted_words(comment.message):
    return "REJECT", "blacklisted_word"
  
  // Stage 2: ML Model (slower, 50ms)
  toxicity_score = ml_model.predict(comment.message)
  if toxicity_score > 0.9:
    return "REJECT", "high_toxicity"
  
  // Stage 3: User Reputation
  if user.is_flagged and toxicity_score > 0.5:
    return "SHADOW_BAN", "repeat_offender"
  
  return "ACCEPT", ""
```

**Moderation Actions**:

- **DELETE**: Remove comment from all viewers' streams (publish delete event)
- **SHADOW_BAN**: Show comment only to author (don't broadcast)
- **FLAG**: Mark for manual review, allow to remain visible

### 5.3 Broadcast and Fanout

**Challenge**: Deliver 1 comment to 5M viewers in <500ms.

**Architecture**: Two-tier fanout (Kafka ‚Üí Redis Pub/Sub ‚Üí WebSocket servers)

#### Tier 1: Kafka to Broadcast Coordinator

```
Kafka Partition (stream_id=789)
      ‚Üì
Broadcast Coordinator (100 instances)
      ‚Üì
Redis Pub/Sub channel (stream:789:comments)
```

#### Tier 2: Redis Pub/Sub to WebSocket Servers

```
Redis Pub/Sub (stream:789:comments)
      ‚Üì
5,000 WebSocket Servers (subscribed to channel)
      ‚Üì
5M WebSocket Connections (push to clients)
```

**Why Redis Pub/Sub?**

- **Low Latency**: <1ms internal routing
- **Automatic Fan-out**: Redis handles message duplication to all subscribers
- **Decoupling**: WebSocket servers don't need to know about each other

**Fanout Performance**:

- **Kafka ‚Üí Coordinator**: 10ms (single partition read)
- **Coordinator ‚Üí Redis**: 5ms (publish to Redis channel)
- **Redis ‚Üí WebSocket Servers**: 10ms (parallel broadcast to 5k servers)
- **WebSocket ‚Üí Clients**: 50ms (network latency to clients)
- **Total Latency**: 75ms (p50), 150ms (p95)

### 5.4 Adaptive Throttling

**Problem**: Peak comment rate (10k/sec) may overwhelm network bandwidth.

**Solution**: Sample comments during extreme traffic.

*See pseudocode.md::adaptive_throttle() for implementation*

```
function adaptive_throttle(stream_id, comment):
  current_rate = get_comment_rate(stream_id)  // Comments/sec
  
  if current_rate > 5000:
    // Sample 20% of comments
    if random() < 0.2:
      broadcast(comment)
      notify_client("High traffic: showing 1 in 5 comments")
    else:
      store_only(comment)  // Save to DB but don't broadcast
  else:
    broadcast(comment)
```

**User Experience**:

- Show banner: "Chat moving fast! Showing sampled comments."
- Allow users to pause chat to read at their own pace
- Always store all comments (available in replay)

**Bandwidth Savings**:

- **Without Sampling**: 10k comments/sec √ó 5M viewers √ó 200 bytes = 10 TB/sec
- **With Sampling (20%)**: 2k comments/sec √ó 5M viewers √ó 200 bytes = 2 TB/sec
- **Reduction**: 80% bandwidth savings during peak

---

## 6. Advanced Features

### 6.1 Emoji Reactions (Fire, Heart, etc.)

**Challenge**: Handle 100k reactions/sec (faster than text comments).

**Solution**: Aggregate reactions locally, flush every 500ms.

*See pseudocode.md::aggregate_reactions() for implementation*

```
Client sends: üî• reaction
  ‚Üì
WebSocket Server buffers locally
  ‚Üì
Every 500ms: Flush aggregated counts to Redis
  ‚Üì
Redis: HINCRBY reactions:stream:789 fire 1250
  ‚Üì
Broadcast aggregated update: {"fire": 15000, "heart": 8000}
```

**Why Aggregation?**

- **Reduces Traffic**: 100k reactions/sec ‚Üí 2 updates/sec (50,000x reduction)
- **User Experience**: Show running total (doesn't need per-reaction update)
- **Bandwidth Savings**: 99.99% reduction in reaction traffic

### 6.2 Pinned Comments

**Flow**:

1. **Broadcaster Pins Comment**: `POST /stream/789/pin (comment_id=123)`
2. **Store in Redis**: `SET stream:789:pinned 123`
3. **Broadcast Event**: `{"type": "COMMENT_PINNED", "comment_id": 123}`
4. **Client UI**: Display pinned comment at top (sticky)

**Persistence**: Store pinned comment ID in Cassandra for replay.

### 6.3 Slow Mode

**Purpose**: Limit comment rate during moderation-heavy streams.

**Implementation**:

```
Set stream setting: slow_mode_interval = 10 seconds

// Enforce per-user rate limit
function allow_comment(user_id, stream_id):
  key = f"rate:user:{user_id}:stream:{stream_id}"
  last_comment_time = redis.GET(key)
  
  if last_comment_time and (now() - last_comment_time) < slow_mode_interval:
    return false, "Slow mode: wait 10 seconds"
  
  redis.SET(key, now(), EX=10)
  return true, ""
```

---

## 7. Availability and Fault Tolerance

### 7.1 WebSocket Server Failures

| Failure | Impact | Mitigation | Recovery Time |
|---------|--------|-----------|---------------|
| **Single Server Down** | 1k connections lost | Load balancer detects failure, clients reconnect | $< 5$ seconds |
| **Redis Pub/Sub Down** | Broadcast stops | Fallback to Kafka direct broadcast (slower) | $< 10$ seconds |
| **Kafka Partition Down** | 1/10th of streams affected | Kafka replicates to other brokers | $< 30$ seconds |
| **Connection Registry Down** | Can't route messages | Broadcast to all WebSocket servers (inefficient) | Immediate |

**Circuit Breaker Pattern**:

- If Redis Pub/Sub fails 50% of requests ‚Üí Open circuit
- Fallback: Broadcast coordinator directly polls WebSocket servers
- Recovery: Half-open after 30 seconds, retry

### 7.2 Graceful Degradation

**Scenarios**:

1. **Moderation Service Down**:
     - **Fallback**: Skip moderation, allow all comments
     - **Risk**: Spam may appear (acceptable for short duration)

2. **Cassandra Write Failure**:
     - **Fallback**: Buffer comments in Kafka (7-day retention)
     - **Recovery**: Replay Kafka to Cassandra when service restores

3. **Bandwidth Saturation**:
     - **Fallback**: Enable adaptive throttling (sample 10% of comments)
     - **Recovery**: Scale WebSocket cluster

---

## 8. Bottlenecks and Optimizations

### 8.1 WebSocket Server CPU Bottleneck

**Problem**: 1M connections √ó 10 messages/sec = 10M messages/sec per server.

**Solution**: Batch message delivery.

*See pseudocode.md::batch_message_delivery() for implementation*

```
function batch_message_delivery():
  buffer = []
  
  while true:
    messages = redis_pubsub.listen(timeout=10ms)
    buffer.extend(messages)
    
    if len(buffer) >= 100 or time_since_last_flush > 10ms:
      // Send all messages in one WebSocket frame
      for connection in connections:
        connection.send_batch(buffer)
      buffer.clear()
```

**Result**:

- **Without Batching**: 10M syscalls/sec
- **With Batching**: 100k syscalls/sec (100x reduction)
- **Latency Impact**: +10ms (acceptable)

### 8.2 Redis Pub/Sub Hot Key

**Problem**: Single Redis Pub/Sub channel (`stream:789:comments`) becomes hot key.

**Solution**: Shard Pub/Sub channels by WebSocket server.

```
// Instead of single channel:
stream:789:comments

// Use sharded channels:
stream:789:comments:shard0
stream:789:comments:shard1
...
stream:789:comments:shard99

// Broadcast coordinator publishes to all shards
for shard_id in 0..99:
  redis.publish(f"stream:789:comments:shard{shard_id}", comment)
```

**Result**:

- Distributes load across 100 Redis keys
- Reduces contention by 100x
- Each WebSocket server subscribes to 1 shard

### 8.3 Network Bandwidth Bottleneck

**Problem**: 10 TB/sec fanout bandwidth exceeds data center capacity.

**Solution**: Aggressive compression + protocol optimization.

**Optimizations**:

1. **MessagePack Serialization**: 50% smaller than JSON
     ```
     JSON: {"user": "john", "message": "Hello"}  // 40 bytes
     MessagePack: \x82\xa4user\xa4john\xa7message\xa5Hello  // 20 bytes
     ```

2. **WebSocket Compression**: permessage-deflate (60% reduction)

3. **Delta Encoding**: Send only changed fields
     ```
     First message: {"id": 123, "user": "john", "msg": "Hello"}
     Next message: {"id": 124, "msg": "World"}  // Reuse user from previous
     ```

4. **Adaptive Sampling**: 80% reduction during peak (as discussed)

**Result**:

- **Before**: 10 TB/sec
- **After**: 0.5 TB/sec (20x reduction)

---

## 9. Common Anti-Patterns

### ‚ùå **1. Polling for New Comments**

**Problem**:

- Client polls `/stream/789/comments?since=timestamp` every second
- 5M clients √ó 1 req/sec = 5M QPS on API servers
- High latency (1-second delay)

**Solution**: Use WebSockets for push-based delivery (this design).

**Why It's Bad**:

- 10x more expensive (server resources)
- 2x worse latency (500ms avg vs push)
- Wasted bandwidth (empty responses when no comments)

---

### ‚ùå **2. Synchronous Moderation**

**Problem**:

- Comment submission blocked until moderation completes
- ML model takes 100ms ‚Üí user waits
- Poor UX during high traffic

**Solution**: Async moderation with eventual consistency.

**Why It's Bad**:

- 3x slower comment delivery
- Users perceive system as "slow"
- Moderation becomes scaling bottleneck

---

### ‚ùå **3. Storing All Connections in Single Redis**

**Problem**:

- Connection registry stored in single Redis instance
- 5M connections √ó 200 bytes = 1 GB
- Single point of failure

**Solution**: Shard connection registry across 100 Redis nodes.

**Why It's Bad**:

- Redis down ‚Üí entire system fails
- Memory limit (single Redis: 64 GB max)
- Can't scale beyond 300M connections

---

### ‚ùå **4. Broadcasting from Kafka Directly to WebSocket Servers**

**Problem**:

- Each WebSocket server consumes from Kafka
- 5,000 servers √ó 10k partitions = 50M consumer connections
- Kafka overwhelmed

**Solution**: Use Redis Pub/Sub as intermediate layer.

**Why It's Bad**:

- Kafka can't handle 5k consumers per partition
- 10x higher Kafka broker CPU
- Message duplication and rebalancing issues

---

### ‚ùå **5. Not Implementing Adaptive Throttling**

**Problem**:

- Allow all 10k comments/sec to broadcast
- 10 TB/sec bandwidth ‚Üí network saturation
- WebSocket servers overloaded

**Solution**: Sample comments during extreme traffic.

**Why It's Bad**:

- Service degradation or outage
- 10x infrastructure cost to handle peaks
- User experience worse (stuttering, disconnects)

---

## 10. Alternative Approaches

### 10.1 HTTP/2 Server-Sent Events (SSE) Instead of WebSockets

**Pros**:

- Simpler protocol (one-way push)
- Easier to deploy (no protocol upgrade)
- Built-in reconnection

**Cons**:

- No bidirectional communication (need separate POST for comments)
- Less efficient (HTTP headers on every message)
- 6 connection limit per domain (browser restriction)

**Decision**: WebSockets chosen for bidirectional efficiency and no connection limits.

---

### 10.2 Cassandra Instead of Kafka for Event Streaming

**Pros**:

- Single database (less operational overhead)
- Strong consistency if needed

**Cons**:

- Not optimized for streaming (Kafka is purpose-built)
- No exactly-once semantics
- Higher write latency (50ms vs 5ms)

**Decision**: Kafka chosen for streaming efficiency and event sourcing capabilities.

---

### 10.3 GraphQL Subscriptions Instead of WebSockets

**Pros**:

- Schema-driven (type safety)
- Flexible field selection

**Cons**:

- GraphQL overhead (query parsing)
- Requires Apollo Server (complex setup)
- 10x higher latency

**Decision**: Raw WebSockets chosen for maximum performance.

---

## 11. Monitoring and Observability

### 11.1 Key Metrics

| Metric | Target | Alert Threshold | Impact |
|--------|--------|----------------|--------|
| **Comment Latency (p95)** | $< 500$ ms | $> 1$ sec | User experience degradation |
| **WebSocket Connection Count** | Baseline | $> 2$x baseline | Potential DDoS or scaling issue |
| **Redis Pub/Sub Latency** | $< 10$ ms | $> 50$ ms | Broadcast delays |
| **Kafka Consumer Lag** | $< 1$ sec | $> 10$ sec | Delayed comments |
| **Moderation Queue Depth** | $< 1000$ | $> 10000$ | Moderation falling behind |
| **Connection Drop Rate** | $< 0.1$% | $> 1$% | Network issues |

### 11.2 Distributed Tracing

**OpenTelemetry Trace Example**:

```
Span: Comment Ingestion (comment_id=123)
  ‚îú‚îÄ Span: Validate Comment (10ms)
  ‚îú‚îÄ Span: Publish to Kafka (5ms)
  ‚îú‚îÄ Span: Broadcast Fanout (50ms)
  ‚îÇ  ‚îú‚îÄ Span: Redis Publish (10ms)
  ‚îÇ  ‚îî‚îÄ Span: WebSocket Broadcast (40ms)
  ‚îî‚îÄ Span: Persist to Cassandra (20ms)

Total: 85ms
```

### 11.3 Alerting Rules

**Critical Alerts**:

- Comment latency p95 > 1 second for 5 minutes ‚Üí Page on-call
- WebSocket connection drop rate > 5% ‚Üí Page on-call
- Kafka consumer lag > 60 seconds ‚Üí Page on-call

**Warning Alerts**:

- Redis CPU > 80% ‚Üí Scale Redis cluster
- Moderation queue depth > 5000 ‚Üí Scale moderation workers
- Bandwidth usage > 80% ‚Üí Enable adaptive throttling

---

## 12. Deployment and Scaling

### 12.1 Horizontal Scaling

**WebSocket Cluster Scaling**:

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: websocket-servers
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: websocket-servers
  minReplicas: 1000
  maxReplicas: 10000
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
    - type: Pods
      pods:
        metric:
          name: active_connections
        target:
          type: AverageValue
          averageValue: "800000"  # 800k connections per pod
```

**Connection Migration During Scale-Up**:

1. New WebSocket server starts
2. Load balancer routes new connections to new server
3. Existing connections remain on old servers
4. No connection disruption

### 12.2 Multi-Region Deployment

**Active-Active Architecture**:

```
Region: US-EAST
  ‚îú‚îÄ Kafka Cluster (primary)
  ‚îú‚îÄ Redis Cluster
  ‚îú‚îÄ WebSocket Servers (2k)
  ‚îî‚îÄ Cassandra Ring (4 nodes)

Region: EU-WEST
  ‚îú‚îÄ Kafka Cluster (replica)
  ‚îú‚îÄ Redis Cluster
  ‚îú‚îÄ WebSocket Servers (2k)
  ‚îî‚îÄ Cassandra Ring (4 nodes)

Region: ASIA-PACIFIC
  ‚îú‚îÄ Kafka Cluster (replica)
  ‚îú‚îÄ Redis Cluster
  ‚îú‚îÄ WebSocket Servers (1k)
  ‚îî‚îÄ Cassandra Ring (3 nodes)
```

**Data Replication**:

- **Kafka**: MirrorMaker 2 for cross-region replication (100ms lag)
- **Cassandra**: Multi-datacenter replication (eventual consistency)
- **Redis**: No replication (local cache only)

---

## 13. Trade-offs Summary

| What You Gain | What You Sacrifice |
|---------------|-------------------|
| ‚úÖ **Ultra-low latency** ($< 500$ ms) | ‚ùå Eventual consistency (comments may be out of order) |
| ‚úÖ **Massive fanout** (50B pushes/sec) | ‚ùå High bandwidth cost ($\sim \$5$M/month) |
| ‚úÖ **Real-time moderation** | ‚ùå Offensive content visible for 1-2 seconds |
| ‚úÖ **Horizontal scalability** | ‚ùå Complex connection management (registry needed) |
| ‚úÖ **Adaptive throttling** (handles peaks) | ‚ùå Not all comments shown during extreme traffic |
| ‚úÖ **Persistent connections** (instant push) | ‚ùå Higher server resources (memory for connections) |

---

## 14. Real-World Examples

### 14.1 Twitch

**Architecture Highlights**:

- **IRC-Based Protocol**: Uses modified IRC for chat
- **TMI (Twitch Messaging Interface)**: Custom WebSocket layer
- **Sharded Chat Servers**: 1,000+ servers for global coverage
- **Badge System**: Subscriber/moderator badges prioritized in fanout

**Key Innovation**: Twitch uses **IRC over WebSocket**, leveraging mature IRC scalability patterns while gaining
WebSocket's efficiency.

**Scale**: 10M+ concurrent viewers, 1M+ messages/minute

---

### 14.2 YouTube Live

**Architecture Highlights**:

- **gRPC Streaming**: Uses gRPC bidirectional streaming
- **Spanner**: Global consistency for comment ordering
- **Content Delivery Network**: Comments served through Google CDN
- **Super Chat**: Paid messages highlighted (monetization)

**Key Innovation**: YouTube uses **Spanner for globally consistent ordering**, ensuring comments appear in correct order
across all regions.

**Scale**: 30M+ concurrent viewers (record: FIFA World Cup final)

---

### 14.3 Facebook Live

**Architecture Highlights**:

- **MQTT over WebSocket**: Lightweight pub/sub protocol
- **TAO**: Custom graph database for social connections
- **Edge Deployment**: Chat servers deployed at CDN edge
- **Reaction Aggregation**: Real-time emoji reaction counts

**Key Innovation**: Facebook uses **MQTT over WebSocket** for efficient mobile battery usage and low bandwidth.

**Scale**: 50M+ peak concurrent viewers (special events)

---

## 15. References

### 15.1 Related Chapters

- **[3.2.1 Twitter Timeline](../3.2.1-twitter-timeline/README.md)** - Fanout strategies
- **[3.3.1 Live Chat System](../3.3.1-live-chat-system/README.md)** - WebSocket architecture
- **[2.0.3 Real-Time Communication](../../02-components/2.0-communication/2.0.3-real-time-communication.md)** - WebSocket
  deep dive
- **[2.3.2 Kafka Deep Dive](../../02-components/2.3-messaging-streaming/2.3.2-kafka-deep-dive.md)** - Event streaming
- **[2.1.9 Cassandra Deep Dive](../../02-components/2.1-databases/2.1.9-cassandra-deep-dive.md)** - Time-series storage

### 15.2 External Resources

- **Twitch Engineering Blog**: "How Twitch Handles 10M+ Concurrent Viewers"
- **YouTube Engineering**: "Scaling YouTube Live Chat"
- **Facebook Engineering**: "MQTT at Facebook Scale"
- **IETF RFC 6455**: WebSocket Protocol Specification

---

## 16. Interview Discussion Points

### 16.1 Common Questions

**Question**: "How would you handle a celebrity streamer with 10M concurrent viewers?"

**Answer**: Use adaptive throttling:

- Enable sampling (show 10% of comments)
- Increase WebSocket server count from 5k to 20k
- Use dedicated Kafka partition for celebrity streams
- Pre-warm connection pool (anticipate traffic spike)

**Key Metrics**:

- 10M connections ‚Üí 20k servers (500k connections/server)
- Bandwidth: 10 TB/sec ‚Üí 1 TB/sec (with 90% sampling)
- Cost: $\sim \$100$k for 2-hour event (acceptable for major events)

---

**Question**: "What happens if Redis Pub/Sub fails?"

**Answer**: Fallback to Kafka direct broadcast:

- Broadcast coordinator directly writes to Kafka topic `ws-messages`
- Each WebSocket server consumes from Kafka (inefficient but functional)
- Latency degrades from 75ms to 200ms (acceptable temporarily)
- Auto-recover when Redis restores

**Trade-off**: 3x higher latency during fallback, but system remains operational.

---

**Question**: "How do you prevent spam and bot attacks?"

**Answer**: Multi-layer defense:

1. **Rate Limiting**: 10 comments/min per user (client-side + server-side)
2. **CAPTCHA**: Require CAPTCHA for new accounts
3. **Phone Verification**: Verified users have higher limits
4. **Reputation Score**: Lower limits for users with history of spam
5. **IP-Based Limiting**: 100 comments/min per IP (prevents bot farms)

**Result**: 99.9% spam reduction without impacting legitimate users.

---

**Question**: "How would you scale from 5M to 50M concurrent viewers?"

**Answer**:

| Component | 5M Viewers | 50M Viewers (10x) | Scaling Strategy |
|-----------|-----------|-------------------|------------------|
| **WebSocket Servers** | 5k servers | 50k servers | Horizontal scaling |
| **Redis Pub/Sub** | 100 nodes | 1000 nodes | Shard by stream_id |
| **Kafka** | 100 partitions | 1000 partitions | Partition by stream_id |
| **Bandwidth** | 1 TB/sec | 10 TB/sec | Enable 90% sampling by default |
| **Cost** | $\sim \$5$M/month | $\sim \$50$M/month | Optimize compression, sampling |

**Key Insight**: Bandwidth cost dominates. At 50M scale, adaptive sampling is mandatory (not optional).

---

## 17. Performance Optimization

### 17.1 Connection Keep-Alive

**Without Optimization**:

- Client sends ping every 30 seconds
- 5M connections √ó 20 bytes/ping = 100 MB/sec wasted

**With Optimization**:

- Server sends ping every 60 seconds (reduce by 50%)
- Use empty frame (0 bytes payload)
- Result: 50 MB/sec saved

### 17.2 Redis Pipelining

**Without Pipelining**:

```
for user_id in users:
  redis.SET(f"conn:user:{user_id}", server_id)  // 1 RTT each
```

**With Pipelining**:

```
pipeline = redis.pipeline()
for user_id in users:
  pipeline.SET(f"conn:user:{user_id}", server_id)
pipeline.execute()  // 1 RTT total
```

**Result**: 100x faster connection registration.

### 17.3 Message Deduplication

**Problem**: Network issues cause duplicate messages.

**Solution**: Client-side deduplication with message IDs.

```
function process_message(message):
  message_id = message.id
  
  if seen_messages.contains(message_id):
    return  // Skip duplicate
  
  seen_messages.add(message_id)
  display_comment(message)
  
  // Clean up old IDs (keep last 1000)
  if seen_messages.size() > 1000:
    seen_messages.remove_oldest()
```

---

## 18. Security Considerations

### 18.1 Authentication

**Token-Based Auth**:

```
Client connects with JWT in header:
Authorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...

WebSocket server validates:
1. Signature (using public key)
2. Expiry (iat, exp claims)
3. Scopes (read:comments, write:comments)
```

**Token Refresh**: Short-lived tokens (15 min), refresh via separate API.

### 18.2 Rate Limiting

**Multi-Layer Rate Limiting**:

| Layer | Limit | Purpose |
|-------|-------|---------|
| **Per User** | 10 comments/min | Prevent individual spam |
| **Per IP** | 100 comments/min | Prevent bot farms |
| **Per Stream** | 10k comments/sec | Protect stream from overload |
| **Global** | 1M comments/sec | Protect infrastructure |

### 18.3 DDoS Protection

**Mitigation Strategies**:

1. **Connection Limits**: Max 5 connections per IP
2. **Geographic Filtering**: Block regions with low legitimate traffic
3. **Behavior Analysis**: Detect bot patterns (identical messages, timing)
4. **Cloudflare Shield**: DDoS protection at edge

---

## 19. Cost Breakdown

### 19.1 Infrastructure Cost

| Component | Monthly Cost | Percentage | Notes |
|-----------|--------------|-----------|-------|
| **WebSocket Servers** | $2.5M | 50% | 5k servers @ $500/server |
| **Bandwidth** | $1.5M | 30% | 2.5 EB @ $0.06/GB |
| **Kafka** | $500k | 10% | 100 nodes @ $5k/node |
| **Redis** | $300k | 6% | 100 nodes @ $3k/node |
| **Cassandra** | $200k | 4% | 50 nodes @ $4k/node |
| **Total** | **$5M** | **100%** | Per month at peak scale |

**Cost Optimization Strategies**:

1. **Spot Instances for WebSocket Servers**: 70% cost reduction (workload resilient to restarts)
2. **Compression**: 50% bandwidth savings
3. **Adaptive Sampling**: 80% bandwidth savings during peaks
4. **Autoscaling**: Scale down during off-peak hours (40% savings)

**Optimized Cost**: $\sim \$2.5$M/month (50% reduction)

---

## 20. Design Challenge

### Problem

You're designing a live commenting system for a major sports streaming platform. The Super Bowl final is expected to
have **30M concurrent viewers** with **50k comments/sec** during key moments (touchdowns, halftime show). The system
must:

Requirements:

1. Deliver comments within 500ms
2. Handle 50k comments/sec burst for 10 minutes
3. Support emoji reactions (100k reactions/sec)
4. Implement adaptive throttling to prevent overload
5. Store comments for replay (watch later)

**Question**: How would you design the WebSocket cluster, choose fanout strategy, and implement adaptive throttling to
handle this extreme scale? What trade-offs would you accept?

---

### Solution

#### üß© Scenario

- **System**: Live commenting for Super Bowl (30M viewers)
- **Scale**: 50k comments/sec peak, 100k reactions/sec
- **Requirements**: <500ms latency, adaptive throttling, replay support
- **Duration**: 4 hours (game + pre/post show)

#### ‚úÖ Goal

- Handle 6x normal scale (5M ‚Üí 30M viewers)
- Deliver comments in <500ms
- Prevent system overload during viral moments
- Store all comments for replay

#### ‚öôÔ∏è Solution: Hybrid Fanout with Aggressive Sampling

**Why Hybrid Fanout + Sampling?**

| Approach | Fanout Bandwidth | Latency | Cost | Sampling |
|----------|------------------|---------|------|----------|
| **Full Broadcast** | 50k √ó 30M √ó 200B = 300 TB/sec | 500ms | $\sim \$10$M/hour | None |
| **Hybrid + 10% Sampling** | 5k √ó 30M √ó 200B = 30 TB/sec | 600ms | $\sim \$1$M/hour | 90% |
| **Hybrid + 5% Sampling** | 2.5k √ó 30M √ó 200B = 15 TB/sec | 650ms | $\sim \$500$k/hour | 95% |

**Decision**: 5% sampling (show 1 in 20 comments during peak).

**Design Decisions**:

**1. WebSocket Cluster Sizing**:

```
30M connections √∑ 500k connections/server = 60k servers

With overprovisioning (2x): 120k servers

Cost: 120k √ó $0.50/hour = $60k/hour
Total event cost: $60k √ó 4 hours = $240k
```

**2. Adaptive Throttling Logic**:

*See pseudocode.md::super_bowl_adaptive_throttle() for implementation*

```
function super_bowl_adaptive_throttle(comment):
  current_rate = get_comment_rate()
  
  if current_rate < 5000:
    broadcast_all(comment)  // Normal traffic
  elif current_rate < 20000:
    sample_rate = 0.5  // Show 50%
    if random() < sample_rate:
      broadcast(comment)
    store_all(comment)  // Always persist
  elif current_rate < 50000:
    sample_rate = 0.1  // Show 10%
    if random() < sample_rate:
      broadcast(comment)
    store_all(comment)
  else:
    sample_rate = 0.05  // Show 5%
    if random() < sample_rate:
      broadcast(comment)
    store_all(comment)
  
  display_banner(f"High traffic: showing {sample_rate*100}% of comments")
```

**3. Redis Pub/Sub Sharding**:

```
// 1000 shards (vs 100 normal)
for shard_id in 0..999:
  redis.publish(f"superbowl:comments:shard{shard_id}", comment)

// Each WebSocket server subscribes to 1 shard
// 60k servers √∑ 1000 shards = 60 servers per shard
```

**4. Emoji Reaction Aggregation**:

```
// Local aggregation every 100ms (vs 500ms normal)
buffer = {"fire": 0, "heart": 0, "touchdown": 0}

every 100ms:
  redis.HINCRBY("reactions:superbowl", "fire", buffer["fire"])
  buffer.clear()

// Broadcast aggregated total every 1 second
broadcast({"reactions": {"fire": 50000, "heart": 30000}})
```

**Example Implementation**:

```
Architecture:
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ  30M Viewers (WebSocket Connections)    ‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ
               ‚Üì
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ  120k WebSocket Servers                 ‚îÇ
  ‚îÇ  - 60k active, 60k standby              ‚îÇ
  ‚îÇ  - 250k connections each                ‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ Redis Pub/Sub (1000 shards)
               ‚Üì
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ  Broadcast Coordinator (1000 instances) ‚îÇ
  ‚îÇ  - Adaptive throttling enabled          ‚îÇ
  ‚îÇ  - 5% sampling during 50k comments/sec  ‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ Kafka (1000 partitions)
               ‚Üì
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ  Kafka Cluster                          ‚îÇ
  ‚îÇ  - 200 brokers                          ‚îÇ
  ‚îÇ  - 3x replication                       ‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ
               ‚Üì
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ  Cassandra (100 nodes)                  ‚îÇ
  ‚îÇ  - Stores all comments (no sampling)    ‚îÇ
  ‚îÇ  - 50k writes/sec capacity              ‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

#### ‚ö†Ô∏è Trade-offs and Challenges

**Challenge 1: Bandwidth Cost During Sampling**

**Problem**: Even with 5% sampling, bandwidth cost is $\sim \$500$k/hour.

**Solution**:

- Pre-negotiate CDN contract for event (bulk discount)
- Enable Brotli compression (additional 30% savings)
- Use MessagePack serialization (50% smaller than JSON)

**Result**: Cost reduced to $\sim \$250$k/hour.

**Challenge 2: User Experience During Heavy Sampling**

**Problem**: Users see only 5% of comments (missing 95%).

**Solution**:

- Display banner: "Chat moving extremely fast! Showing 1 in 20 comments. Pause to read."
- Implement "pause chat" button (user can scroll through at own pace)
- Highlight important comments (verified users, high engagement)

**Result**: Users understand why not all comments visible, acceptable UX.

**Challenge 3: Connection Stability with 60k Servers**

**Problem**: Managing 60k ephemeral servers (risk of misconfiguration).

**Solution**:

- Use Kubernetes StatefulSets (predictable naming)
- Health checks every 5 seconds (automatic pod replacement)
- Pre-warm servers 1 hour before event (detect issues early)

**Result**: 99.9% connection stability during event.

#### üß† Practical Considerations

**Real-World Architecture**:

- **Pre-Event Load Testing**: Simulate 50M connections 1 week before
- **Gradual Ramp-Up**: Open connections 30 minutes before kickoff
- **Regional Distribution**: 40% US, 30% EU, 20% Asia, 10% other
- **Fallback Strategy**: If sampling not enough, increase to 2% (show 1 in 50)

**Scaling Strategies**:

- **Horizontal Scaling**: Auto-scale WebSocket servers based on connection count
- **Vertical Scaling**: Use high-memory instances (768 GB RAM) for WebSocket servers
- **Geographic Distribution**: Deploy in 10 regions (reduce latency)

#### ‚úÖ Final Answer

| Aspect | Decision | Reason |
|--------|----------|--------|
| **WebSocket Cluster** | 120k servers (2x overprovision) | Handle 30M connections with redundancy |
| **Fanout Strategy** | Hybrid (Redis Pub/Sub + Kafka) | Balance latency and reliability |
| **Adaptive Sampling** | 5% during 50k comments/sec peak | Prevent bandwidth saturation (95% reduction) |
| **Redis Sharding** | 1000 shards (vs 100 normal) | Distribute Pub/Sub load |
| **Reaction Aggregation** | 100ms local buffer | Reduce reaction traffic by 99.9% |
| **Trade-off Accepted** | Not all comments shown (95% sampled) | Bandwidth cost $500k/hour ‚Üí $250k/hour |

**Performance Metrics**:

- **Latency**: 650ms p95 (vs 500ms target, acceptable during peak)
- **Bandwidth**: 15 TB/sec (vs 300 TB/sec without sampling)
- **Cost**: $\sim \$1$M for 4-hour event (acceptable for Super Bowl)
- **Uptime**: 99.9% (minor connection drops acceptable)

**Why NOT Full Broadcast (No Sampling):**

- Cost: $10M/hour (10x more expensive)
- Bandwidth: 300 TB/sec (exceeds data center capacity)
- User experience: System overload ‚Üí stuttering, disconnects (worse than sampling)

**When to Reconsider:**

- Bandwidth cost drops below $0.01/GB (50x cheaper)
- WebSocket server cost drops to $0.05/hour (10x cheaper)
- Network infrastructure supports 300 TB/sec sustained (unlikely)

---

**Total Line Count**: ~1,500 lines

This comprehensive design covers all aspects of building a live commenting system at extreme scale, with deep dives into
architecture, trade-offs, and real-world implementation strategies.

