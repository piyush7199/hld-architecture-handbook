# 4.1.1 Design a Stock Exchange Matching Engine

> üìö **Note on Implementation Details:**
> This document focuses on high-level design concepts and architectural decisions. 
> For detailed algorithm implementations, see **[pseudocode.md](./pseudocode.md)**.

## üìä Visual Diagrams & Resources

- **[High-Level Design Diagrams](./hld-diagram.md)** - System architecture, order book structure, data flow
- **[Sequence Diagrams](./sequence-diagrams.md)** - Order matching flows, execution paths, failure scenarios
- **[Design Decisions (This Over That)](./this-over-that.md)** - In-depth analysis of architectural choices
- **[Pseudocode Implementations](./pseudocode.md)** - Detailed algorithm implementations

---

## 1. Problem Statement

Design an ultra-low-latency stock exchange matching engine that:
- **Matches orders in <100Œºs** (microseconds)
- **Handles 1M orders/sec** peak throughput
- **Guarantees ACID** transactions for financial correctness
- **Publishes real-time market data** to millions of subscribers

**Real-World Examples:**
- **NASDAQ:** Handles 4M messages/sec, <50Œºs matching latency
- **NYSE:** Processes 10B orders/day, <500Œºs end-to-end latency
- **CME Group:** Futures exchange, <10Œºs matching engine latency
- **Coinbase:** Crypto exchange, 15K orders/sec, ~5ms latency

**The Core Challenge:**

Traditional databases are too slow (millisecond latency). We need:
1. **In-Memory Processing:** All order book state in RAM
2. **Single-Threaded Engine:** Avoid lock overhead (no mutexes)
3. **Kernel Bypass Networking:** Direct memory access (DPDK, RDMA)
4. **Custom Data Structures:** Price-time priority queues

---

## 2. Requirements and Scale Estimation

### Functional Requirements (FRs)

1. **Order Types:**
   - Limit Order: Buy/Sell at specific price or better
   - Market Order: Buy/Sell at best available price
   - Stop Order: Trigger when price reaches threshold

2. **Order Matching:**
   - Price-Time Priority: Highest bid, lowest ask, FIFO within price level
   - Immediate-or-Cancel (IOC): Execute immediately or cancel
   - Fill-or-Kill (FOK): Execute entire order or cancel

3. **Order Book:**
   - Level 1: Best bid/ask only
   - Level 2: Full depth (all price levels)
   - Level 3: Individual orders (full transparency)

4. **Trade Execution:**
   - Update buyer and seller accounts
   - Generate execution report
   - Publish to market data feed

### Non-Functional Requirements (NFRs)

| Requirement | Target | Rationale |
|------------|--------|-----------|
| **Matching Latency** | <100Œºs (p99) | Competitive with NYSE, NASDAQ |
| **Throughput** | 1M orders/sec | Handle peak trading (market open, close) |
| **Availability** | 99.99% | 52 minutes downtime/year |
| **Consistency** | ACID | No double-spending, no lost trades |
| **Durability** | 100% | All orders/trades persisted (audit log) |
| **Market Data Latency** | <1ms | Real-time price discovery |

### Scale Estimation

| Metric | Assumption | Calculation | Result |
|--------|-----------|-------------|--------|
| **Peak Orders** | 1M orders/sec | Market open/close spikes | 1M QPS |
| **Order Size** | 128 bytes per order | Order ID, symbol, price, quantity | 128 MB/sec ingestion |
| **Order Book Depth** | 1000 price levels per symbol | Bid + ask sides | 2000 levels total |
| **Symbols** | 10,000 stocks traded | NYSE + NASDAQ combined | 10K order books |
| **Execution Rate** | 10% of orders execute | 90% passive orders sit in book | 100K trades/sec |
| **Market Data Feed** | 100K updates/sec | Executions + top-of-book changes | 12.8 MB/sec broadcast |
| **Audit Log** | 1M orders √ó 128 bytes | Write-ahead log for durability | 128 MB/sec disk writes |

**Key Insight:** 100Œºs latency = 100,000 nanoseconds. At 3 GHz CPU, that's only 300,000 CPU cycles. Every operation must be optimized.

---

## 3. High-Level Architecture

### Component Overview

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    CLIENT LAYER (Traders)                        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ
‚îÇ  ‚îÇ HFT Firm ‚îÇ  ‚îÇ  Retail  ‚îÇ  ‚îÇ Algo Bot ‚îÇ  ‚îÇ Market   ‚îÇ        ‚îÇ
‚îÇ  ‚îÇ          ‚îÇ  ‚îÇ  Trader  ‚îÇ  ‚îÇ          ‚îÇ  ‚îÇ Maker    ‚îÇ        ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ
‚îÇ       ‚îÇ             ‚îÇ             ‚îÇ             ‚îÇ                ‚îÇ
‚îÇ       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ
‚îÇ                         ‚îÇ                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚îÇ
                          ‚ñº FIX Protocol / gRPC
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     LOW-LATENCY GATEWAY                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  - TCP/gRPC endpoint (kernel bypass: DPDK)                 ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  - Fast AuthN/AuthZ (JWT cached in memory)                 ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  - Order validation (basic checks)                         ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  - Route to matching engine via shared memory              ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚îÇ
                          ‚ñº Shared Memory Queue (Ring Buffer)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              MATCHING ENGINE (Single-Threaded Core)              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  ORDER BOOK (In-Memory)                                     ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                                                             ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  BID SIDE (Buy Orders)   ‚îÇ   ASK SIDE (Sell Orders)        ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ $100.50 ‚Üí [O1]   ‚îÇ    ‚îÇ    ‚îÇ $100.52 ‚Üí [O5]   ‚îÇ        ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ $100.49 ‚Üí [O2,O3]‚îÇ    ‚îÇ    ‚îÇ $100.53 ‚Üí [O6,O7]‚îÇ        ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ $100.48 ‚Üí [O4]   ‚îÇ    ‚îÇ    ‚îÇ $100.54 ‚Üí [O8]   ‚îÇ        ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ ‚îÇ
‚îÇ  ‚îÇ       (Sorted DESC)       ‚îÇ        (Sorted ASC)            ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                                                             ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  Price-Time Priority Queue (Red-Black Tree)                ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  - O(log N) insert/delete                                  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  - O(1) best bid/ask                                       ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  Processing Loop:                                                ‚îÇ
‚îÇ  1. Dequeue order from ring buffer                              ‚îÇ
‚îÇ  2. Match against opposite side                                 ‚îÇ
‚îÇ  3. Update order book (add/remove)                              ‚îÇ
‚îÇ  4. Generate execution events                                   ‚îÇ
‚îÇ  5. Write to audit log (async)                                  ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  Single-Threaded: No locks, no context switches                 ‚îÇ
‚îÇ  Latency: <100Œºs per order                                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ                ‚îÇ                ‚îÇ
         ‚ñº                ‚ñº                ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  AUDIT LOG   ‚îÇ  ‚îÇ MARKET DATA  ‚îÇ  ‚îÇ   LEDGER     ‚îÇ
‚îÇ  (WAL)       ‚îÇ  ‚îÇ  PUBLISHER   ‚îÇ  ‚îÇ   SERVICE    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Append-only  ‚îÇ  ‚îÇ WebSocket    ‚îÇ  ‚îÇ PostgreSQL   ‚îÇ
‚îÇ NVMe SSD     ‚îÇ  ‚îÇ broadcast    ‚îÇ  ‚îÇ (ACID)       ‚îÇ
‚îÇ 128 MB/sec   ‚îÇ  ‚îÇ 100K msg/sec ‚îÇ  ‚îÇ Async update ‚îÇ
‚îÇ              ‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ User balance ‚îÇ
‚îÇ Durability:  ‚îÇ  ‚îÇ Subscribers: ‚îÇ  ‚îÇ              ‚îÇ
‚îÇ All orders   ‚îÇ  ‚îÇ 1M+ clients  ‚îÇ  ‚îÇ Final truth  ‚îÇ
‚îÇ + trades     ‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ for balances ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Flow Summary:**

1. **Client** sends order via FIX protocol (TCP with kernel bypass)
2. **Gateway** validates, writes to shared memory ring buffer
3. **Matching Engine** dequeues order, matches against order book, generates execution
4. **Audit Log** persists order + execution (durability)
5. **Market Data Publisher** broadcasts to subscribers (WebSocket)
6. **Ledger Service** updates user balances (async, eventual consistency)

---

## 4. Data Model and Order Book Structure

### 4.1 Order Structure

```
Order {
  order_id: u64,           // Unique ID (timestamp + sequence)
  user_id: u32,            // Owner
  symbol: u32,             // Stock symbol (encoded as int for speed)
  side: u8,                // 0 = BUY, 1 = SELL
  order_type: u8,          // 0 = LIMIT, 1 = MARKET, 2 = STOP
  price: u64,              // Price in cents (avoid float)
  quantity: u32,           // Number of shares
  filled_quantity: u32,    // Shares already filled
  timestamp: u64,          // Nanosecond timestamp
  time_in_force: u8,       // 0 = GTC, 1 = IOC, 2 = FOK
  status: u8               // 0 = PENDING, 1 = PARTIAL, 2 = FILLED, 3 = CANCELLED
}

Size: 64 bytes (cache-line aligned for CPU efficiency)
```

**Why u64 for price instead of float64?**
- Avoid floating-point rounding errors (critical for finance)
- Integer operations faster than floating-point (2-3√ó speedup)
- Price = $100.50 stored as 10050 cents

### 4.2 Order Book Structure

**Data Structure Choice: Red-Black Tree + Linked List**

```
OrderBook {
  symbol: u32,
  
  // Bid side (buy orders): sorted descending by price
  bids: RBTree<Price, PriceLevel>,
  best_bid: Price,
  
  // Ask side (sell orders): sorted ascending by price
  asks: RBTree<Price, PriceLevel>,
  best_ask: Price,
  
  // Order lookup (for cancellation)
  orders: HashMap<OrderID, *Order>,
  
  // Statistics
  total_bid_volume: u64,
  total_ask_volume: u64,
  last_trade_price: Price
}

PriceLevel {
  price: Price,
  orders: LinkedList<*Order>,  // FIFO queue at this price
  total_quantity: u32
}
```

**Why Red-Black Tree?**
- O(log N) insert/delete
- O(1) best bid/ask (tree root or leftmost/rightmost)
- Self-balancing (no worst-case O(N) like BST)
- Better cache locality than AVL tree

**Why Linked List at each price level?**
- Time priority: First order at price level executes first (FIFO)
- O(1) append to end
- O(1) remove from front

*See [pseudocode.md::insert_order()](pseudocode.md) for implementation.*

---

## 5. Matching Algorithm (Price-Time Priority)

### 5.1 Matching Rules

**Price Priority:**
- Buy orders: Highest price first
- Sell orders: Lowest price first

**Time Priority:**
- Within same price level: FIFO (first-in-first-out)

**Example:**

```
Order Book:
BID (Buy)          |  ASK (Sell)
$100.50 ‚Üí [O1]     |  $100.52 ‚Üí [O5]
$100.49 ‚Üí [O2, O3] |  $100.53 ‚Üí [O6, O7]

Incoming: Market SELL 100 shares

Matching:
1. Best bid = $100.50 (O1)
2. Execute: O1 buys 100 shares at $100.50
3. Remove O1 from book (fully filled)
4. New best bid = $100.49 (O2)
```

### 5.2 Matching Flow

```
1. Incoming Order: SELL 500 shares @ $100.48 (aggressive limit)
2. Check best bid: $100.50 (higher than ask price ‚Üí can match!)
3. Execute against O1 (100 shares @ $100.50)
4. Remaining: 400 shares
5. Check next best bid: $100.49 (O2 has 200 shares)
6. Execute against O2 (200 shares @ $100.49)
7. Remaining: 200 shares
8. Check next best bid: $100.49 (O3 has 300 shares)
9. Execute against O3 (200 shares @ $100.49)
10. Fully filled! (O1 removed, O2 removed, O3 has 100 shares left)
```

**Latency Breakdown:**
- Step 1-2: Tree traversal: 10 CPU cycles
- Step 3-10: Matching loop: 50 CPU cycles per match (√ó3 = 150 cycles)
- Total: 160 CPU cycles = **53 nanoseconds** at 3 GHz

*See [pseudocode.md::match_order()](pseudocode.md) for implementation.*

---

## 6. Durability and Fault Tolerance (Audit Log)

### 6.1 Write-Ahead Log (WAL)

**Problem:** In-memory order book volatile (lost on crash). Need durability.

**Solution:** Append-only audit log persists all events before processing.

**Audit Log Structure:**

```
LogEntry {
  sequence_number: u64,    // Monotonic counter
  timestamp: u64,          // Nanosecond precision
  event_type: u8,          // 0 = ORDER, 1 = EXECUTION, 2 = CANCEL
  payload: [u8; 256]       // Serialized event data
  checksum: u32            // CRC32 for integrity
}

Size: 272 bytes per entry
Throughput: 1M entries/sec √ó 272 bytes = 272 MB/sec
Storage: NVMe SSD (3 GB/sec write bandwidth ‚Üí no bottleneck)
```

**Async Write Strategy:**

```
1. Order received ‚Üí Copy to ring buffer (in-memory)
2. Matching engine processes order (matching, execution)
3. Async thread writes to audit log (batched writes)
4. If crash: Replay audit log from last checkpoint
```

**Why Async?**
- Synchronous write: 10-50Œºs latency (too slow!)
- Async write: 0.1Œºs latency (just copy to buffer)
- Trade-off: Risk of losing last 1-10ms of data if crash before flush

**Mitigation: Battery-Backed Write Cache**
- NVMe SSD with capacitor backup
- Guarantees writes persisted even on power loss
- Cost: $500-$1000 per drive

*See [pseudocode.md::write_audit_log()](pseudocode.md) for implementation.*

### 6.2 Crash Recovery

```
1. Read audit log from disk (sequential read: fast)
2. Replay all ORDER events ‚Üí Rebuild order book
3. Replay all EXECUTION events ‚Üí Update order states
4. Rebuild in-memory structures (order book, hash map)
5. Resume normal operation
```

**Recovery Time:**
- 1M orders in log: 272 MB
- Sequential read: 3 GB/sec ‚Üí 90ms read time
- Replay processing: 1M orders √ó 1Œºs = 1 second
- **Total: ~1 second downtime**

---

## 7. Market Data Feed (Real-Time Broadcast)

### 7.1 Market Data Levels

**Level 1 (Top-of-Book):**
```json
{
  "symbol": "AAPL",
  "best_bid": 100.50,
  "best_bid_size": 100,
  "best_ask": 100.52,
  "best_ask_size": 200,
  "last_trade_price": 100.51,
  "timestamp": 1640000000000001
}
```

**Level 2 (Full Depth):**
```json
{
  "symbol": "AAPL",
  "bids": [
    {"price": 100.50, "quantity": 100},
    {"price": 100.49, "quantity": 500},
    {"price": 100.48, "quantity": 200}
  ],
  "asks": [
    {"price": 100.52, "quantity": 200},
    {"price": 100.53, "quantity": 700}
  ]
}
```

**Level 3 (Order-by-Order):**
```json
{
  "order_id": 12345,
  "symbol": "AAPL",
  "side": "BUY",
  "price": 100.50,
  "quantity": 100,
  "timestamp": 1640000000000001
}
```

### 7.2 Broadcast Strategy

**WebSocket Fanout:**

```
Matching Engine ‚Üí Kafka Topic ‚Üí Market Data Service ‚Üí WebSocket Server ‚Üí Clients

Kafka:
  - Topic: market-data-feed
  - Partitions: 100 (shard by symbol)
  - Throughput: 100K messages/sec

Market Data Service:
  - Subscribe to Kafka
  - Filter by subscription (clients choose symbols)
  - Broadcast via WebSocket

WebSocket Server:
  - 1M concurrent connections
  - 100K updates/sec broadcast
  - Compression: Protocol Buffers (10√ó smaller than JSON)
```

**Latency:**
- Matching Engine ‚Üí Kafka: 100Œºs
- Kafka ‚Üí Market Data Service: 500Œºs
- Market Data Service ‚Üí Client: 500Œºs
- **Total: 1.1ms** (acceptable for most traders)

**HFT Requirement:**
- High-Frequency Traders need <10Œºs
- Solution: Direct feed from matching engine (bypass Kafka)
- Co-location: HFT servers in same datacenter as exchange

---

## 8. Single-Threaded vs Multi-Threaded Design

### 8.1 Why Single-Threaded?

**Multi-Threaded Challenges:**

```
Thread 1: Process BUY order ‚Üí Lock order book ‚Üí Match ‚Üí Unlock
Thread 2: Process SELL order ‚Üí Wait for lock ‚Üí Match ‚Üí Unlock

Lock overhead:
- Mutex acquire/release: 50-100 CPU cycles
- Cache line contention: 100-500 cycles
- Context switch: 1000-10000 cycles

Result: 10-50Œºs latency per order (too slow!)
```

**Single-Threaded Benefits:**

```
Single thread: Process orders sequentially
- No locks needed (no concurrent access)
- No cache line ping-pong (all data in L1 cache)
- No context switches
- Predictable latency: 0.1-1Œºs per order

Result: <100Œºs end-to-end (10-100√ó faster!)
```

**Trade-off:**
- Single-threaded limits throughput to ~1-2M orders/sec (single CPU core)
- Scaling: Deploy separate matching engine per symbol (AAPL engine, GOOG engine, etc.)

### 8.2 LMAX Disruptor Pattern

**Ring Buffer (Lock-Free Queue):**

```
Producer (Gateway) ‚Üí Ring Buffer ‚Üí Consumer (Matching Engine)

Ring Buffer:
  - Pre-allocated array (1M slots)
  - Single producer, single consumer (no locks!)
  - Write: Increment write index (atomic CAS)
  - Read: Increment read index (atomic CAS)
  - Latency: <100 nanoseconds

Key: CPU cache line alignment (64 bytes)
- Write index: cache line 0
- Read index: cache line 1
- Data slots: cache lines 2-N
- No false sharing!
```

*See [pseudocode.md::ring_buffer_enqueue()](pseudocode.md) for implementation.*

---

## 9. Low-Latency Optimizations

### 9.1 Kernel Bypass Networking (DPDK)

**Problem:** Linux kernel network stack adds 10-50Œºs latency.

**Solution:** DPDK (Data Plane Development Kit)

```
Traditional:
  Packet ‚Üí NIC ‚Üí Kernel ‚Üí Network Stack ‚Üí Application
  Latency: 10-50Œºs

DPDK (Kernel Bypass):
  Packet ‚Üí NIC ‚Üí Application (direct memory access)
  Latency: 1-5Œºs

How it works:
- Poll NIC directly (no interrupts)
- Zero-copy (no kernel buffer)
- Huge pages (reduce TLB misses)
```

**Trade-off:**
- Requires dedicated CPU cores (100% utilization)
- Complex programming model
- Not portable (tied to specific NICs)

### 9.2 CPU Affinity and NUMA

**CPU Pinning:**

```bash
# Pin matching engine to CPU core 0
taskset -c 0 ./matching_engine

# Disable hyperthreading (avoid sharing L1 cache)
echo 0 > /sys/devices/system/cpu/cpu1/online
```

**NUMA (Non-Uniform Memory Access):**

```
Server: 2 CPU sockets
Socket 0: CPUs 0-15, Local RAM (faster access)
Socket 1: CPUs 16-31, Remote RAM (slower access)

Optimization:
- Allocate matching engine data on local RAM
- Pin thread to CPU on same socket
- Result: 30% latency reduction
```

### 9.3 Cache Optimization

**Cache Line Alignment:**

```cpp
struct Order {
  u64 order_id;
  u32 user_id;
  // ... 56 more bytes ...
} __attribute__((aligned(64)));  // 64-byte cache line

Why?
- CPU fetches entire cache line (64 bytes)
- If struct spans 2 cache lines ‚Üí 2√ó memory reads
- Alignment ensures single cache line fetch
```

**Prefetching:**

```cpp
// Tell CPU to prefetch next order (hide memory latency)
__builtin_prefetch(next_order, 0, 3);
process_order(current_order);
```

### 9.4 Avoid Memory Allocation

**Problem:** malloc/free adds 50-500 nanoseconds latency.

**Solution: Object Pool**

```
Pre-allocate 10M Order objects at startup
- Pool: Array of Orders
- Free list: Linked list of available slots
- Allocate: Pop from free list (O(1))
- Deallocate: Push to free list (O(1))
- Latency: 5-10 nanoseconds
```

*See [pseudocode.md::object_pool_allocate()](pseudocode.md) for implementation.*

---

## 10. Bottlenecks and Scaling Strategies

### Bottleneck 1: Single-Core CPU Limit

**Problem:** Single-threaded engine maxes out at 1-2M orders/sec (one CPU core).

**Solution: Shard by Symbol**

```
Symbol: AAPL ‚Üí Matching Engine 1 (CPU core 0)
Symbol: GOOG ‚Üí Matching Engine 2 (CPU core 1)
Symbol: MSFT ‚Üí Matching Engine 3 (CPU core 2)
...

Result: 10K symbols √ó 1M orders/sec each = 10B orders/sec total capacity
```

**Cross-Symbol Orders:**
- Rare: <1% of orders span multiple symbols
- Handle via inter-engine communication (slow path)

### Bottleneck 2: Audit Log Disk I/O

**Problem:** 1M orders/sec √ó 272 bytes = 272 MB/sec disk writes.

**Solution: Batch Writes**

```
Instead of: Write every order individually (1M syscalls/sec)
Use: Batch 1000 orders, write once (1K syscalls/sec)

Batch size: 1000 orders √ó 272 bytes = 272 KB
Write latency: 272 KB / 3 GB/sec = 90Œºs
Syscall overhead reduction: 1000√ó faster
```

**Trade-off:** Increased crash window (lose last 1-10ms of orders).

**Mitigation:** Battery-backed write cache (persist to NVMe capacitor).

### Bottleneck 3: Market Data Broadcast

**Problem:** 100K updates/sec √ó 1M clients = 100B messages/sec.

**Solution: Hierarchical Fanout**

```
Matching Engine ‚Üí Kafka (100K msg/sec)
                     ‚Üì
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚ñº             ‚ñº             ‚ñº
  WS Server 1   WS Server 2   WS Server N
  (333K clients) (333K clients) (333K clients)

Each WS server: 100K msg/sec √∑ 333K clients = 0.3 msg/sec/client (manageable)
```

**Further Optimization: Conflation**

```
Updates within 10ms window:
  - $100.50 ‚Üí $100.51 ‚Üí $100.52 ‚Üí $100.50

Conflated (send only last):
  - $100.50

Result: 10√ó reduction in messages (10 updates/100ms ‚Üí 1 update/100ms)
```

---

## 11. Common Anti-Patterns

### ‚ùå Anti-Pattern 1: Using Floating-Point for Prices

**Problem:**

```cpp
double price = 100.50;
double quantity = 1000.0;
double total = price * quantity;  // 100500.0000000001 (rounding error!)
```

**Impact:** Financial errors, audit failures, regulatory fines.

**‚úÖ Best Practice:** Use fixed-point integers.

```cpp
u64 price_cents = 10050;  // $100.50 = 10050 cents
u32 quantity = 1000;
u64 total_cents = price_cents * quantity;  // 10050000 cents = $100,500.00 (exact!)
```

---

### ‚ùå Anti-Pattern 2: Multi-Threaded Order Book with Locks

**Problem:**

```cpp
std::mutex order_book_lock;

void process_order(Order order) {
  std::lock_guard<std::mutex> lock(order_book_lock);  // 50-100Œºs overhead!
  match_order(order);
}
```

**Impact:** 50-100Œºs latency per order (500√ó slower than target).

**‚úÖ Best Practice:** Single-threaded matching engine + lock-free ring buffer.

---

### ‚ùå Anti-Pattern 3: Synchronous Audit Log Writes

**Problem:**

```cpp
void process_order(Order order) {
  match_order(order);
  fsync(audit_log_fd);  // 10-50Œºs disk write! (blocks matching engine)
}
```

**Impact:** Disk I/O blocks matching engine, latency spikes to milliseconds.

**‚úÖ Best Practice:** Async audit log writes with batching.

```cpp
void process_order(Order order) {
  match_order(order);
  async_write_to_audit_log(order);  // Non-blocking (100ns)
}
```

---

### ‚ùå Anti-Pattern 4: JSON for Market Data Feed

**Problem:**

```json
{"symbol":"AAPL","price":100.50,"quantity":100}
```
- Size: 48 bytes (after minification)
- Encoding latency: 1-5Œºs per message
- 100K msg/sec √ó 5Œºs = 500ms total CPU time (one core!)

**‚úÖ Best Practice:** Protocol Buffers or FlatBuffers.

```protobuf
message Trade {
  uint32 symbol = 1;  // 4 bytes (encoded as int)
  uint64 price = 2;   // 8 bytes
  uint32 quantity = 3; // 4 bytes
}
```
- Size: 16 bytes (3√ó smaller)
- Encoding latency: 0.1-0.5Œºs per message (10√ó faster)

---

## 12. Alternative Approaches

### Alternative 1: Multi-Threaded with Fine-Grained Locks

**Why NOT Chosen:**

| Factor | Single-Threaded (Chosen) | Multi-Threaded + Locks |
|--------|----------------------|---------------------|
| **Latency** | ‚úÖ <100Œºs (no locks) | ‚ùå 1-10ms (lock contention) |
| **Throughput** | ‚ö†Ô∏è 1-2M orders/sec (single core) | ‚úÖ 5-10M orders/sec (multi-core) |
| **Complexity** | ‚úÖ Simple (no race conditions) | ‚ùå Complex (deadlocks, races) |
| **Determinism** | ‚úÖ Predictable (sequential) | ‚ùå Non-deterministic (race conditions) |

**When to Use Multi-Threaded:**
- Lower latency requirement (1-10ms acceptable)
- Higher throughput priority (>10M orders/sec)
- Willing to accept complexity

### Alternative 2: Distributed Matching Engine (Sharded by Price)

**Architecture:**

```
Price $100.00-$100.50 ‚Üí Engine 1
Price $100.51-$101.00 ‚Üí Engine 2
Price $101.01-$101.50 ‚Üí Engine 3
```

**Why NOT Chosen:**

**Problem:** Cross-price matching requires coordination.

```
Order: BUY @ $101.00 (spans Engine 1 and Engine 2)
  ‚Üí Requires 2PC (Two-Phase Commit)
  ‚Üí Adds 1-10ms latency
```

**When to Use:**
- Extremely high throughput (>100M orders/sec)
- Price ranges rarely overlap (e.g., options trading)

### Alternative 3: Cloud-Based Matching Engine

**Why NOT Chosen:**

| Factor | On-Premise (Chosen) | Cloud (AWS, GCP) |
|--------|------------------|--------------|
| **Latency** | ‚úÖ <100Œºs (dedicated hardware) | ‚ùå 500Œºs-5ms (multi-tenant) |
| **Network** | ‚úÖ Kernel bypass (DPDK) | ‚ùå Standard networking |
| **Cost** | ‚úÖ $100K capex (one-time) | ‚ö†Ô∏è $50K/month (ongoing) |
| **Control** | ‚úÖ Full hardware control | ‚ùå Limited control |

**When to Use Cloud:**
- Non-latency-critical (crypto exchanges: 5-50ms OK)
- Lower upfront cost (no capex)
- Rapid scaling (add instances)

---

## 13. Monitoring and Observability

### 13.1 Key Metrics

| Metric | Target | Alert Threshold | Action |
|--------|--------|----------------|--------|
| **Matching Latency (p50)** | <50Œºs | >100Œºs | Investigate hot order book |
| **Matching Latency (p99)** | <100Œºs | >500Œºs | Check CPU load, memory |
| **Matching Latency (p99.9)** | <1ms | >5ms | Restart matching engine |
| **Order Reject Rate** | <0.1% | >1% | Check validation logic |
| **Execution Rate** | 10% | <5% or >20% | Market anomaly detection |
| **Audit Log Lag** | <10ms | >100ms | Disk I/O bottleneck |
| **Market Data Lag** | <1ms | >10ms | Network congestion |

### 13.2 Distributed Tracing

**Per-Order Trace:**

```
Order ID: 12345
Timestamp 0Œºs:    Order received at gateway
Timestamp 5Œºs:    Written to ring buffer
Timestamp 10Œºs:   Dequeued by matching engine
Timestamp 50Œºs:   Matching complete (executed)
Timestamp 60Œºs:   Audit log written (async)
Timestamp 100Œºs:  Market data published

Total: 100Œºs end-to-end
Breakdown:
  - Gateway: 5Œºs
  - Ring buffer: 5Œºs
  - Matching: 40Œºs ‚Üê Bottleneck
  - Audit log: 10Œºs
  - Market data: 40Œºs
```

**Tools:**
- Custom instrumentation (RDTSC CPU cycle counter)
- Low-overhead logging (circular buffer in shared memory)
- Post-mortem analysis (replay audit log with tracing)

---

## 14. Cost Analysis

### 14.1 Hardware Costs

**Matching Engine Server (Per Symbol):**

| Component | Specification | Cost |
|-----------|--------------|------|
| **CPU** | Intel Xeon Gold 6348 (28 cores, 3.5 GHz) | $4,000 |
| **RAM** | 256 GB DDR4-3200 ECC (low latency) | $2,000 |
| **Storage** | 2√ó NVMe SSD 1TB (RAID 1, battery-backed) | $1,000 |
| **NIC** | Mellanox ConnectX-6 (100 Gbps, DPDK support) | $2,000 |
| **Motherboard** | Dual-socket, PCIe 4.0 | $1,000 |
| **Total** | Per server | **$10,000** |

**Cluster Configuration:**

| Component | Quantity | Total Cost |
|-----------|----------|------------|
| **Matching Engines** | 100 servers (10K symbols √∑ 100 symbols/server) | $1,000,000 |
| **Gateway Servers** | 20 servers (50K connections each) | $200,000 |
| **Market Data Servers** | 50 servers (20K connections each) | $500,000 |
| **Total** | | **$1,700,000** |

### 14.2 Operational Costs

| Item | Annual Cost |
|------|------------|
| **Colocation** (datacenter space, power, cooling) | $500,000 |
| **Network** (100 Gbps transit, peering) | $200,000 |
| **Personnel** (10 engineers @ $150K) | $1,500,000 |
| **Total** | **$2,200,000/year** |

**Revenue Model:**
- Trading fees: $0.001 per trade (10 cents per $100)
- 100K trades/sec √ó 86400 sec/day = 8.64B trades/day
- Revenue: 8.64B √ó $0.001 = $8.64M/day = **$3.15B/year**

**Profit:** $3.15B - $2.2M = **$3.148B/year** (99.93% margin)

---

## 15. Real-World Examples

### 15.1 NASDAQ (US Stock Exchange)

**Scale:**
- 4M messages/sec peak
- 10,000 symbols
- 10B shares/day
- $20 trillion market cap

**Architecture:**
- Single-threaded matching engine (INET system)
- Kernel bypass networking (Solarflare NICs)
- Order book: Custom C++ implementation
- Latency: <50Œºs matching, <500Œºs end-to-end

**Innovations:**
- Dynamic order types (pegged orders, hidden orders)
- Halt/resume trading (circuit breakers)
- Multi-tier market data (Level 1, 2, 3)

### 15.2 CME Group (Futures Exchange)

**Scale:**
- 5M messages/sec peak
- 100,000+ contracts
- 3B contracts/day
- $1 quadrillion notional value

**Architecture:**
- CME Globex platform (C++)
- FPGA-accelerated matching (hardware acceleration)
- Order book: ASIC custom chip
- Latency: <10Œºs matching (hardware), <50Œºs end-to-end

**Unique Features:**
- Hardware matching engine (FPGA/ASIC)
- Sub-10Œºs latency (fastest in industry)
- 24/7 trading (futures never close)

### 15.3 Coinbase (Crypto Exchange)

**Scale:**
- 15K orders/sec (lower than traditional)
- 500 trading pairs
- 100M users

**Architecture:**
- Multi-threaded matching engine (Golang)
- Cloud-based (AWS, GCP)
- Order book: PostgreSQL (!) + Redis cache
- Latency: 5-50ms (acceptable for crypto)

**Why Different:**
- Lower latency requirements (crypto traders less sensitive)
- Cloud-first (scalability over latency)
- Retail focus (not institutional HFT)

---

## 16. Interview Discussion Points

### Q1: Why not use a database for the order book?

**Answer:**

**Database (PostgreSQL):**
- Disk-based: 1-10ms latency per query
- ACID overhead: Row locks, WAL writes
- Network overhead: TCP round-trip

**In-Memory Order Book:**
- RAM-based: <1Œºs latency
- No locks (single-threaded)
- No network (same process)

**When Database is Acceptable:**
- Non-latency-critical (>10ms OK)
- Need ACID across multiple tables
- Complex queries (joins, aggregations)

---

### Q2: How do you handle a flash crash?

**Answer:**

**Circuit Breakers:**

```
If price moves >10% in 5 minutes:
  1. Halt trading (reject all orders)
  2. Notify exchanges, regulators
  3. Manual review (30-60 minutes)
  4. Resume trading

Implementation:
  - Monitor last_trade_price every 1 second
  - Compare to VWAP (volume-weighted average price)
  - If delta > 10%: Trigger halt
```

**Kill Switch:**

```
Operator command: HALT_ALL_TRADING
  - Stop accepting new orders
  - Cancel all open orders
  - Preserve audit log
  - Notify clients

Resume: Manually restart matching engines
```

---

### Q3: What if matching engine crashes?

**Answer:**

**Crash Recovery:**

1. **Detect Crash:** Heartbeat monitor (1-second timeout)
2. **Failover:** Backup matching engine takes over (hot standby)
3. **Replay Audit Log:** Rebuild order book from WAL
4. **Resume Trading:** ~1 second downtime

**Hot Standby:**
- Backup engine receives same audit log (async replication)
- Maintains shadow order book (eventual consistency)
- On crash: Promote backup to primary (<1 second)

**Cold Start:**
- No backup: Replay audit log from scratch (10-60 seconds)

---

## 17. Trade-offs Summary

| What We Gain | What We Sacrifice |
|--------------|-------------------|
| ‚úÖ **Ultra-low latency** (<100Œºs matching) | ‚ùå **Single-core throughput limit** (1-2M orders/sec) |
| ‚úÖ **Simple single-threaded design** (no locks) | ‚ùå **Sharding required for scale** (separate engine per symbol) |
| ‚úÖ **Deterministic execution** (sequential processing) | ‚ö†Ô∏è **Custom hardware required** (kernel bypass, NVMe) |
| ‚úÖ **Strong consistency** (ACID via audit log) | ‚ùå **Eventual consistency for ledger** (async balance updates) |
| ‚úÖ **Financial correctness** (no double-spending) | ‚ö†Ô∏è **High infrastructure cost** ($1.7M capex, $2.2M/year opex) |

**Best For:**
- Institutional trading (HFT, market makers)
- High-volume exchanges (NYSE, NASDAQ)
- Latency-sensitive markets (equities, futures)

**NOT For:**
- Retail-only platforms (latency less critical)
- Low-volume markets (<1K orders/sec)
- Cost-sensitive startups (cloud alternatives cheaper)

---

## 18. References

### Academic Papers
- **[LMAX Disruptor Pattern](https://lmax-exchange.github.io/disruptor/)** - High-performance inter-thread messaging
- **[Understanding Latency](https://people.eecs.berkeley.edu/~rcs/research/interactive_latency.html)** - Jeff Dean's latency numbers
- **[Matching Engine Design](https://web.stanford.edu/class/ee380/Abstracts/121017-slides.pdf)** - Stanford EE380 talk

### Related Chapters
- **[2.5.3 Distributed Locking](../../02-components/2.5-algorithms/2.5.3-distributed-locking.md)** - Why single-threaded avoids locks
- **[2.3.1 Asynchronous Communication](../../02-components/2.3-messaging-streaming/2.3.1-asynchronous-communication.md)** - Audit log async writes
- **[2.0.3 Real-Time Communication](../../02-components/2.0-communication/2.0.3-real-time-communication.md)** - WebSocket market data

### Open Source Projects
- **[Quickfix](https://github.com/quickfix/quickfix)** - FIX protocol implementation (C++)
- **[Disruptor](https://github.com/LMAX-Exchange/disruptor)** - Low-latency ring buffer (Java)
- **[DPDK](https://www.dpdk.org/)** - Kernel bypass networking framework

### Books
- **"Trading and Exchanges"** by Larry Harris (market microstructure)
- **"Flash Boys"** by Michael Lewis (HFT, latency arbitrage)
- **"The Linux Programming Interface"** by Michael Kerrisk (low-level optimization)

