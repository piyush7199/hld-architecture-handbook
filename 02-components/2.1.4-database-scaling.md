# 2.1.4 Database Scaling: Partitioning, Replication, Federation, and Sharding

## Intuitive Explanation

When your single database can no longer handle the load (either reads or writes), you must scale it. Partitioning is the
general term for splitting a large database into smaller pieces to improve manageability and performance.

- **Replication (The Copy Machine):** Makes copies of your data on separate servers. Great for handling more read
  traffic.
- **Horizontal Partitioning (Sharding):** Splits your data (rows) across multiple independent databases. Essential for
  handling high write traffic and massive data volume.
- **Vertical Partitioning:** Splits your schema (columns or tables) within a single database or server, dividing
  resources by function or field.
- **Federation (The Service Split):** Splits your schema (tables) into specialized, independent databases based on the
  service boundary.

---

## In-Depth Analysis

### 1. Replication (Handling Read Load)

Replication involves copying data from a primary database **(Master/Leader)** to one or more secondary databases **(
Slaves/Followers)**.

- **Master/Leader:** Handles all write operations ($\text{CUD}$ in $\text{CRUD}$).
- **Slave/Follower:** Handles read operations ($\text{R}$ in $\text{CRUD}$).
- **Read-Replicas:** By directing most reads to followers, you offload the master, enabling read scaling.
- **Trade-off:** **Consistency Lag:** Replication is often **Asynchronous** for performance. This means followers may
  temporarily lag behind the master, leading to **Eventual Consistency** (a user might read stale data immediately after
  writing it).

### 2. Horizontal Partitioning (Sharding)

Horizontal partitioning, or Sharding, distributes the write load and the total dataset size across multiple machines by
splitting the rows.

- **Why Sharding?** Sharding distributes the write load and the total dataset size across multiple machines. If you
  have $100$ million users, each of $10$ shards holds $10$ million users.
- **Sharding Key (The Router):** The column used to determine which shard a row belongs to (e.g., `user_id`,
  `geolocation`). **Choosing the right shard key is the most critical decision in sharding.**

| Sharding Strategy       | Mechanism                                                                                               | Pros                                                                       | Cons                                                                                          |
|-------------------------|---------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------|
| Key-Based (Hash/Modulo) | Row goes to shard $N$ where $N = \text{Hash}(\text{Key}) \bmod (\text{Number of Shards})$.              | Simple, generally even distribution of data.                               | Cannot easily add or remove shards without massive data migration.                            |
| Range-Based             | Data is partitioned by key range (e.g., Shard $1$ gets $\text{User}$ $\text{IDs}$ $1$ to $10\text{M}$). | Easier to add shards for future growth.                                    | Can lead to hotspots (all new users go to the latest shard, Shard $N$)                        |
| Directory-Based         | Uses a separate Lookup Service to map keys to shards.                                                   | Highly flexible; allows dynamic re-sharding without changing client logic. | The lookup service becomes a Single Point of Failure ($\text{SPOF}$) if not highly available. |

### 3. Vertical Partitioning (Splitting by Schema/Column)

Vertical partitioning involves splitting a table either by **columns** or by **functionally distinct tables** onto
separate disks or servers.

- **Splitting by Columns (Row Splitting):** If a table has $50$ columns, but $5$ of them are accessed $99\%$ of the
  time, you split the table into two physically separate storage units.
    - **Example:** Moving large, rarely accessed fields (like `user_bio_text` or `order_history_details`) to a separate
      storage volume or disk.
    - **Pros:** Improves cache hits and I/O performance for the frequently accessed columns.
    - **Cons:** Requires a join operation (on the Primary Key) to reconstruct the full row.
- **Splitting by Function (Table Splitting):** Moving logically separate tables (but ones that often run large queries
  against each other) to separate database instances to relieve resource contention on the original server.

### 4. Federation (Splitting by Service/Schema)

Federation is a specialized form of vertical partitioning that decomposes the entire database by function or service
boundary.

- **Example:** Splitting a monolithic database into dedicated $\text{User}$ $\text{DB}$, $\text{Product}$ $\text{DB}$,
  and $\text{Order}$ $\text{DB}$.
- **Pros:** Allows each team to select the best database technology for their needs (e.g., $\text{NoSQL}$
  for $\text{Product}$ $\text{Catalog}$, $\text{RDBMS}$ for $\text{Order}$ $\text{Ledger}$).
- **Cons:** Makes queries that span services (cross-DB joins) complex, requiring the Application Layer to execute the
  join logic.

---

## ‚úèÔ∏è Design Challenge

### Problem

You are designing the back-end for a globally deployed blogging platform. You have $1$ billion users and expect $80\%$
of your traffic to be reads. Explain how you would combine Replication and Sharding to handle this load. Which
architectural choice would you make to mitigate the Write Contention bottleneck?

### Solution

#### üß© Scenario Summary

- Platform: Global blogging platform (similar to Medium or Substack)
- Scale: 1 billion users
- Traffic: 80% reads, 20% writes
- Goal:
    - Scale horizontally for both read and write workloads
    - Minimize write contention
    - Serve low-latency reads globally

#### ‚úÖ Step 1: Combine Sharding + Replication for Scale

##### üß± Sharding ‚Äî Distribute Write Load

- **Shard Key:** `user_id` or `author_id`. Each author‚Äôs data (posts, drafts, comments) goes to one shard.
- **Sharding Strategy:**
    - Use **consistent hashing or range-based** partitioning.
    - Each shard = one logical database cluster.
    - Metadata service keeps track of shard placements and routing.
- **Effect:**
    - Writes are isolated per shard ‚Üí avoids global locking and scales horizontally.
    - Each shard can scale independently (both storage and compute).

##### üåç Replication ‚Äî Distribute Read Load

- **Replication Setup:**
    - Each shard has 1 Primary (leader) and multiple Read Replicas.
    - Replicas are deployed in different regions for geo-local low-latency reads.

- **Traffic Flow:**
    - **Writes** ‚Üí Primary (single writer per shard).
    - **Reads** ‚Üí Replicas (via regional load balancers).
- **Replication Type:**
    - **Asynchronous replication** for low latency.
    - **Synchronous replication** only within region if strong consistency needed.

| Purpose          | Technique   | Benefit                                              |
|------------------|-------------|------------------------------------------------------|
| **Scale writes** | Sharding    | Parallel writes across multiple primaries            |
| **Scale reads**  | Replication | Low-latency regional reads & horizontal read scaling |

#### ‚úÖ Step 2: Mitigate the Write Contention Bottleneck

##### üß† Problem

Even with sharding, some primaries can become hotspots ‚Äî
for example, when a popular author or trending topic causes many concurrent writes to one shard.

**üí° Solution: CQRS + Append-Only Write Log (Event Sourcing)**

Use **CQRS (Command Query Responsibility Segregation)** with an append-only log per shard.

- **Command (Write) Path:**
    - Writes are appended as immutable events (e.g., ‚ÄúPostCreated‚Äù, ‚ÄúCommentAdded‚Äù) into a partitioned log (
      Kafka-like).
    - Append is O(1) ‚Äî no contention or locking on hot rows.
    - Consumers read the log and update database + materialized read models asynchronously.

- **Query (Read) Path:**
    - Reads hit read-optimized replicas or denormalized stores (Cassandra, Redis, ElasticSearch).
    - These are updated asynchronously from the write log.

Result:

- Writes don‚Äôt block each other ‚Üí **no contention.**
- Reads are isolated and extremely fast via replicas/caches.

| Component                      | Purpose                                        |
|--------------------------------|------------------------------------------------|
| **Kafka / Pulsar**             | Append-only write log (per shard)              |
| **Primary DB**                 | Durable write store (MySQL/Postgres/Cassandra) |
| **Read Replicas**              | Serve 80% read traffic                         |
| **Redis / CDN / Search index** | Fast caching and discovery                     |
| **Consumers**                  | Update read models from event streams          |

##### ‚úÖ Step 3: Handle Hotspots & Scale Dynamically

- **Hot Shard Splitting:** Split large shards based on user ranges or content IDs.
- **Sub-sharding:** Partition popular posts (e.g., comments by post_id + time_window).
- **CRDT Counters:** For distributed likes/views counters (avoid central write locks).
- **Write Batching:** Aggregate frequent writes in background to reduce contention.

##### ‚úÖ Step 4: Read Architecture Optimization

- Geo-local Replicas: Deploy near major user regions for fast reads.
- Edge Caching: CDN or Redis layer caches popular blog pages.
- Materialized Views: Precompute timelines or trending feeds asynchronously.

##### ‚öôÔ∏è Step 5: Consistency & Trade-offs

| Aspect           | Design Choice                                | Reason                              |
|------------------|----------------------------------------------|-------------------------------------|
| **Consistency**  | Eventual consistency for most reads          | Faster, scalable global performance |
| **Strong reads** | Route to primary or read-after-write replica | Needed only for recent post edits   |
| **Durability**   | Append-only log + replication                | No data loss, high availability     |
| **Availability** | Per-shard failover to replica                | Minimal global impact               |
| **Scalability**  | Add more shards or replicas independently    | Linear horizontal scaling           |

##### ‚ö†Ô∏è Trade-offs / Gotchas

- Eventual Consistency: Users may briefly see stale data.
- Complexity: Log + consumer model requires robust monitoring.
- Hotspot authors: Require dynamic resharding or caching strategies.

##### ‚úÖ Final Summary

| Aspect                        | Design Decision                            | Reason                             |
|-------------------------------|--------------------------------------------|------------------------------------|
| **Sharding**                  | By `user_id` or `author_id`                | Distribute writes horizontally     |
| **Replication**               | Async replicas per shard                   | Scale reads globally               |
| **Write Contention Solution** | CQRS + Append-only per-shard event log     | Lock-free, scalable writes         |
| **Read Path**                 | Read replicas + materialized views + cache | Serve 80% reads efficiently        |
| **Hotspot Handling**          | Sub-sharding, CRDTs, batching              | Avoid overload on popular entities |
| **Trade-off**                 | Eventual consistency                       | Acceptable for global performance  |
