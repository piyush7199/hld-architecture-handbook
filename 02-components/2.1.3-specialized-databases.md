# 2.1.3 Specialized Databases: Time-Series, Graph, and Geospatial

## Intuitive Explanation

While $\text{RDBMS}$ and standard $\text{NoSQL}$ databases cover most applications, certain types of data and queries
are fundamentally inefficient in general-purpose databases. Specialized $\text{DBs}$ are designed with a specific access
pattern in mind, leading to massive performance gains for those tasks.

- **Time-Series DB (The Logging Specialist):** Optimized for recording and querying sequences of data points over time (
  e.g., server metrics, stock prices).
- **Graph DB (The Relationship Mapper):** Optimized for storing relationships between entities (e.g., social
  connections, recommendation networks).
- **Geospatial DB (The Map Tracker):** Optimized for queries involving location, distance, and proximity (e.g., "Find
  all restaurants within 5 km").

---

## In-Depth Analysis

### 1. Time-Series Databases (TSDB)

$\text{TSDBs}$ are highly optimized for high-volume writes and range queries (i.e., "Give me the average CPU usage
between 2 PM and 4 PM yesterday").

- **Internal Design:** Data is typically compressed and stored sequentially on disk by time, often using an $\text{LSM}$
  -Tree structure (see 2.1.5) and indexing on both time and the metric key. This structure makes time-range queries
  extremely fast compared to general-purpose databases.
- **Write-Heavy Focus:** Designed to ingest millions of data points per second. Writes are appended, rarely updated.
- **Use Cases:** $\text{IoT}$ sensor data, infrastructure monitoring (Prometheus, Grafana data), stock market data
  analysis.
- **Examples:** InfluxDB, TimescaleDB, Prometheus.

### 2. Graph Databases

Graph databases use nodes, edges, and properties to represent and store data. They excel at traversing relationships,
where a traditional $\text{RDBMS}$ would require complex, slow multi-table joins.

- **Components:**
    - **Node:** An entity (e.g., a User, a Product).
    - **Edge (Relationship):** A connection between two nodes (e.g., "FOLLOWS," "BUYS"). Edges can have properties (
      e.g., the date a user started following another).
- **Use Cases:** Social networks ($\text{Friend}$ $\text{of}$ $\text{a}$ $\text{Friend}$ queries), recommendation
  engines, fraud detection (tracing complex transaction links).
- **Examples:** Neo4j, AWS Neptune.

### 3. Geospatial Databases

These systems, often implemented as extensions to $\text{PostgreSQL}$ ($\text{PostGIS}$) or $\text{MongoDB}$, use
specialized indexing structures to perform spatial queries efficiently.

- **Indexing:** Uses structures like **Geohash** or **Quadtrees** to translate $2\text{D}$ coordinates into a single,
  ordered string or tree structure, allowing proximity queries to be resolved quickly.
- **Query Types:** Point-in-Polygon, distance calculations (e.g., finding all drivers within a radius of a customer).
- **Use Cases:** Ride-sharing (Uber, Lyft), location-based services (Yelp, Foursquare).

### Key Concepts / Tradeoffs

| DB Type     | Rationale (Why Specialized?)                                                                                                 | Trade-off / Limitation                                                                                                                  |
|-------------|------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------|
| Time-Series | Querying across a time range is $\text{O}(1)$ or $\text{O}(\log N)$, compared to $\text{O}(N)$ in a standard $\text{RDBMS}$. | Extremely poor performance for non-time-based queries (e.g., finding the latest record based on a non-indexed attribute).               |
| Graph       | The performance of traversing relationships is constant, regardless of the size of the total dataset.                        | Poor performance for non-relational, bulk data storage and retrieval. Requires a complex schema design focused purely on relationships. |
| Geospatial  | Enables complex mathematical calculations (spherical geometry) at scale.                                                     | $\text{Geohash}$ indexing introduces complexities and potential inaccuracies near region boundaries (the "bounding box" problem).       |

### 4. Comprehensive Specialized Database Comparison

| Feature | Time-Series DB | Graph DB | Geospatial DB | Search Engine | Vector DB |
|---------|---------------|----------|---------------|---------------|-----------|
| **Data Model** | Time-stamped metrics | Nodes + Edges | Points/Polygons | Documents + Indexes | Embeddings/Vectors |
| **Primary Access Pattern** | Time-range queries | Relationship traversal | Proximity/distance | Full-text search | Similarity search |
| **Write Performance** | ⭐⭐⭐⭐⭐ (append-only) | ⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐ |
| **Read Performance** | ⭐⭐⭐⭐ (time-range) | ⭐⭐⭐⭐⭐ (traversal) | ⭐⭐⭐⭐ (proximity) | ⭐⭐⭐⭐⭐ (keyword) | ⭐⭐⭐⭐ (kNN) |
| **Aggregations** | Excellent | Limited | Limited | Good | Limited |
| **Scalability** | Excellent (horizontal) | Good | Good | Excellent | Good |
| **Best Use Cases** | Metrics, IoT, Logs | Social, Fraud, Recommendations | Maps, Delivery, Location | Search, Analytics | ML, Recommendations, Semantic Search |
| **Examples** | InfluxDB, TimescaleDB, Prometheus | Neo4j, Neptune, JanusGraph | PostGIS, MongoDB Geo | Elasticsearch, Solr | Pinecone, Weaviate, Milvus |
| **Query Language** | SQL-like, Flux | Cypher, Gremlin | SQL + Spatial | DSL, JSON | Vector similarity APIs |
| **When NOT to Use** | Non-time-series data | Few relationships | No location data | Simple CRUD | No ML/embeddings |

### Detailed Use Case Matrix

| Use Case | Best Database Type | Why | Example Implementation |
|----------|-------------------|-----|----------------------|
| **Server Monitoring** | Time-Series | High-volume metrics, time-range aggregations | Prometheus + Grafana |
| **Social Network Feed** | Graph | Friend-of-friend queries, recommendation | Neo4j with user-follows-user relationships |
| **Ride-Sharing** | Geospatial | Distance calculations, driver matching | PostGIS for driver location queries |
| **Product Search** | Search Engine | Full-text search, filters, facets | Elasticsearch with product catalog |
| **Fraud Detection** | Graph | Transaction pattern analysis | Neo4j tracing payment chains |
| **Stock Market Data** | Time-Series | Historical prices, real-time ticks | InfluxDB with tick data |
| **Recommendation Engine** | Graph + Vector | User preferences, item similarity | Neo4j + Pinecone hybrid |
| **Real Estate Search** | Geospatial + Search | Location + amenities + keywords | MongoDB Geo + Elasticsearch |
| **IoT Sensor Data** | Time-Series | Millions of data points per second | TimescaleDB hypertables |
| **Semantic Document Search** | Vector | Natural language understanding | Weaviate with text embeddings |
| **Supply Chain Tracking** | Graph | Multi-hop relationships | Neptune with vendor-product-warehouse |
| **Weather Forecasting** | Time-Series | Historical weather patterns | InfluxDB with sensor aggregations |

---

## ⚠️ Common Specialized Database Anti-Patterns

### Anti-Pattern 1: Using RDBMS for Time-Series Data

**Problem:**
```sql
-- Storing metrics in traditional RDBMS
CREATE TABLE metrics (
    id BIGSERIAL PRIMARY KEY,
    metric_name VARCHAR(100),
    value DOUBLE PRECISION,
    timestamp TIMESTAMP,
    server_id INT
);

-- Query for last hour's CPU usage
SELECT AVG(value) 
FROM metrics 
WHERE metric_name = 'cpu_usage' 
  AND timestamp > NOW() - INTERVAL '1 hour'
GROUP BY DATE_TRUNC('minute', timestamp);

-- Slow! Full table scan even with timestamp index
-- Table has billions of rows
```

**Why It's Wrong:**
- **Poor compression**: RDBMS doesn't optimize for time-series patterns
- **Slow aggregations**: No built-in downsampling or rollups
- **Storage explosion**: Stores every data point without compression
- **Query performance**: Indexes don't help much for range scans

**Better Approach:**
```sql
-- Use TimescaleDB (PostgreSQL extension) or InfluxDB
-- TimescaleDB example:
CREATE TABLE metrics (
    time TIMESTAMPTZ NOT NULL,
    metric_name TEXT,
    value DOUBLE PRECISION,
    server_id INT
);

-- Convert to hypertable (automatic partitioning by time)
SELECT create_hypertable('metrics', 'time');

-- Automatic compression, continuous aggregates
-- 10-100x better performance for time-range queries
-- 95% storage reduction with compression
```

---

### Anti-Pattern 2: Modeling Graph Data in RDBMS

**Problem:**
```sql
-- Social network in RDBMS
CREATE TABLE users (id INT, name VARCHAR);
CREATE TABLE friendships (user_id INT, friend_id INT);

-- Find friends-of-friends (2 hops)
SELECT DISTINCT f2.friend_id
FROM friendships f1
JOIN friendships f2 ON f1.friend_id = f2.user_id
WHERE f1.user_id = 123;

-- Find friends-of-friends-of-friends (3 hops)
-- Becomes exponentially complex with more JOINs!
SELECT DISTINCT f3.friend_id
FROM friendships f1
JOIN friendships f2 ON f1.friend_id = f2.user_id
JOIN friendships f3 ON f2.friend_id = f3.user_id
WHERE f1.user_id = 123;
-- Extremely slow, cartesian explosion
```

**Why It's Wrong:**
- **JOIN explosion**: N hops = N JOINs (complexity grows exponentially)
- **Poor performance**: RDBMS not optimized for graph traversal
- **Hard to maintain**: Complex SQL for multi-hop queries
- **Limited expressiveness**: Can't easily express graph algorithms

**Better Approach:**
```cypher
// Neo4j Cypher query
// Find friends-of-friends
MATCH (user:User {id: 123})-[:FRIENDS_WITH*2]-(fof:User)
RETURN DISTINCT fof

// Find friends within 3 hops
MATCH (user:User {id: 123})-[:FRIENDS_WITH*1..3]-(friend:User)
RETURN DISTINCT friend

// Constant-time traversal regardless of hops!
// Graph databases are optimized for this
```

---

### Anti-Pattern 3: Storing Coordinates as Separate Lat/Lng Columns

**Problem:**
```sql
CREATE TABLE restaurants (
    id INT PRIMARY KEY,
    name VARCHAR(255),
    latitude DECIMAL(10, 8),
    longitude DECIMAL(11, 8)
);

-- Find restaurants within 5km (naive approach)
SELECT * FROM restaurants
WHERE 
    latitude BETWEEN (user_lat - 0.045) AND (user_lat + 0.045)
    AND longitude BETWEEN (user_lng - 0.045) AND (user_lng + 0.045);
-- Returns square bounding box, not circular radius!
-- No proper distance calculation
-- Can't use spatial indexes
```

**Why It's Wrong:**
- **Bounding box inaccuracy**: Square approximation, not true distance
- **No spatial indexes**: Can't use R-tree or geohash
- **Slow queries**: Full table scan for every proximity search
- **Incorrect near poles**: Longitude degrees vary by latitude

**Better Approach:**
```sql
-- PostGIS (PostgreSQL)
CREATE TABLE restaurants (
    id INT PRIMARY KEY,
    name VARCHAR(255),
    location GEOMETRY(POINT, 4326)  -- WGS84 coordinates
);

-- Create spatial index
CREATE INDEX idx_restaurants_location 
ON restaurants USING GIST(location);

-- Find restaurants within 5km (true distance)
SELECT * FROM restaurants
WHERE ST_DWithin(
    location::geography,
    ST_SetSRID(ST_MakePoint(:user_lng, :user_lat), 4326)::geography,
    5000  -- meters
);
-- Fast! Uses spatial index
-- Accurate spherical distance calculation
```

---

### Anti-Pattern 4: Using Full-Text Search Without Proper Indexing

**Problem:**
```sql
-- Searching with LIKE (slow!)
SELECT * FROM products 
WHERE name LIKE '%phone%' 
   OR description LIKE '%phone%';
-- Full table scan, no index can help with leading wildcard
-- Case-sensitive, no stemming, no relevance ranking
```

**Why It's Wrong:**
- **No index support**: LIKE with leading wildcard can't use indexes
- **No relevance ranking**: Can't sort by match quality
- **No language processing**: Misses plurals, synonyms
- **Poor performance**: Full table scan on every search

**Better Approach:**
```sql
-- PostgreSQL full-text search
CREATE INDEX idx_products_search 
ON products USING GIN(to_tsvector('english', name || ' ' || description));

SELECT * FROM products
WHERE to_tsvector('english', name || ' ' || description) @@ to_tsquery('english', 'phone')
ORDER BY ts_rank(to_tsvector('english', name || ' ' || description), to_tsquery('english', 'phone')) DESC;

-- Or use Elasticsearch (better for complex search)
{
  "query": {
    "multi_match": {
      "query": "phone",
      "fields": ["name^2", "description"],  // Boost name matches
      "fuzziness": "AUTO"  // Handle typos
    }
  }
}
```

---

### Anti-Pattern 5: Not Using Compression for Time-Series Data

**Problem:**
```sql
-- Storing uncompressed metrics
-- Each row: 8 bytes (timestamp) + 8 bytes (value) + overhead = ~50 bytes

-- 1 metric per second for 1 year
-- 31,536,000 rows × 50 bytes = 1.5 GB per metric
-- 1,000 metrics = 1.5 TB per year!
```

**Why It's Wrong:**
- **Storage explosion**: Uncompressed time-series grows rapidly
- **Slow queries**: More data to scan
- **Higher costs**: More storage, more memory

**Better Approach:**
```sql
-- InfluxDB / TimescaleDB compression
-- TimescaleDB example:
ALTER TABLE metrics SET (
    timescaledb.compress,
    timescaledb.compress_segmentby = 'metric_name, server_id',
    timescaledb.compress_orderby = 'time DESC'
);

-- Enable automatic compression policy
SELECT add_compression_policy('metrics', INTERVAL '7 days');

-- Typical compression ratio: 10-100x
-- 1.5 TB → 15-150 GB
-- Queries still fast (transparent decompression)
```

---

### Anti-Pattern 6: Over-Indexing Geospatial Data

**Problem:**
```sql
CREATE TABLE locations (
    id INT PRIMARY KEY,
    name VARCHAR(255),
    location GEOMETRY(POINT, 4326)
);

-- Creating too many spatial indexes
CREATE INDEX idx1 ON locations USING GIST(location);
CREATE INDEX idx2 ON locations USING GIST(ST_Buffer(location, 100));
CREATE INDEX idx3 ON locations USING GIST(ST_Buffer(location, 1000));
CREATE INDEX idx4 ON locations USING GIST(ST_Buffer(location, 10000));
-- Slows down inserts, wastes storage
```

**Why It's Wrong:**
- **Write penalty**: Every insert updates all indexes
- **Storage waste**: Spatial indexes are large
- **Diminishing returns**: One good index is usually enough

**Better Approach:**
```sql
-- Single well-designed spatial index
CREATE INDEX idx_locations_geom 
ON locations USING GIST(location);

-- Use query planner, not pre-buffered indexes
SELECT * FROM locations
WHERE ST_DWithin(location, :point, :radius);
-- Query planner uses idx_locations_geom efficiently
```

---

### Anti-Pattern 7: Not Setting Retention Policies for Time-Series

**Problem:**
```python
# Writing metrics forever
influxdb.write_points([
    {"measurement": "cpu", "fields": {"value": 75.0}, "time": now}
])

# After 5 years: billions of old data points
# Most queries only need recent data
# Storage and query costs explode
```

**Why It's Wrong:**
- **Unbounded growth**: Storage grows forever
- **Query slowdown**: More data to scan
- **Cost explosion**: Storage costs compound

**Better Approach:**
```python
# InfluxDB retention policy
CREATE RETENTION POLICY "30_days" ON "metrics" 
DURATION 30d 
REPLICATION 1 
DEFAULT

# Or continuous aggregates for historical data
# Keep raw data: 30 days
# Keep 1-minute rollups: 1 year
# Keep hourly rollups: forever

# TimescaleDB example:
SELECT add_retention_policy('metrics', INTERVAL '30 days');

# Create continuous aggregate for older data
CREATE MATERIALIZED VIEW metrics_hourly
WITH (timescaledb.continuous) AS
SELECT time_bucket('1 hour', time) AS hour,
       metric_name,
       AVG(value) as avg_value
FROM metrics
GROUP BY hour, metric_name;
```

---

### Anti-Pattern 8: Using Graph DB for Simple Hierarchies

**Problem:**
```cypher
// Using Neo4j for simple org chart
CREATE (ceo:Employee {name: "Alice", role: "CEO"})
CREATE (cto:Employee {name: "Bob", role: "CTO"})
CREATE (dev:Employee {name: "Charlie", role: "Developer"})
CREATE (ceo)-[:MANAGES]->(cto)
CREATE (cto)-[:MANAGES]->(dev)

// Query reporting chain
MATCH path = (e:Employee {name: "Charlie"})-[:MANAGES*]->(manager)
RETURN manager
```

**Why It's Wrong:**
- **Overkill**: Simple tree structures don't need graph DB
- **Operational complexity**: Another database to maintain
- **Cost**: Graph DBs are expensive to run

**Better Approach:**
```sql
-- PostgreSQL with recursive CTE (Common Table Expression)
CREATE TABLE employees (
    id INT PRIMARY KEY,
    name VARCHAR(255),
    role VARCHAR(100),
    manager_id INT REFERENCES employees(id)
);

-- Query reporting chain (recursive)
WITH RECURSIVE reporting_chain AS (
    SELECT id, name, role, manager_id, 1 as level
    FROM employees
    WHERE id = 3  -- Charlie
    
    UNION ALL
    
    SELECT e.id, e.name, e.role, e.manager_id, rc.level + 1
    FROM employees e
    JOIN reporting_chain rc ON e.id = rc.manager_id
)
SELECT * FROM reporting_chain;

-- Simple, fast, no additional database needed
```

**When Graph DB IS Worth It:**
- Many-to-many relationships (social networks)
- Complex graph algorithms (PageRank, shortest path)
- Variable-depth traversals (unknown number of hops)

---

## 💡 Specialized Database Best Practices

| Practice | Description | Benefit |
|----------|-------------|---------|
| **Match DB to access pattern** | Use time-series for metrics, graph for relationships | Optimal performance for your workload |
| **Set retention policies** | Auto-delete old time-series data | Control storage costs |
| **Use compression** | Enable compression for time-series | 10-100x storage savings |
| **Proper spatial indexing** | Single GIST index for geospatial | Fast proximity queries |
| **Aggregate old data** | Rollups for historical time-series | Fast queries, less storage |
| **Monitor query patterns** | Identify slow queries | Optimize indexes and data model |
| **Test at scale** | Benchmark with production data volumes | Avoid surprises |
| **Consider hybrid approaches** | Multiple DBs for different patterns | Best of both worlds |

---

## ✏️ Design Challenge

### Problem

You are building a social feed for a hyper-local app (like Nextdoor) that must prioritize showing users events and posts
from their exact neighborhood. How would you model the boundaries of neighborhoods and efficiently query for "all posts
within a $1 \text{km}$ radius" using a Geospatial approach? What index type would you use?

### Solution

#### 🧩 Scenario Summary

- **App:** Hyper-local social platform (like Nextdoor)
- **Goal:** Show posts/events near a user’s location (e.g., within 1 km radius)
- **Challenge:** Efficiently query millions of posts filtered by proximity

#### ✅ Step 1: Model Neighborhood Boundaries

Neighborhoods can be represented in one of two ways depending on your precision needs:

##### Option A — Polygon Boundaries

- Store each neighborhood as a polygon (GeoJSON Polygon) defining its real geographic area.
- Each post has a latitude and longitude.
- To find which neighborhood a post belongs to:
  ````sql 
  SELECT neighborhood_id
  FROM neighborhoods
  WHERE ST_Contains(geometry, ST_Point(:lat, :lon));
  ````
- This uses geometric containment — useful if neighborhoods are irregularly shaped.

##### Option B — Radius Approximation (for simplicity)

- Represent each neighborhood by its center point and a radius (e.g., 1 km).
- Then use proximity queries (distance-based) rather than polygon containment.

#### ✅ Step 2: Store Posts with Geo Coordinates

Each post should have:

```json
{
  "post_id": "123",
  "user_id": "u456",
  "text": "Community cleanup event",
  "location": {
    "type": "Point",
    "coordinates": [
      "longitude",
      "latitude"
    ]
  },
  "timestamp": "2025-10-12T10:00:00Z"
}
```

This format follows **GeoJSON**, supported by most document stores and spatial databases.

#### ✅ Step 3: Query — “All posts within 1 km radius”

Use a **geospatial** “`near`” query, for example:

In PostGIS (PostgreSQL extension)

```sql
SELECT *
FROM posts
WHERE ST_DWithin(
  location::geography,
  ST_MakePoint(:lon, :lat)::geography,
  1000
);
```

Both return all posts within a 1 km radius of the given coordinate efficiently.

#### ✅ Step 4: Index Type — Use a Geospatial Index

##### Choice: 🗺️ Geohash / R-Tree / 2dsphere index

- MongoDB → `2dsphere` index (supports spherical coordinates & distance queries)
- PostgreSQL (PostGIS) → `GiST` index (R-Tree–based for geometry)
- Elasticsearch / DynamoDB → Geohash–based index for geo-queries

| Database      | Index Type                   | Use Case                             |
|---------------|------------------------------|--------------------------------------|
| MongoDB       | **2dsphere index**           | Queries by radius or polygon         |
| PostGIS       | **GiST (R-Tree)**            | Precise geospatial joins and filters |
| Elasticsearch | **Geohash / GeoPoint index** | Text + location search combined      |

These indexes enable logarithmic lookup for spatial regions instead of full scans.

#### ✅ Step 5: Why This Works

- **Spatial locality:** Points close together are grouped in index cells (geohash buckets or R-tree nodes).
- **Efficient radius queries:** The engine filters by nearby bounding boxes first, then refines by exact distance.
- **Scalable:** Works across millions of posts with real-time query speed.

#### ⚠️ Trade-offs / Gotchas

- **Boundary precision:** Geohash bucketing can cause edge cases near borders (posts slightly outside might be
  included/excluded).
- **Updates:** Re-indexing is needed when post location changes (rare).
- **Storage overhead:** Spatial indexes are larger than normal B-trees.

#### ✅ Final Summary

| Aspect                     | Design Decision                                                        | Reason                             |
|----------------------------|------------------------------------------------------------------------|------------------------------------|
| **Neighborhood model**     | Polygon or center + radius                                             | Represents local boundaries        |
| **Post location storage**  | GeoJSON `{ type: Point, coordinates: [lon, lat] }`                     | Standard geospatial format         |
| **Query for posts nearby** | `ST_DWithin` / `$nearSphere` within 1 km                               | Fast proximity filter              |
| **Index type**             | **2dsphere / R-Tree (GiST)**                                           | Enables efficient spatial lookup   |
| **Trade-off**              | Minor precision loss at boundaries (eventual consistency with geohash) | Acceptable for social feed latency |
