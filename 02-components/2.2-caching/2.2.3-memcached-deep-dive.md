# 2.2.3 Memcached Deep Dive: The Simple, Fast Cache

## Intuitive Explanation

Memcached is a **distributed in-memory key-value cache** designed for **extreme simplicity** and **speed**. Unlike Redis, which is a feature-rich data structure store, Memcached does one thing exceptionally well: **store and retrieve strings blazingly fast**. It's like a giant hash table distributed across multiple servers, with no persistence, no complex data structures, and no replication — just pure caching performance.

- **In-Memory:** All data in RAM (no disk persistence)
- **Distributed:** Spreads cache across multiple servers (client-side sharding)
- **Simple:** Only strings (no lists, sets, or sorted sets like Redis)
- **Use Cases:** Web page caching, session storage, database query result caching

**When to Use:** When you need the **absolute fastest** cache and don't need Redis's advanced features (persistence, data structures, pub/sub).

---

## In-Depth Analysis

### 1. Architecture: Client-Side Sharding

Memcached uses **client-side consistent hashing** to distribute keys:

```
┌─────────────────────────────────────────────┐
│         Memcached Architecture               │
├─────────────────────────────────────────────┤
│  Application                                 │
│      ├─ Memcached Client Library            │
│      │   (Handles sharding via consistent   │
│      │    hashing - decides which server)   │
│      └─ Hash(key) → Server                  │
└─────────────────────────────────────────────┘
         │
         ├────────────────────┬───────────────┐
         ▼                    ▼               ▼
┌──────────────┐    ┌──────────────┐  ┌──────────────┐
│ Memcached 1  │    │ Memcached 2  │  │ Memcached 3  │
│ (Server 1)   │    │ (Server 2)   │  │ (Server 3)   │
└──────────────┘    └──────────────┘  └──────────────┘
  No replication        No persistence       No clustering
```

**Key Points:**
- **Client-side sharding:** Client decides which server to connect to (not server-side like Redis Cluster)
- **No replication:** Each key stored on only one server
- **No persistence:** Data lost on restart
- **No clustering:** Servers don't communicate with each other

---

### 2. Operations: Simple Key-Value

**Basic Operations:**

```python
import memcache

# Connect to servers
mc = memcache.Client(['192.168.1.1:11211', '192.168.1.2:11211'])

# Set (key, value, expiration_time)
mc.set('user:1000', '{"name": "Alice", "age": 30}', time=3600)  # 1 hour TTL

# Get
user_data = mc.get('user:1000')

# Delete
mc.delete('user:1000')

# Add (only if key doesn't exist)
mc.add('counter', 0)

# Increment
mc.incr('counter', 1)  # Atomic increment

# Decrement
mc.decr('counter', 1)  # Atomic decrement

# Multi-get (efficient batch operation)
users = mc.get_multi(['user:1000', 'user:1001', 'user:1002'])
```

**Supported Commands:**

| Command | Purpose | Atomicity |
|---------|---------|-----------|
| `set` | Store key-value | Yes |
| `get` | Retrieve value | Yes |
| `delete` | Remove key | Yes |
| `add` | Store only if not exists | Yes |
| `replace` | Update only if exists | Yes |
| `incr/decr` | Increment/decrement counter | Yes |
| `cas` (Compare-And-Swap) | Optimistic locking | Yes |
| `get_multi` | Batch get | Yes |

---

### 3. Memcached vs. Redis

| Feature | Memcached | Redis |
|---------|-----------|-------|
| **Data structures** | Strings only | Strings, Lists, Sets, Sorted Sets, Hashes, Streams, etc. |
| **Persistence** | ❌ No | ✅ RDB + AOF |
| **Replication** | ❌ No | ✅ Master-slave |
| **Clustering** | ❌ Client-side only | ✅ Redis Cluster (server-side) |
| **Pub/Sub** | ❌ No | ✅ Yes |
| **Transactions** | ❌ No | ✅ MULTI/EXEC |
| **Lua scripting** | ❌ No | ✅ Yes |
| **Multi-threading** | ✅ Yes (true multi-threaded) | ❌ No (single-threaded event loop) |
| **Performance** | ⚡ Slightly faster (multi-threaded) | ⚡ Very fast (but single-threaded) |
| **Memory overhead** | Lower | Higher (more features) |
| **Use case** | Pure caching (no durability needed) | Cache + data structures + persistence |

**Performance Comparison:**

| Metric | Memcached | Redis |
|--------|-----------|-------|
| **Simple GET** | ~1 million ops/sec | ~800K ops/sec |
| **Latency** | <0.5ms | <1ms |
| **Memory efficiency** | Better (minimal overhead) | Good (slightly more overhead) |
| **Multi-core utilization** | ✅ Excellent (multi-threaded) | ⚠️ Limited (single-threaded) |

---

### 4. Consistent Hashing (Client-Side)

**How Memcached Distributes Keys:**

```python
import hashlib

def get_server(key, servers):
    # Hash the key
    hash_value = int(hashlib.md5(key.encode()).hexdigest(), 16)
    
    # Modulo to select server
    server_index = hash_value % len(servers)
    
    return servers[server_index]

servers = ['192.168.1.1:11211', '192.168.1.2:11211', '192.168.1.3:11211']

# Key 'user:1000' always goes to same server
server = get_server('user:1000', servers)
```

**Problem: Adding/Removing Servers**

```
Initial: 3 servers
Key 'user:1000' → hash(user:1000) % 3 = Server 1

After adding 4th server: 4 servers
Key 'user:1000' → hash(user:1000) % 4 = Server 2  ← CACHE MISS!
```

**Solution: Consistent Hashing with Virtual Nodes**

```python
from bisect import bisect_right

class ConsistentHashRing:
    def __init__(self, servers, virtual_nodes=150):
        self.ring = {}
        self.sorted_keys = []
        
        for server in servers:
            for i in range(virtual_nodes):
                # Create virtual nodes
                key = hashlib.md5(f"{server}:{i}".encode()).hexdigest()
                self.ring[int(key, 16)] = server
        
        self.sorted_keys = sorted(self.ring.keys())
    
    def get_server(self, key):
        hash_value = int(hashlib.md5(key.encode()).hexdigest(), 16)
        idx = bisect_right(self.sorted_keys, hash_value)
        
        if idx == len(self.sorted_keys):
            idx = 0
        
        return self.ring[self.sorted_keys[idx]]
```

**Benefit:** Adding/removing servers only affects ~1/N of keys (not all keys).

---

### 5. Eviction Policy

**LRU (Least Recently Used) - Default:**

When memory is full, Memcached evicts the **least recently used** items:

```
Memory: [--------- FULL ---------]

New item arrives → Evict LRU item

Order (most recent → least recent):
1. user:1005 (accessed 1s ago)
2. user:1003 (accessed 5s ago)
3. user:1001 (accessed 30s ago) ← EVICTED FIRST
```

**Slab Allocator:**

Memcached uses a **slab allocator** to reduce memory fragmentation:

```
┌────────────────────────────────────┐
│      Memcached Memory (Slabs)      │
├────────────────────────────────────┤
│  Slab 1: 64-byte chunks            │
│  Slab 2: 128-byte chunks           │
│  Slab 3: 256-byte chunks           │
│  ...                               │
│  Slab N: 1MB chunks                │
└────────────────────────────────────┘
```

**Implication:** Items stored in pre-allocated chunks (some memory waste for odd sizes, but faster allocation).

---

### 6. Use Cases

#### **6.1 Database Query Caching**

```python
import memcache
import mysql.connector

mc = memcache.Client(['localhost:11211'])
db = mysql.connector.connect(host='localhost', database='mydb')

def get_user(user_id):
    # Check cache first
    cache_key = f"user:{user_id}"
    user = mc.get(cache_key)
    
    if user:
        return json.loads(user)
    
    # Cache miss: Query database
    cursor = db.cursor()
    cursor.execute("SELECT * FROM users WHERE id = %s", (user_id,))
    user = cursor.fetchone()
    
    # Store in cache (1 hour TTL)
    mc.set(cache_key, json.dumps(user), time=3600)
    
    return user
```

#### **6.2 Session Storage**

```python
import memcache
from flask import session

mc = memcache.Client(['localhost:11211'])

def store_session(session_id, data):
    mc.set(f"session:{session_id}", json.dumps(data), time=86400)  # 24 hour TTL

def get_session(session_id):
    data = mc.get(f"session:{session_id}")
    return json.loads(data) if data else None
```

#### **6.3 Page Fragment Caching (Web)**

```python
import memcache

mc = memcache.Client(['localhost:11211'])

def render_homepage():
    cache_key = "homepage:fragment:trending"
    html = mc.get(cache_key)
    
    if not html:
        # Generate expensive HTML
        html = generate_trending_section()
        mc.set(cache_key, html, time=300)  # 5 minute TTL
    
    return html
```

---

### 7. When to Use Memcached vs. Redis

#### **✅ Use Memcached When:**

1. **Pure caching** — No need for persistence (data loss on restart is OK)
2. **Simple key-value** — Only storing strings (no complex data structures)
3. **Multi-core servers** — Need to fully utilize multiple CPU cores
4. **Minimal memory overhead** — Every byte of RAM matters
5. **Existing infrastructure** — Already using Memcached (migration cost)
6. **Absolute maximum performance** — Slightly faster than Redis for simple gets/sets

#### **✅ Use Redis When:**

1. **Persistence required** — Data must survive restarts (RDB/AOF)
2. **Complex data structures** — Need lists, sets, sorted sets, hashes
3. **Pub/Sub** — Real-time messaging
4. **Transactions** — MULTI/EXEC support
5. **Lua scripting** — Custom atomic operations
6. **Replication** — Master-slave for HA
7. **Redis Cluster** — Server-side sharding

---

### 8. Real-World Examples

| Company | Use Case | Why Memcached? |
|---------|----------|----------------|
| **Facebook** | User session storage | Handles billions of requests/day with minimal overhead |
| **YouTube** | Video metadata caching | Fast, simple, reduces database load |
| **Wikipedia** | Page fragment caching | Multi-core servers, pure caching need |
| **Twitter** | Tweet timeline caching | High throughput, simple key-value |

---

### 9. Common Anti-Patterns

#### ❌ **1. Using Memcached for Persistent Data**

**Problem:** Memcached has no persistence — data lost on restart.

**Solution:** Use Redis with AOF or use Memcached only for ephemeral data.

#### ❌ **2. Not Handling Cache Misses**

**Problem:** 
```python
# Bad: Assumes cache always has data
user = mc.get('user:1000')
print(user['name'])  # Error if cache miss!
```

**Solution:**
```python
# Good: Handle cache miss
user = mc.get('user:1000')
if not user:
    user = fetch_from_db('user:1000')
    mc.set('user:1000', user, time=3600)
```

#### ❌ **3. Storing Large Objects**

**Problem:** Memcached has 1MB value size limit (default).

**Solution:** 
- Compress large objects
- Use chunking for objects >1MB
- Or use Redis (512MB limit)

---

### 10. Trade-offs Summary

| What You Gain | What You Sacrifice |
|---------------|-------------------|
| ✅ Extreme simplicity | ❌ No persistence (data loss on restart) |
| ✅ Multi-threaded (full CPU utilization) | ❌ No complex data structures |
| ✅ Slightly faster than Redis | ❌ No replication (single point of failure per shard) |
| ✅ Lower memory overhead | ❌ No pub/sub, transactions, or scripting |
| ✅ Proven at massive scale (Facebook) | ❌ Client-side sharding only |

---

### 11. References

- **Memcached Official Website:** [https://memcached.org/](https://memcached.org/)
- **Memcached Protocol:** [https://github.com/memcached/memcached/blob/master/doc/protocol.txt](https://github.com/memcached/memcached/blob/master/doc/protocol.txt)
- **Related Chapters:**
  - [2.2.1 Caching Deep Dive](./2.2.1-caching-deep-dive.md) — Caching strategies
  - [2.1.11 Redis Deep Dive](./2.1.11-redis-deep-dive.md) — Redis vs. Memcached
  - [2.2.2 Consistent Hashing](./2.2.2-consistent-hashing.md) — How Memcached distributes keys

---

## ✏️ Design Challenge

### Problem

You're building a **high-traffic news website** (100M page views/day) with the following caching needs:

1. **Article content:** Cache full HTML articles (average 50KB each)
2. **User sessions:** Cache user login sessions (average 1KB each, 10M active users)
3. **Homepage fragments:** Cache expensive-to-render sections (sidebar, trending articles)
4. **API responses:** Cache API responses from backend (average 5KB each)

Current infrastructure: 10 cache servers (64GB RAM each)

**Question:** Should you use Memcached or Redis? How would you distribute data across servers? What TTLs would you set? How would you handle cache invalidation when articles are updated?

### Solution

#### 🧩 Scenario

- **Traffic:** 100M page views/day = 1,157 requests/sec
- **Cache servers:** 10 servers × 64GB RAM = 640GB total cache
- **Data types:** Articles (50KB), sessions (1KB), fragments (10KB), API responses (5KB)
- **Active data:** 1M articles + 10M sessions + homepage fragments

#### ✅ Goal

- Minimize database queries (high cache hit rate >95%)
- Handle 1,157 requests/sec with <1ms latency
- Efficient memory usage across 10 servers
- Proper TTLs for each data type
- Cache invalidation strategy for article updates

#### ⚙️ Solution: Memcached (Recommended for This Use Case)

**Why Memcached over Redis?**

| Requirement | Memcached | Redis |
|-------------|-----------|-------|
| **Data structures** | Simple strings (articles, sessions are JSON strings) | ✅ Better if needed |
| **Persistence** | Not needed (all data can be regenerated) | ❌ Overkill |
| **Multi-core** | ✅ Fully utilize 16-core servers | ⚠️ Single-threaded |
| **Simplicity** | ✅ Pure caching (perfect fit) | More features than needed |
| **Performance** | ✅ Slightly faster for simple gets | Very fast but single-core |
| **Memory efficiency** | ✅ Lower overhead | Slightly more overhead |

**Recommended: Memcached** — Perfect fit for pure caching with no persistence needs.

**Data Distribution Strategy:**

```python
import memcache
import hashlib

# Connect to all servers
servers = [f'cache{i}.example.com:11211' for i in range(1, 11)]
mc = memcache.Client(servers)

# Client library handles consistent hashing automatically
```

**TTL Strategy:**

| Data Type | TTL | Rationale |
|-----------|-----|-----------|
| **Articles** | 1 hour (3600s) | Content rarely changes, long TTL acceptable |
| **User sessions** | 24 hours (86400s) | Keep users logged in |
| **Homepage fragments** | 5 minutes (300s) | Needs to be fresh (trending content) |
| **API responses** | 15 minutes (900s) | Balance freshness and DB load |

**Implementation:**

```python
import memcache
import json

mc = memcache.Client([f'cache{i}.example.com:11211' for i in range(1, 11)])

# 1. Cache article
def get_article(article_id):
    cache_key = f"article:{article_id}"
    article = mc.get(cache_key)
    
    if article:
        return article  # Cache hit
    
    # Cache miss: Fetch from database
    article = db.query("SELECT * FROM articles WHERE id = %s", article_id)
    article_html = render_article_html(article)
    
    # Store with 1 hour TTL
    mc.set(cache_key, article_html, time=3600)
    
    return article_html

# 2. Cache user session
def set_session(session_id, user_data):
    cache_key = f"session:{session_id}"
    mc.set(cache_key, json.dumps(user_data), time=86400)  # 24 hours

def get_session(session_id):
    cache_key = f"session:{session_id}"
    data = mc.get(cache_key)
    return json.loads(data) if data else None

# 3. Cache homepage fragment
def get_trending_articles():
    cache_key = "homepage:trending"
    html = mc.get(cache_key)
    
    if html:
        return html
    
    # Generate expensive HTML
    trending = db.query("SELECT * FROM articles ORDER BY views DESC LIMIT 10")
    html = render_trending_html(trending)
    
    # Store with 5 minute TTL
    mc.set(cache_key, html, time=300)
    
    return html

# 4. Cache API response
def get_api_response(endpoint, params):
    cache_key = f"api:{endpoint}:{hash(str(params))}"
    response = mc.get(cache_key)
    
    if response:
        return json.loads(response)
    
    # Call backend API
    response = requests.get(f"https://api.example.com/{endpoint}", params=params)
    
    # Store with 15 minute TTL
    mc.set(cache_key, response.text, time=900)
    
    return response.json()
```

#### ⚠️ Cache Invalidation Strategy

**Challenge: Article is updated, cache needs to be cleared**

**Solution 1: Explicit Invalidation (Write-Through)**

```python
def update_article(article_id, new_content):
    # 1. Update database
    db.execute("UPDATE articles SET content = %s WHERE id = %s", (new_content, article_id))
    
    # 2. Invalidate cache
    cache_key = f"article:{article_id}"
    mc.delete(cache_key)
    
    # Next request will fetch fresh data from DB
```

**Solution 2: Cache Versioning**

```python
# Include version in cache key
def get_article(article_id):
    # Get current version from DB (fast query, indexed)
    version = db.query("SELECT version FROM articles WHERE id = %s", article_id)
    
    cache_key = f"article:{article_id}:v{version}"
    article = mc.get(cache_key)
    
    if article:
        return article
    
    # Fetch and cache
    article = db.query("SELECT * FROM articles WHERE id = %s", article_id)
    article_html = render_article_html(article)
    mc.set(cache_key, article_html, time=3600)
    
    return article_html
```

**Solution 3: Pub/Sub Invalidation (Requires External System)**

```python
# Use Redis Pub/Sub or Kafka for invalidation messages
# Publisher (on article update)
redis_pubsub.publish('cache_invalidation', f'article:{article_id}')

# Subscriber (cache invalidation service)
def handle_invalidation(message):
    cache_key = message['data']
    mc.delete(cache_key)
```

#### 🧠 Memory Usage Estimation

**Calculation:**

```
Articles:
- 1M articles × 50KB each = 50GB
- With 1-hour TTL, ~100K active at once = 5GB

Sessions:
- 10M active users × 1KB each = 10GB

Homepage fragments:
- ~100 fragments × 10KB each = 1MB (negligible)

API responses:
- ~10K unique responses × 5KB = 50MB (negligible)

Total: 5GB + 10GB = ~15GB

Available: 640GB (10 servers × 64GB)
Utilization: 15GB / 640GB = 2.3% ✅ Plenty of headroom
```

**Distribution Across Servers (Automatic via Consistent Hashing):**

```
Server 1: ~1.5GB (articles + sessions)
Server 2: ~1.5GB
...
Server 10: ~1.5GB
```

#### ✅ Final Answer

| Aspect | Decision | Reason |
|--------|----------|--------|
| **Cache System** | **Memcached** | Pure caching, no persistence needed, multi-core efficiency |
| **Cluster Size** | 10 servers × 64GB = 640GB | 15GB needed, 2.3% utilization (plenty of headroom) |
| **TTL Strategy** | Articles: 1h, Sessions: 24h, Fragments: 5m, API: 15m | Balance freshness and DB load |
| **Invalidation** | Explicit delete on article update | Simple, reliable, works with Memcached |
| **Sharding** | Client-side consistent hashing | Built into Memcached client libraries |
| **High Availability** | No replication (acceptable for cache) | Cache misses regenerate data from DB |
| **Performance** | <1ms latency, >95% hit rate | Memcached strength |

**Performance Metrics:**
- **Cache hit rate:** 95%+ (with proper TTLs)
- **Latency:** <1ms (in-memory lookups)
- **Throughput:** 100K+ ops/sec per server (1M+ total)
- **Memory usage:** 15GB / 640GB = 2.3% utilization

**Why NOT Redis:**
- ❌ Don't need persistence (Memcached simpler)
- ❌ Don't need complex data structures (only storing strings)
- ❌ Don't need replication (cache can be regenerated)
- ✅ Memcached is slightly faster and simpler for this use case

**When to Reconsider:**
- If need persistence → Switch to Redis with AOF
- If need complex data structures (leaderboards) → Switch to Redis
- If need pub/sub for invalidation → Add Redis for pub/sub, keep Memcached for caching
