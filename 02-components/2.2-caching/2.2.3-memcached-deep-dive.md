# 2.2.3 Memcached Deep Dive: The Simple, Fast Cache

## Intuitive Explanation

Memcached is a **distributed in-memory key-value cache** designed for **extreme simplicity** and **speed**. Unlike
Redis, which is a feature-rich data structure store, Memcached does one thing exceptionally well: **store and retrieve
strings blazingly fast**. It's like a giant hash table distributed across multiple servers, with no persistence, no
complex data structures, and no replication â€” just pure caching performance.

- **In-Memory:** All data in RAM (no disk persistence)
- **Distributed:** Spreads cache across multiple servers (client-side sharding)
- **Simple:** Only strings (no lists, sets, or sorted sets like Redis)
- **Use Cases:** Web page caching, session storage, database query result caching

**When to Use:** When you need the **absolute fastest** cache and don't need Redis's advanced features (persistence,
data structures, pub/sub).

---

## In-Depth Analysis

### 1. Architecture: Client-Side Sharding

Memcached uses **client-side consistent hashing** to distribute keys:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Memcached Architecture               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Application                                 â”‚
â”‚      â”œâ”€ Memcached Client Library            â”‚
â”‚      â”‚   (Handles sharding via consistent   â”‚
â”‚      â”‚    hashing - decides which server)   â”‚
â”‚      â””â”€ Hash(key) â†’ Server                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â–¼                    â–¼               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Memcached 1  â”‚    â”‚ Memcached 2  â”‚  â”‚ Memcached 3  â”‚
â”‚ (Server 1)   â”‚    â”‚ (Server 2)   â”‚  â”‚ (Server 3)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  No replication        No persistence       No clustering
```

**Key Points:**

- **Client-side sharding:** Client decides which server to connect to (not server-side like Redis Cluster)
- **No replication:** Each key stored on only one server
- **No persistence:** Data lost on restart
- **No clustering:** Servers don't communicate with each other

---

### 2. Operations: Simple Key-Value

**Basic Operations:**

```python
import memcache

# Connect to servers
mc = memcache.Client(['192.168.1.1:11211', '192.168.1.2:11211'])

# Set (key, value, expiration_time)
mc.set('user:1000', '{"name": "Alice", "age": 30}', time=3600)  # 1 hour TTL

# Get
user_data = mc.get('user:1000')

# Delete
mc.delete('user:1000')

# Add (only if key doesn't exist)
mc.add('counter', 0)

# Increment
mc.incr('counter', 1)  # Atomic increment

# Decrement
mc.decr('counter', 1)  # Atomic decrement

# Multi-get (efficient batch operation)
users = mc.get_multi(['user:1000', 'user:1001', 'user:1002'])
```

**Supported Commands:**

| Command                  | Purpose                     | Atomicity |
|--------------------------|-----------------------------|-----------|
| `set`                    | Store key-value             | Yes       |
| `get`                    | Retrieve value              | Yes       |
| `delete`                 | Remove key                  | Yes       |
| `add`                    | Store only if not exists    | Yes       |
| `replace`                | Update only if exists       | Yes       |
| `incr/decr`              | Increment/decrement counter | Yes       |
| `cas` (Compare-And-Swap) | Optimistic locking          | Yes       |
| `get_multi`              | Batch get                   | Yes       |

---

### 3. Memcached vs. Redis

| Feature             | Memcached                           | Redis                                                    |
|---------------------|-------------------------------------|----------------------------------------------------------|
| **Data structures** | Strings only                        | Strings, Lists, Sets, Sorted Sets, Hashes, Streams, etc. |
| **Persistence**     | âŒ No                                | âœ… RDB + AOF                                              |
| **Replication**     | âŒ No                                | âœ… Master-slave                                           |
| **Clustering**      | âŒ Client-side only                  | âœ… Redis Cluster (server-side)                            |
| **Pub/Sub**         | âŒ No                                | âœ… Yes                                                    |
| **Transactions**    | âŒ No                                | âœ… MULTI/EXEC                                             |
| **Lua scripting**   | âŒ No                                | âœ… Yes                                                    |
| **Multi-threading** | âœ… Yes (true multi-threaded)         | âŒ No (single-threaded event loop)                        |
| **Performance**     | âš¡ Slightly faster (multi-threaded)  | âš¡ Very fast (but single-threaded)                        |
| **Memory overhead** | Lower                               | Higher (more features)                                   |
| **Use case**        | Pure caching (no durability needed) | Cache + data structures + persistence                    |

**Performance Comparison:**

| Metric                     | Memcached                    | Redis                         |
|----------------------------|------------------------------|-------------------------------|
| **Simple GET**             | ~1 million ops/sec           | ~800K ops/sec                 |
| **Latency**                | <0.5ms                       | <1ms                          |
| **Memory efficiency**      | Better (minimal overhead)    | Good (slightly more overhead) |
| **Multi-core utilization** | âœ… Excellent (multi-threaded) | âš ï¸ Limited (single-threaded)  |

---

### 4. Consistent Hashing (Client-Side)

**How Memcached Distributes Keys:**

```python
import hashlib

def get_server(key, servers):
    # Hash the key
    hash_value = int(hashlib.md5(key.encode()).hexdigest(), 16)
    
    # Modulo to select server
    server_index = hash_value % len(servers)
    
    return servers[server_index]

servers = ['192.168.1.1:11211', '192.168.1.2:11211', '192.168.1.3:11211']

# Key 'user:1000' always goes to same server
server = get_server('user:1000', servers)
```

**Problem: Adding/Removing Servers**

```
Initial: 3 servers
Key 'user:1000' â†’ hash(user:1000) % 3 = Server 1

After adding 4th server: 4 servers
Key 'user:1000' â†’ hash(user:1000) % 4 = Server 2  â† CACHE MISS!
```

**Solution: Consistent Hashing with Virtual Nodes**

```python
from bisect import bisect_right

class ConsistentHashRing:
    def __init__(self, servers, virtual_nodes=150):
        self.ring = {}
        self.sorted_keys = []
        
        for server in servers:
            for i in range(virtual_nodes):
                # Create virtual nodes
                key = hashlib.md5(f"{server}:{i}".encode()).hexdigest()
                self.ring[int(key, 16)] = server
        
        self.sorted_keys = sorted(self.ring.keys())
    
    def get_server(self, key):
        hash_value = int(hashlib.md5(key.encode()).hexdigest(), 16)
        idx = bisect_right(self.sorted_keys, hash_value)
        
        if idx == len(self.sorted_keys):
            idx = 0
        
        return self.ring[self.sorted_keys[idx]]
```

**Benefit:** Adding/removing servers only affects ~1/N of keys (not all keys).

---

### 5. Eviction Policy

**LRU (Least Recently Used) - Default:**

When memory is full, Memcached evicts the **least recently used** items:

```
Memory: [--------- FULL ---------]

New item arrives â†’ Evict LRU item

Order (most recent â†’ least recent):
1. user:1005 (accessed 1s ago)
2. user:1003 (accessed 5s ago)
3. user:1001 (accessed 30s ago) â† EVICTED FIRST
```

**Slab Allocator:**

Memcached uses a **slab allocator** to reduce memory fragmentation:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Memcached Memory (Slabs)      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Slab 1: 64-byte chunks            â”‚
â”‚  Slab 2: 128-byte chunks           â”‚
â”‚  Slab 3: 256-byte chunks           â”‚
â”‚  ...                               â”‚
â”‚  Slab N: 1MB chunks                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Implication:** Items stored in pre-allocated chunks (some memory waste for odd sizes, but faster allocation).

---

### 6. Use Cases

#### **6.1 Database Query Caching**

```python
import memcache
import mysql.connector

mc = memcache.Client(['localhost:11211'])
db = mysql.connector.connect(host='localhost', database='mydb')

def get_user(user_id):
    # Check cache first
    cache_key = f"user:{user_id}"
    user = mc.get(cache_key)
    
    if user:
        return json.loads(user)
    
    # Cache miss: Query database
    cursor = db.cursor()
    cursor.execute("SELECT * FROM users WHERE id = %s", (user_id,))
    user = cursor.fetchone()
    
    # Store in cache (1 hour TTL)
    mc.set(cache_key, json.dumps(user), time=3600)
    
    return user
```

#### **6.2 Session Storage**

```python
import memcache
from flask import session

mc = memcache.Client(['localhost:11211'])

def store_session(session_id, data):
    mc.set(f"session:{session_id}", json.dumps(data), time=86400)  # 24 hour TTL

def get_session(session_id):
    data = mc.get(f"session:{session_id}")
    return json.loads(data) if data else None
```

#### **6.3 Page Fragment Caching (Web)**

```python
import memcache

mc = memcache.Client(['localhost:11211'])

def render_homepage():
    cache_key = "homepage:fragment:trending"
    html = mc.get(cache_key)
    
    if not html:
        # Generate expensive HTML
        html = generate_trending_section()
        mc.set(cache_key, html, time=300)  # 5 minute TTL
    
    return html
```

---

### 7. When to Use Memcached vs. Redis

#### **âœ… Use Memcached When:**

1. **Pure caching** â€” No need for persistence (data loss on restart is OK)
2. **Simple key-value** â€” Only storing strings (no complex data structures)
3. **Multi-core servers** â€” Need to fully utilize multiple CPU cores
4. **Minimal memory overhead** â€” Every byte of RAM matters
5. **Existing infrastructure** â€” Already using Memcached (migration cost)
6. **Absolute maximum performance** â€” Slightly faster than Redis for simple gets/sets

#### **âœ… Use Redis When:**

1. **Persistence required** â€” Data must survive restarts (RDB/AOF)
2. **Complex data structures** â€” Need lists, sets, sorted sets, hashes
3. **Pub/Sub** â€” Real-time messaging
4. **Transactions** â€” MULTI/EXEC support
5. **Lua scripting** â€” Custom atomic operations
6. **Replication** â€” Master-slave for HA
7. **Redis Cluster** â€” Server-side sharding

---

### 8. Real-World Examples

| Company       | Use Case               | Why Memcached?                                         |
|---------------|------------------------|--------------------------------------------------------|
| **Facebook**  | User session storage   | Handles billions of requests/day with minimal overhead |
| **YouTube**   | Video metadata caching | Fast, simple, reduces database load                    |
| **Wikipedia** | Page fragment caching  | Multi-core servers, pure caching need                  |
| **Twitter**   | Tweet timeline caching | High throughput, simple key-value                      |

---

### 9. Common Anti-Patterns

#### âŒ **1. Using Memcached for Persistent Data**

**Problem:** Memcached has no persistence â€” data lost on restart.

**Solution:** Use Redis with AOF or use Memcached only for ephemeral data.

#### âŒ **2. Not Handling Cache Misses**

**Problem:**

```python
# Bad: Assumes cache always has data
user = mc.get('user:1000')
print(user['name'])  # Error if cache miss!
```

**Solution:**

```python
# Good: Handle cache miss
user = mc.get('user:1000')
if not user:
    user = fetch_from_db('user:1000')
    mc.set('user:1000', user, time=3600)
```

#### âŒ **3. Storing Large Objects**

**Problem:** Memcached has 1MB value size limit (default).

**Solution:**

- Compress large objects
- Use chunking for objects >1MB
- Or use Redis (512MB limit)

---

### 10. Trade-offs Summary

| What You Gain                           | What You Sacrifice                                   |
|-----------------------------------------|------------------------------------------------------|
| âœ… Extreme simplicity                    | âŒ No persistence (data loss on restart)              |
| âœ… Multi-threaded (full CPU utilization) | âŒ No complex data structures                         |
| âœ… Slightly faster than Redis            | âŒ No replication (single point of failure per shard) |
| âœ… Lower memory overhead                 | âŒ No pub/sub, transactions, or scripting             |
| âœ… Proven at massive scale (Facebook)    | âŒ Client-side sharding only                          |

---

### 11. References

- **Memcached Official Website:** [https://memcached.org/](https://memcached.org/)
- **Memcached Protocol:
  ** [https://github.com/memcached/memcached/blob/master/doc/protocol.txt](https://github.com/memcached/memcached/blob/master/doc/protocol.txt)
- **Related Chapters:**
    - [2.2.1 Caching Deep Dive](./2.2.1-caching-deep-dive.md) â€” Caching strategies
    - [2.1.11 Redis Deep Dive](./2.1.11-redis-deep-dive.md) â€” Redis vs. Memcached
    - [2.2.2 Consistent Hashing](./2.2.2-consistent-hashing.md) â€” How Memcached distributes keys

---

## âœï¸ Design Challenge

### Problem

You're building a **high-traffic news website** (100M page views/day) with the following caching needs:

1. **Article content:** Cache full HTML articles (average 50KB each)
2. **User sessions:** Cache user login sessions (average 1KB each, 10M active users)
3. **Homepage fragments:** Cache expensive-to-render sections (sidebar, trending articles)
4. **API responses:** Cache API responses from backend (average 5KB each)

Current infrastructure: 10 cache servers (64GB RAM each)

**Question:** Should you use Memcached or Redis? How would you distribute data across servers? What TTLs would you set?
How would you handle cache invalidation when articles are updated?

### Solution

#### ğŸ§© Scenario

- **Traffic:** 100M page views/day = 1,157 requests/sec
- **Cache servers:** 10 servers Ã— 64GB RAM = 640GB total cache
- **Data types:** Articles (50KB), sessions (1KB), fragments (10KB), API responses (5KB)
- **Active data:** 1M articles + 10M sessions + homepage fragments

#### âœ… Goal

- Minimize database queries (high cache hit rate >95%)
- Handle 1,157 requests/sec with <1ms latency
- Efficient memory usage across 10 servers
- Proper TTLs for each data type
- Cache invalidation strategy for article updates

#### âš™ï¸ Solution: Memcached (Recommended for This Use Case)

**Why Memcached over Redis?**

| Requirement           | Memcached                                            | Redis                     |
|-----------------------|------------------------------------------------------|---------------------------|
| **Data structures**   | Simple strings (articles, sessions are JSON strings) | âœ… Better if needed        |
| **Persistence**       | Not needed (all data can be regenerated)             | âŒ Overkill                |
| **Multi-core**        | âœ… Fully utilize 16-core servers                      | âš ï¸ Single-threaded        |
| **Simplicity**        | âœ… Pure caching (perfect fit)                         | More features than needed |
| **Performance**       | âœ… Slightly faster for simple gets                    | Very fast but single-core |
| **Memory efficiency** | âœ… Lower overhead                                     | Slightly more overhead    |

**Recommended: Memcached** â€” Perfect fit for pure caching with no persistence needs.

**Data Distribution Strategy:**

```python
import memcache
import hashlib

# Connect to all servers
servers = [f'cache{i}.example.com:11211' for i in range(1, 11)]
mc = memcache.Client(servers)

# Client library handles consistent hashing automatically
```

**TTL Strategy:**

| Data Type              | TTL               | Rationale                                   |
|------------------------|-------------------|---------------------------------------------|
| **Articles**           | 1 hour (3600s)    | Content rarely changes, long TTL acceptable |
| **User sessions**      | 24 hours (86400s) | Keep users logged in                        |
| **Homepage fragments** | 5 minutes (300s)  | Needs to be fresh (trending content)        |
| **API responses**      | 15 minutes (900s) | Balance freshness and DB load               |

**Implementation:**

```python
import memcache
import json

mc = memcache.Client([f'cache{i}.example.com:11211' for i in range(1, 11)])

# 1. Cache article
def get_article(article_id):
    cache_key = f"article:{article_id}"
    article = mc.get(cache_key)
    
    if article:
        return article  # Cache hit
    
    # Cache miss: Fetch from database
    article = db.query("SELECT * FROM articles WHERE id = %s", article_id)
    article_html = render_article_html(article)
    
    # Store with 1 hour TTL
    mc.set(cache_key, article_html, time=3600)
    
    return article_html

# 2. Cache user session
def set_session(session_id, user_data):
    cache_key = f"session:{session_id}"
    mc.set(cache_key, json.dumps(user_data), time=86400)  # 24 hours

def get_session(session_id):
    cache_key = f"session:{session_id}"
    data = mc.get(cache_key)
    return json.loads(data) if data else None

# 3. Cache homepage fragment
def get_trending_articles():
    cache_key = "homepage:trending"
    html = mc.get(cache_key)
    
    if html:
        return html
    
    # Generate expensive HTML
    trending = db.query("SELECT * FROM articles ORDER BY views DESC LIMIT 10")
    html = render_trending_html(trending)
    
    # Store with 5 minute TTL
    mc.set(cache_key, html, time=300)
    
    return html

# 4. Cache API response
def get_api_response(endpoint, params):
    cache_key = f"api:{endpoint}:{hash(str(params))}"
    response = mc.get(cache_key)
    
    if response:
        return json.loads(response)
    
    # Call backend API
    response = requests.get(f"https://api.example.com/{endpoint}", params=params)
    
    # Store with 15 minute TTL
    mc.set(cache_key, response.text, time=900)
    
    return response.json()
```

#### âš ï¸ Cache Invalidation Strategy

**Challenge: Article is updated, cache needs to be cleared**

**Solution 1: Explicit Invalidation (Write-Through)**

```python
def update_article(article_id, new_content):
    # 1. Update database
    db.execute("UPDATE articles SET content = %s WHERE id = %s", (new_content, article_id))
    
    # 2. Invalidate cache
    cache_key = f"article:{article_id}"
    mc.delete(cache_key)
    
    # Next request will fetch fresh data from DB
```

**Solution 2: Cache Versioning**

```python
# Include version in cache key
def get_article(article_id):
    # Get current version from DB (fast query, indexed)
    version = db.query("SELECT version FROM articles WHERE id = %s", article_id)
    
    cache_key = f"article:{article_id}:v{version}"
    article = mc.get(cache_key)
    
    if article:
        return article
    
    # Fetch and cache
    article = db.query("SELECT * FROM articles WHERE id = %s", article_id)
    article_html = render_article_html(article)
    mc.set(cache_key, article_html, time=3600)
    
    return article_html
```

**Solution 3: Pub/Sub Invalidation (Requires External System)**

```python
# Use Redis Pub/Sub or Kafka for invalidation messages
# Publisher (on article update)
redis_pubsub.publish('cache_invalidation', f'article:{article_id}')

# Subscriber (cache invalidation service)
def handle_invalidation(message):
    cache_key = message['data']
    mc.delete(cache_key)
```

#### ğŸ§  Memory Usage Estimation

**Calculation:**

```
Articles:
- 1M articles Ã— 50KB each = 50GB
- With 1-hour TTL, ~100K active at once = 5GB

Sessions:
- 10M active users Ã— 1KB each = 10GB

Homepage fragments:
- ~100 fragments Ã— 10KB each = 1MB (negligible)

API responses:
- ~10K unique responses Ã— 5KB = 50MB (negligible)

Total: 5GB + 10GB = ~15GB

Available: 640GB (10 servers Ã— 64GB)
Utilization: 15GB / 640GB = 2.3% âœ… Plenty of headroom
```

**Distribution Across Servers (Automatic via Consistent Hashing):**

```
Server 1: ~1.5GB (articles + sessions)
Server 2: ~1.5GB
...
Server 10: ~1.5GB
```

#### âœ… Final Answer

| Aspect                | Decision                                             | Reason                                                     |
|-----------------------|------------------------------------------------------|------------------------------------------------------------|
| **Cache System**      | **Memcached**                                        | Pure caching, no persistence needed, multi-core efficiency |
| **Cluster Size**      | 10 servers Ã— 64GB = 640GB                            | 15GB needed, 2.3% utilization (plenty of headroom)         |
| **TTL Strategy**      | Articles: 1h, Sessions: 24h, Fragments: 5m, API: 15m | Balance freshness and DB load                              |
| **Invalidation**      | Explicit delete on article update                    | Simple, reliable, works with Memcached                     |
| **Sharding**          | Client-side consistent hashing                       | Built into Memcached client libraries                      |
| **High Availability** | No replication (acceptable for cache)                | Cache misses regenerate data from DB                       |
| **Performance**       | <1ms latency, >95% hit rate                          | Memcached strength                                         |

**Performance Metrics:**

- **Cache hit rate:** 95%+ (with proper TTLs)
- **Latency:** <1ms (in-memory lookups)
- **Throughput:** 100K+ ops/sec per server (1M+ total)
- **Memory usage:** 15GB / 640GB = 2.3% utilization

**Why NOT Redis:**

- âŒ Don't need persistence (Memcached simpler)
- âŒ Don't need complex data structures (only storing strings)
- âŒ Don't need replication (cache can be regenerated)
- âœ… Memcached is slightly faster and simpler for this use case

**When to Reconsider:**

- If need persistence â†’ Switch to Redis with AOF
- If need complex data structures (leaderboards) â†’ Switch to Redis
- If need pub/sub for invalidation â†’ Add Redis for pub/sub, keep Memcached for caching
