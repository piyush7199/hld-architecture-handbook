# 2.4.2 Observability: Logging, Metrics, and Distributed Tracing

## Intuitive Explanation

Observability is the ability to understand the internal state of a system merely by examining the data it outputs (logs,
metrics, and traces). It answers the question: **"Why did that order fail at 3 $\text{AM}$?"**

It goes beyond basic **Monitoring** (which tells you if the system is up or down) to explain why something is happening.

---

## In-Depth Analysis

The practice of observability relies on three core data pillars:

### 1. Metrics (The Dashboard)

Metrics are numerical measurements collected over time, providing aggregate views of system health and performance.

- **What they Measure:** $\text{CPU}$ usage, $\text{Memory}$
  use, $\text{Request}$ $\text{per}$ $\text{Second}$ ($\text{QPS}$), Latency ($\text{P95}$ or $\text{P99}$), and
  business-level counts ($\text{Orders}$ $\text{Placed}$).
- **Tools:** $\text{Prometheus}$ (collection and storage), $\text{Grafana}$ (visualization).
- **Types:**
    - **Counter:** A value that only increases (e.g., total number of $\text{HTTP}$ requests since startup).
    - **Gauge:** A value that can go up or down (e.g., current $\text{CPU}$ usage, $\text{Queue}$ $\text{size}$).
    - **Histogram/Summary:** Used to calculate statistical distribution (e.g., $95\%$ of requests completed
      in $<100$ $\text{ms}$).

### 2. Logging (The Diary)

Logs are discrete, chronological records of events that occur within an application instance. They are the "who, what,
and when" of a specific incident.

- **What they Measure:** Error messages, exception stack traces, debugging output, and business
  events ($\text{User}$ $\text{A}$ $\text{logged}$ $\text{in}$).
- **Tools:** $\text{Elasticsearch}$, $\text{Logstash}$, $\text{Kibana}$ ($\text{ELK}$ stack) or $\text{Loki}$.
- **Correlation ID:** Crucial for distributed systems. Every request that enters the system is assigned a
  unique $\text{Correlation}$ $\text{ID}$. This ID is passed to every service that handles the request, allowing all
  related log lines to be queried and grouped together.

### 3. Distributed Tracing (The Journey Map)

Tracing visualizes the entire path of a single request as it hops between microservices, databases, and queues.

- **What they Measure:** How much time is spent in each service, revealing latency bottlenecks.
- **Tools:** $\text{Jaeger}$, $\text{Zipkin}$.
- **Spans:** Each unit of work within a service (like a database query or an $\text{API}$ call) is recorded as
  a $\text{Span}$. Spans are nested and linked to form the complete trace of the request's journey.

### 4. Alerting and Remediation

- **Alerts:** Notifications triggered when metrics cross predefined thresholds (e.g., "CPU is $>80\%$ for $5$ minutes"
  or "P99 latency is $>500$ $\text{ms}$").
- **Runbooks:** Documented procedures that define the steps an engineer must take when a specific alert is triggered,
  allowing for fast, consistent remediation.

---

## ‚úèÔ∏è Design Challenge

### Problem

A customer reports that their order took $10$ seconds to process, but your $\text{CPU}$ utilization dashboards (Metrics)
look normal. Explain how **Distributed Tracing** would help you diagnose this latency spike, and what the trace would
likely reveal in this scenario.

### Solution

#### üß© Scenario Summary

| Signal                       | Observation                                              |
|------------------------------|----------------------------------------------------------|
| **Metric (CPU Utilization)** | Normal ‚Äî no overload on servers                          |
| **Symptom**                  | One specific request took 10 seconds                     |
| **Goal**                     | Find which part of the **request path** caused the delay |

Traditional metrics can‚Äôt pinpoint latency across multiple services ‚Äî you need **Distributed Tracing.**

#### ‚úÖ Step 1: What is Distributed Tracing

**Distributed Tracing** tracks a single request as it flows through multiple microservices, APIs, and datastores.
It assigns each request a unique Trace ID and records spans (timed segments) for each service call.

Example (Order Processing Flow):

```
OrderService ‚Üí PaymentService ‚Üí InventoryService ‚Üí ShippingService
```

Each arrow represents a span that records timing and context (start, end, duration, errors).

#### ‚úÖ Step 2: How It Diagnoses the Latency

Distributed tracing shows a timeline of every component involved in the order request:

| Service          | Span Duration    | Notes                  |
|------------------|------------------|------------------------|
| OrderService     | 50ms             | API received order     |
| PaymentService   | 200ms            | Payment authorized     |
| InventoryService | **8,000ms (8s)** | Database query delay   |
| ShippingService  | 300ms            | Shipping label created |

> üí° The trace visually highlights where one span dominates total request time, pinpointing the slow dependency.

#### ‚úÖ Step 3: What the Trace Likely Reveals

Since CPU metrics are normal (no compute bottleneck), the trace likely shows:

1. A slow downstream dependency
    - e.g., a database query, third-party API, or network call took 8‚Äì9 seconds.

2. Queueing delay or retry storm
    - Possibly the service was waiting on a connection pool or retrying due to transient errors.

3. High latency in one microservice
    - One specific span (say InventoryService.fetchStock()) has unusually long duration compared to others.

This helps isolate whether the issue is:

- Inside your system (bad query / deadlock), or
- External (third-party API / slow network).

#### ‚úÖ Step 4: Why Metrics Alone Missed It

| Tool        | Visibility                                 | Limitation                             |
|-------------|--------------------------------------------|----------------------------------------|
| **Metrics** | Aggregate-level (CPU, latency percentiles) | Can hide per-request outliers          |
| **Logs**    | Point-in-time events                       | Hard to correlate across services      |
| **Traces**  | End-to-end per-request timing              | Pinpoints the exact span causing delay |

Metrics looked normal because only one request (outlier) was affected ‚Äî not enough to change aggregate averages.

#### ‚öôÔ∏è Step 5: Example Visualization

```
Trace ID: 12345abcd

OrderService [50ms]
 ‚îú‚îÄ‚îÄ PaymentService [200ms]
 ‚îú‚îÄ‚îÄ InventoryService [8,000ms]  ‚ö†Ô∏è
 ‚îî‚îÄ‚îÄ ShippingService [300ms]

```

The trace clearly shows that **InventoryService** caused the delay.

#### ‚úÖ Final Summary

| Aspect             | Detail                                                                                    |
|--------------------|-------------------------------------------------------------------------------------------|
| **Problem**        | Order took 10s; CPU metrics normal                                                        |
| **Tool Used**      | Distributed Tracing (e.g., Jaeger, Zipkin, OpenTelemetry)                                 |
| **What It Shows**  | Timeline of all service calls for a single request                                        |
| **Likely Finding** | One downstream span (e.g., DB query / API call) consumed most time                        |
| **Benefit**        | Pinpoints latency source across distributed systems even when system metrics look healthy |
