# 2.4.6 Distributed Tracing Deep Dive: Jaeger, Zipkin, and OpenTelemetry

## Intuitive Explanation

Imagine you're a detective investigating a crime. You have witness statements (logs) and surveillance footage (metrics),
but you need to follow the suspect's exact path through the city to understand what happened.

**Distributed Tracing** is like having a GPS tracker on every request as it moves through your system. It shows you:

- **Where the request went:** Which services it visited
- **How long it spent:** Time spent in each service
- **What happened:** Success or failure at each step

**In distributed systems:**

- **Distributed Tracing:** Tracks requests across multiple services
- **Spans:** Individual operations within a service
- **Traces:** Complete request journey (collection of spans)
- **Goal:** Debug latency issues, understand request flow, identify bottlenecks
- **Benefit:** Pinpoint exactly where problems occur in complex microservices

---

## In-Depth Analysis

### 1. What is Distributed Tracing?

**Distributed Tracing** is a method of observing requests as they propagate through distributed systems, especially
microservices architectures.

**Key Concepts:**

- **Trace:** The complete journey of a request through the system
- **Span:** A single operation within a trace (e.g., database query, API call)
- **Trace ID:** Unique identifier for the entire request
- **Span ID:** Unique identifier for a single operation
- **Parent-Child Relationship:** Spans form a tree structure

**Example Trace:**

```
Trace ID: abc-123

Span 1: HTTP Request (100ms)
  â”œâ”€ Span 2: User Service (50ms)
  â”‚   â””â”€ Span 3: Database Query (45ms)
  â”œâ”€ Span 4: Order Service (30ms)
  â”‚   â””â”€ Span 5: Payment API (25ms)
  â””â”€ Span 6: Inventory Service (20ms)
```

### 2. Trace Context Propagation

**Problem:** How to pass trace context across service boundaries?

**Solution:** W3C Trace Context standard

**HTTP Headers:**

```
traceparent: 00-4bf92f3577b34da6a3ce929d0e0e4736-00f067aa0ba902b7-01
tracestate: congo=t61rcWkgMzE
```

**traceparent Format:**

```
version-trace_id-parent_id-flags
00-4bf92f3577b34da6a3ce929d0e0e4736-00f067aa0ba902b7-01
```

**Components:**

- **version:** 00 (current version)
- **trace_id:** 128-bit identifier (unique per trace)
- **parent_id:** 64-bit identifier (parent span)
- **flags:** 8-bit flags (sampled, etc.)

**Propagation:**

```
Service A â†’ Service B:
  1. Service A creates span
  2. Service A adds traceparent header to HTTP request
  3. Service B receives request, extracts traceparent
  4. Service B creates child span with same trace_id
  5. Service B sets parent_id to Service A's span_id
```

### 3. Major Distributed Tracing Tools

#### A. Jaeger

**Architecture:**

- **Open-source:** CNCF project
- **Components:** Agent, Collector, Query, Storage
- **Storage:** Cassandra, Elasticsearch, Kafka
- **UI:** Web-based trace visualization

**Key Features:**

- **Distributed Context Propagation:** W3C Trace Context
- **Sampling:** Head-based, tail-based, adaptive
- **Storage:** Multiple backends (Cassandra, Elasticsearch)
- **Query:** Trace search by service, operation, tags

**Deployment:**

```
Jaeger Agent (per host):
  - Receives spans from applications
  - Batches and sends to Collector

Jaeger Collector:
  - Receives spans from Agents
  - Validates and processes spans
  - Writes to storage backend

Jaeger Query:
  - Provides API and UI
  - Queries storage backend
  - Displays traces
```

#### B. Zipkin

**Architecture:**

- **Open-source:** Apache 2.0
- **Components:** Collector, Storage, Query, UI
- **Storage:** In-memory, MySQL, Cassandra, Elasticsearch
- **UI:** Web-based trace visualization

**Key Features:**

- **Simple Architecture:** Easier to deploy than Jaeger
- **HTTP/JSON API:** Easy integration
- **Storage:** Multiple backends
- **Dependency Graph:** Visualizes service dependencies

**Deployment:**

```
Applications â†’ Zipkin Collector:
  - HTTP POST /api/v2/spans
  - JSON or Protobuf format

Zipkin Collector:
  - Validates spans
  - Writes to storage

Zipkin UI:
  - Queries storage
  - Displays traces
```

#### C. OpenTelemetry

**Architecture:**

- **Standard:** CNCF project (merger of OpenTracing + OpenCensus)
- **Components:** SDK, Collector, Exporters
- **Vendor-Agnostic:** Export to any backend (Jaeger, Zipkin, Datadog, etc.)

**Key Features:**

- **Unified API:** Single API for all languages
- **Auto-Instrumentation:** Automatic tracing for common libraries
- **Multiple Signals:** Traces, metrics, logs
- **Vendor-Neutral:** Not tied to specific backend

**Components:**

```
OpenTelemetry SDK:
  - Creates spans
  - Propagates context
  - Exports to Collector

OpenTelemetry Collector:
  - Receives telemetry data
  - Processes (filtering, batching)
  - Exports to backends (Jaeger, Zipkin, Prometheus, etc.)

Exporters:
  - Jaeger Exporter
  - Zipkin Exporter
  - OTLP Exporter (OpenTelemetry Protocol)
```

### 4. Span Structure

**Span Components:**

```json
{
  "trace_id": "4bf92f3577b34da6a3ce929d0e0e4736",
  "span_id": "00f067aa0ba902b7",
  "parent_span_id": "0af7651916cd43dd",
  "operation_name": "GET /api/users",
  "start_time": 1699123456000000,
  "duration": 50000000,
  "tags": {
    "http.method": "GET",
    "http.status_code": 200,
    "service.name": "user-service"
  },
  "logs": [
    {
      "timestamp": 1699123456050000,
      "fields": {
        "event": "cache miss",
        "cache.key": "user:123"
      }
    }
  ]
}
```

**Span Fields:**

- **trace_id:** Links spans to same trace
- **span_id:** Unique identifier for span
- **parent_span_id:** Parent span (null for root)
- **operation_name:** What operation was performed
- **start_time:** When span started (nanoseconds)
- **duration:** How long span took (nanoseconds)
- **tags:** Key-value pairs (immutable metadata)
- **logs:** Timestamped events (mutable, for debugging)

### 5. Sampling Strategies

**Problem:** Tracing every request is expensive (storage, performance)

**Solution:** Sample only a subset of requests

#### A. Head-Based Sampling

**How It Works:**

```
Decision made at trace start:
  - Sample 1% of requests
  - If sampled, trace entire request
  - If not sampled, no spans created
```

**Configuration:**

```
Sampling Rate: 1% (0.01)
  - 1 in 100 requests traced
  - Consistent across all services
```

**Pros:**

- Simple to implement
- Predictable cost
- Works well for high-traffic services

**Cons:**

- May miss rare errors (only 1% sampled)
- Can't sample based on outcome (errors might not be sampled)

#### B. Tail-Based Sampling

**How It Works:**

```
Decision made after trace completes:
  - Buffer all spans
  - After trace completes, decide to keep or discard
  - Keep traces with errors, slow traces, or random sample
```

**Configuration:**

```
Keep traces if:
  - Has error (100% keep)
  - Duration > 1 second (100% keep)
  - Random 1% sample
```

**Pros:**

- Captures all errors (even rare ones)
- Captures slow traces (performance issues)
- Better signal-to-noise ratio

**Cons:**

- More complex (buffer management)
- Higher memory usage
- Requires trace completion detection

#### C. Adaptive Sampling

**How It Works:**

```
Dynamically adjust sampling rate:
  - High traffic â†’ Lower sampling rate
  - Low traffic â†’ Higher sampling rate
  - Maintain target trace volume
```

**Configuration:**

```
Target: 1000 traces/second
  - If traffic = 10K req/sec â†’ 10% sampling
  - If traffic = 100K req/sec â†’ 1% sampling
  - If traffic = 1M req/sec â†’ 0.1% sampling
```

**Pros:**

- Maintains consistent storage cost
- Adapts to traffic patterns
- Better coverage during low traffic

**Cons:**

- More complex implementation
- Requires traffic monitoring

### 6. Trace Correlation

**Problem:** How to correlate traces with logs and metrics?

**Solution:** Use trace_id in logs and metrics

**Log Correlation:**

```json
{
  "timestamp": "2024-01-15T10:30:00Z",
  "level": "ERROR",
  "message": "Database connection failed",
  "trace_id": "4bf92f3577b34da6a3ce929d0e0e4736",
  "span_id": "00f067aa0ba902b7"
}
```

**Metric Correlation:**

```
http_request_duration_seconds{
  trace_id="4bf92f3577b34da6a3ce929d0e0e4736",
  service="user-service",
  endpoint="/api/users"
} 0.05
```

**Benefits:**

- Find all logs for a trace
- Correlate metrics with specific requests
- Debug issues across observability tools

### 7. Performance Impact

**Overhead Sources:**

```
1. Span Creation: ~1-5 microseconds per span
2. Context Propagation: ~1-2 microseconds (header manipulation)
3. Span Export: ~10-100 microseconds (network I/O)
4. Storage: Depends on sampling rate
```

**Optimization Strategies:**

```
1. Sampling: Reduce trace volume (1% sampling = 99% less overhead)
2. Async Export: Don't block request (export in background)
3. Batching: Batch multiple spans before sending
4. Sampling at Edge: Sample at API Gateway (don't trace internal services)
```

**Typical Overhead:**

```
Without Sampling: 1-5% latency increase
With 1% Sampling: <0.1% latency increase
With Async Export: <0.05% latency increase
```

### 8. Integration Patterns

#### A. Manual Instrumentation

**How It Works:**

```
Developer adds tracing code:
  - Create spans manually
  - Add tags and logs
  - Propagate context
```

**Example (Python):**

```python
from opentelemetry import trace

tracer = trace.get_tracer(__name__)

def get_user(user_id):
    with tracer.start_as_current_span("get_user") as span:
        span.set_attribute("user.id", user_id)
        try:
            user = db.get_user(user_id)
            span.set_attribute("user.found", True)
            return user
        except Exception as e:
            span.record_exception(e)
            span.set_status(Status(StatusCode.ERROR))
            raise
```

**Pros:**

- Full control
- Custom tags and logs
- Fine-grained tracing

**Cons:**

- More code to write
- Easy to forget
- Maintenance burden

#### B. Auto-Instrumentation

**How It Works:**

```
Library automatically creates spans:
  - HTTP requests â†’ Automatic spans
  - Database queries â†’ Automatic spans
  - Message queue operations â†’ Automatic spans
```

**Example:**

```
OpenTelemetry Auto-Instrumentation:
  - Detects HTTP libraries (Flask, Django, FastAPI)
  - Automatically creates spans for requests
  - Adds tags (method, status, path)
  - Propagates context
```

**Pros:**

- Zero code changes
- Consistent instrumentation
- Easy to enable/disable

**Cons:**

- Less control
- May miss custom operations
- Performance overhead (minimal)

### 9. Common Use Cases

#### A. Latency Debugging

**Problem:** Request is slow, but which service is slow?

**Solution:** Trace shows time spent in each service

```
Trace: GET /api/orders/123 (500ms)
  â”œâ”€ User Service: 50ms âœ…
  â”œâ”€ Order Service: 200ms âœ…
  â”œâ”€ Payment Service: 240ms âš ï¸ (slow!)
  â””â”€ Inventory Service: 10ms âœ…

Finding: Payment Service is the bottleneck
```

#### B. Error Investigation

**Problem:** Error occurs, but where in the request flow?

**Solution:** Trace shows which span failed

```
Trace: POST /api/orders (failed)
  â”œâ”€ User Service: 50ms âœ…
  â”œâ”€ Order Service: 100ms âœ…
  â”œâ”€ Payment Service: ERROR âŒ
  â”‚   â””â”€ Error: "Payment gateway timeout"
  â””â”€ Inventory Service: (not reached)

Finding: Payment Service failed, order not created
```

#### C. Service Dependency Mapping

**Problem:** What services does my service depend on?

**Solution:** Trace analysis shows service call graph

```
Service Dependencies:
  Order Service
    â”œâ”€ User Service
    â”œâ”€ Payment Service
    â”‚   â””â”€ External Payment Gateway
    â””â”€ Inventory Service
        â””â”€ Warehouse Service
```

### 10. Best Practices

**1. Consistent Naming:**

```
Operation Names:
  âœ… Good: "GET /api/users", "db.query.users"
  âŒ Bad: "get_user", "query", "operation1"
```

**2. Meaningful Tags:**

```
Tags:
  âœ… Good: http.method="GET", db.query="SELECT * FROM users"
  âŒ Bad: tag1="value1", data="something"
```

**3. Appropriate Sampling:**

```
Sampling Strategy:
  - High-traffic services: 0.1-1% sampling
  - Low-traffic services: 10-100% sampling
  - Always sample errors (100%)
```

**4. Async Export:**

```
Export Strategy:
  - Don't block request for span export
  - Use async/background export
  - Batch spans before sending
```

**5. Trace Limits:**

```
Span Limits:
  - Max spans per trace: 1000
  - Max span duration: 1 hour
  - Max tags per span: 100
```

---

## When to Use Distributed Tracing

### âœ… Use Distributed Tracing When:

1. **Microservices Architecture:** Multiple services, need to track requests
2. **Latency Issues:** Need to identify slow services
3. **Error Debugging:** Need to find where errors occur
4. **Service Dependencies:** Need to understand service call graph
5. **Performance Optimization:** Need to identify bottlenecks
6. **Complex Systems:** Many services, hard to debug without tracing

### âŒ Don't Use Distributed Tracing When:

1. **Monolithic Application:** Single service, simple debugging
2. **Low Traffic:** Not worth the overhead
3. **Simple Systems:** Few services, easy to debug
4. **Cost Sensitive:** Tracing can be expensive at scale
5. **No Observability Budget:** Need infrastructure for storage/query

---

## Real-World Examples

### Netflix (Distributed Tracing)

**Use Case:** Debug latency in microservices

**Scale:**

- Thousands of microservices
- Millions of requests per second
- Real-time trace analysis

**Tools:**

- Custom tracing (before OpenTelemetry)
- Zipkin for visualization
- Trace correlation with logs

### Uber (Jaeger)

**Use Case:** End-to-end request tracking

**Scale:**

- Hundreds of microservices
- Billions of requests per day
- Multi-region deployment

**Features:**

- Jaeger for trace storage
- OpenTelemetry for instrumentation
- Trace-based alerting

### Google (Dapper)

**Use Case:** Internal distributed tracing

**Scale:**

- All Google services
- Trillions of requests
- Real-time analysis

**Features:**

- Custom Dapper system
- Low overhead (<1%)
- Automatic instrumentation

---

## Distributed Tracing vs. Other Solutions

| Solution                | Best For                         | Overhead            | Complexity | Features                |
|-------------------------|----------------------------------|---------------------|------------|-------------------------|
| **Distributed Tracing** | Microservices, latency debugging | Low (with sampling) | Medium     | End-to-end visibility   |
| **Logs**                | Point-in-time events             | Low                 | Low        | Event details           |
| **Metrics**             | Aggregate statistics             | Very Low            | Low        | System health           |
| **APM Tools**           | Full-stack monitoring            | Medium              | Low        | Traces + metrics + logs |

---

## Common Anti-Patterns

### âŒ **1. Tracing Everything**

**Problem:** 100% sampling on high-traffic service

**Solution:** Use appropriate sampling rate

```
âŒ Bad:
Sampling: 100% (trace every request)
â†’ 1M requests/sec = 1M traces/sec
â†’ Storage cost: $10,000/month
â†’ Performance impact: 5% latency increase

âœ… Good:
Sampling: 1% (trace 1 in 100 requests)
â†’ 1M requests/sec = 10K traces/sec
â†’ Storage cost: $100/month
â†’ Performance impact: <0.1% latency increase
```

### âŒ **2. Blocking Export**

**Problem:** Synchronous span export blocks requests

**Solution:** Use async export

```
âŒ Bad:
def handle_request():
    span = tracer.start_span("request")
    # ... process request ...
    span.end()
    exporter.export(span)  # Blocks for 10ms!
    return response

âœ… Good:
def handle_request():
    span = tracer.start_span("request")
    # ... process request ...
    span.end()
    exporter.export_async(span)  # Non-blocking
    return response
```

### âŒ **3. Inconsistent Trace IDs**

**Problem:** Different services use different trace ID formats

**Solution:** Use W3C Trace Context standard

```
âŒ Bad:
Service A: trace_id="abc-123" (custom format)
Service B: trace_id="xyz-456" (different format)
â†’ Can't correlate traces

âœ… Good:
All services: traceparent header (W3C standard)
â†’ Consistent trace correlation
```

---

## Trade-offs Summary

| Aspect                     | What You Gain            | What You Sacrifice                 |
|----------------------------|--------------------------|------------------------------------|
| **End-to-End Visibility**  | Complete request journey | Storage cost, performance overhead |
| **Sampling**               | Cost control             | May miss rare errors               |
| **Auto-Instrumentation**   | Zero code changes        | Less control, potential overhead   |
| **Manual Instrumentation** | Full control             | More code, maintenance             |
| **Async Export**           | Low performance impact   | Potential span loss on crash       |

---

## References

- **OpenTelemetry Documentation:** [https://opentelemetry.io/docs/](https://opentelemetry.io/docs/)
- **Jaeger Documentation:** [https://www.jaegertracing.io/docs/](https://www.jaegertracing.io/docs/)
- **Zipkin Documentation:** [https://zipkin.io/](https://zipkin.io/)
- **W3C Trace Context:** [https://www.w3.org/TR/trace-context/](https://www.w3.org/TR/trace-context/)
- **Related Chapters:**
    - [2.4.2 Observability](./2.4.2-observability.md) - High-level observability concepts
    - [2.4.3 Prometheus & Grafana Deep Dive](./2.4.3-prometheus-grafana-deep-dive.md) - Metrics collection
    - [2.4.5 ELK Stack & Logging Deep Dive](./2.4.5-elk-stack-logging-deep-dive.md) - Log aggregation

---

## âœï¸ Design Challenge

### Problem

You are designing a distributed tracing system for a microservices platform with 100 services that must:

1. **Track requests** across 100 services (end-to-end visibility)
2. **Handle 1M requests per second** (high volume)
3. **Store traces for 7 days** (compliance requirement)
4. **Support trace search** (find traces by service, error, duration)
5. **Minimal performance impact** (<0.1% latency overhead)
6. **Correlate with logs** (trace_id in logs)

**Constraints:**

- Services use multiple languages (Go, Java, Python, Node.js)
- Need to support both head-based and tail-based sampling
- Cost-sensitive (prefer open-source)
- Must handle traffic spikes (10Ã— normal traffic)

Design a distributed tracing strategy that:

- Handles high request volume
- Stores traces efficiently
- Enables fast trace search
- Minimizes performance impact
- Supports multiple languages
- Correlates with logs

### Solution

#### ðŸ§© Scenario

- **Services:** 100 microservices
- **Requests:** 1M requests/second
- **Retention:** 7 days
- **Languages:** Go, Java, Python, Node.js

**Calculations:**

- **Traces per Second:** 1M requests/sec Ã— 1% sampling = 10K traces/sec
- **Spans per Trace:** Average 10 spans per trace
- **Spans per Second:** 10K traces/sec Ã— 10 spans = 100K spans/sec
- **Storage:** 100K spans/sec Ã— 7 days Ã— 1KB/span = ~60 TB (compressed)

#### âœ… Step 1: Tracing Tool Choice

**Choice: OpenTelemetry + Jaeger**

**Why:**

- **OpenTelemetry:** Vendor-neutral, multi-language support
- **Jaeger:** Open-source, scalable, mature
- **Flexibility:** Can export to multiple backends
- **Cost:** Open-source (no licensing fees)

**Architecture:**

```
Applications â†’ OpenTelemetry SDK â†’ OpenTelemetry Collector â†’ Jaeger
```

#### âœ… Step 2: Instrumentation Strategy

**Auto-Instrumentation:**

```
For each language:
  - Go: OpenTelemetry Go SDK (auto-instrumentation)
  - Java: OpenTelemetry Java Agent (JVM instrumentation)
  - Python: OpenTelemetry Python (auto-instrumentation)
  - Node.js: OpenTelemetry Node.js (auto-instrumentation)

Benefits:
  - Zero code changes
  - Consistent instrumentation
  - Automatic context propagation
```

**Manual Instrumentation (for custom operations):**

```
Custom Business Logic:
  - Create spans for critical operations
  - Add business-specific tags
  - Record custom events
```

#### âœ… Step 3: Sampling Strategy

**Hybrid Sampling (Head-Based + Tail-Based):**

**Head-Based Sampling (API Gateway):**

```
Sampling Rate: 0.1% (1 in 1000 requests)
  - Reduces trace volume at edge
  - Consistent across all services
  - Low overhead
```

**Tail-Based Sampling (Collector):**

```
Keep traces if:
  - Has error (100% keep)
  - Duration > 500ms (100% keep)
  - Random 0.1% sample

Benefits:
  - Captures all errors
  - Captures slow traces
  - Maintains cost control
```

**Final Sampling:**

```
1M requests/sec Ã— 0.1% = 1K traces/sec (head-based)
1K traces/sec Ã— 10% (errors + slow) = 100 traces/sec (tail-based)
Total: ~100-200 traces/sec stored
```

#### âœ… Step 4: Context Propagation

**W3C Trace Context:**

```
HTTP Headers:
  traceparent: 00-4bf92f3577b34da6a3ce929d0e0e4736-00f067aa0ba902b7-01
  tracestate: (optional vendor-specific data)

Propagation:
  - API Gateway adds traceparent
  - Services extract and propagate traceparent
  - Consistent across all services
```

**Message Queue Propagation:**

```
Kafka Headers:
  traceparent: 00-4bf92f3577b34da6a3ce929d0e0e4736-00f067aa0ba902b7-01

Propagation:
  - Producer adds traceparent to message headers
  - Consumer extracts traceparent
  - Creates child span with same trace_id
```

#### âœ… Step 5: OpenTelemetry Collector

**Collector Architecture:**

```
Receivers:
  - OTLP Receiver (OpenTelemetry Protocol)
  - Jaeger Receiver (legacy support)
  - Zipkin Receiver (legacy support)

Processors:
  - Batch Processor (batch spans)
  - Tail Sampling Processor (tail-based sampling)
  - Attribute Processor (add/modify tags)

Exporters:
  - Jaeger Exporter (primary)
  - OTLP Exporter (backup)
```

**Collector Configuration:**

```yaml
receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317
      http:
        endpoint: 0.0.0.0:4318

processors:
  batch:
    timeout: 1s
    send_batch_size: 1000
  tail_sampling:
    policies:
      - name: error-policy
        type: status_code
        status_code:
          status_codes: [ ERROR ]
      - name: latency-policy
        type: latency
        latency:
          threshold_ms: 500
      - name: default-policy
        type: probabilistic
        probabilistic:
          sampling_percentage: 0.1

exporters:
  jaeger:
    endpoint: jaeger-collector:14250
    tls:
      insecure: false

service:
  pipelines:
    traces:
      receivers: [ otlp ]
      processors: [ batch, tail_sampling ]
      exporters: [ jaeger ]
```

#### âœ… Step 6: Jaeger Storage

**Storage Backend: Elasticsearch**

**Why:**

- **Scalability:** Handles 100K spans/sec
- **Search:** Fast trace search by tags
- **Retention:** Easy to configure (7 days)
- **Cost:** Open-source, self-hosted

**Jaeger Configuration:**

```
Storage: Elasticsearch
  - Index per day: jaeger-span-2024-01-15
  - Shards: 10 per index
  - Replicas: 1
  - Retention: 7 days (ILM policy)
```

**Storage Optimization:**

```
Index Lifecycle Management (ILM):
  - Hot: 1 day (fast storage, SSD)
  - Warm: 3 days (slower storage, HDD)
  - Cold: 3 days (archive storage)
  - Delete: >7 days

Compression:
  - Elasticsearch compression (default)
  - Reduces storage by ~50%
```

#### âœ… Step 7: Performance Optimization

**Async Export:**

```
OpenTelemetry SDK:
  - Export spans asynchronously
  - Don't block request processing
  - Batch spans before sending

Performance:
  - Span creation: <1 microsecond
  - Context propagation: <1 microsecond
  - Async export: 0 latency (background)
  - Total overhead: <0.1% (meets requirement)
```

**Batching:**

```
Collector Batching:
  - Batch size: 1000 spans
  - Batch timeout: 1 second
  - Reduces network overhead
  - Improves throughput
```

#### âœ… Step 8: Log Correlation

**Trace ID in Logs:**

```
Structured Logging:
  {
    "timestamp": "2024-01-15T10:30:00Z",
    "level": "ERROR",
    "message": "Database connection failed",
    "trace_id": "4bf92f3577b34da6a3ce929d0e0e4736",
    "span_id": "00f067aa0ba902b7",
    "service": "user-service"
  }
```

**Log-Trace Correlation:**

```
ELK Stack:
  - Logs stored in Elasticsearch
  - Trace ID indexed
  - Search logs by trace_id

Query:
  GET /logs/_search
  {
    "query": {
      "term": {
        "trace_id": "4bf92f3577b34da6a3ce929d0e0e4736"
      }
    }
  }
```

#### âœ… Complete Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Applications (100 Services)                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Go Service   â”‚  â”‚ Java Service â”‚  â”‚ Python Svc   â”‚  â”‚
â”‚  â”‚ (OTel SDK)   â”‚  â”‚ (OTel Agent) â”‚  â”‚ (OTel SDK)   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                  â”‚                  â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   OpenTelemetry Collector              â”‚
        â”‚   (3 instances, load balanced)         â”‚
        â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
        â”‚   â”‚ OTLP Receiver                    â”‚ â”‚
        â”‚   â”‚ Batch Processor                  â”‚ â”‚
        â”‚   â”‚ Tail Sampling Processor          â”‚ â”‚
        â”‚   â”‚ Jaeger Exporter                  â”‚ â”‚
        â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Jaeger Collector                     â”‚
        â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
        â”‚   â”‚ Receives spans from OTel         â”‚ â”‚
        â”‚   â”‚ Validates and processes          â”‚ â”‚
        â”‚   â”‚ Writes to Elasticsearch          â”‚ â”‚
        â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Elasticsearch Cluster                 â”‚
        â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
        â”‚   â”‚ Daily Indices (jaeger-span-*)    â”‚ â”‚
        â”‚   â”‚ ILM Policy (7-day retention)     â”‚ â”‚
        â”‚   â”‚ Shards: 10 per index             â”‚ â”‚
        â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Jaeger Query + UI                     â”‚
        â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
        â”‚   â”‚ Trace Search API                 â”‚ â”‚
        â”‚   â”‚ Trace Visualization UI           â”‚ â”‚
        â”‚   â”‚ Service Dependency Graph         â”‚ â”‚
        â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Request Flow:**

```
1. Request â†’ API Gateway: Creates root span, adds traceparent header
2. API Gateway â†’ Service A: Propagates traceparent
3. Service A: Creates child span, processes request
4. Service A â†’ Service B: Propagates traceparent
5. Service B: Creates child span, processes request
6. Services â†’ OpenTelemetry Collector: Export spans (async)
7. Collector: Batches spans, applies tail sampling
8. Collector â†’ Jaeger Collector: Exports sampled spans
9. Jaeger Collector â†’ Elasticsearch: Stores spans
10. User â†’ Jaeger UI: Searches and visualizes traces
```

#### âš–ï¸ Trade-offs Summary

| Decision                  | What We Gain                    | What We Sacrifice                  |
|---------------------------|---------------------------------|------------------------------------|
| **OpenTelemetry**         | Vendor-neutral, multi-language  | Additional abstraction layer       |
| **Hybrid Sampling**       | Captures errors + controls cost | More complex configuration         |
| **Elasticsearch Storage** | Fast search, scalable           | Higher storage cost than Cassandra |
| **Async Export**          | Low performance impact          | Potential span loss on crash       |
| **Auto-Instrumentation**  | Zero code changes               | Less control over spans            |

#### âœ… Final Summary

**Distributed Tracing Strategy:**

- **Instrumentation:** OpenTelemetry (auto + manual)
- **Sampling:** Hybrid (0.1% head-based + tail-based for errors/slow)
- **Collector:** OpenTelemetry Collector (batching, tail sampling)
- **Storage:** Jaeger + Elasticsearch (7-day retention, ILM)
- **Correlation:** Trace ID in logs (ELK Stack)

**Performance:**

- **Request Volume:** 1M requests/sec (handled)
- **Trace Volume:** ~100-200 traces/sec stored (after sampling)
- **Performance Impact:** <0.1% latency overhead (meets requirement)
- **Storage:** ~30 TB (7 days, compressed)

**Result:**

- âœ… Tracks requests across 100 services
- âœ… Handles 1M requests/second
- âœ… Stores traces for 7 days
- âœ… Fast trace search (Elasticsearch)
- âœ… Minimal performance impact (<0.1%)
- âœ… Multi-language support (OpenTelemetry)
- âœ… Log correlation (trace_id in logs)

