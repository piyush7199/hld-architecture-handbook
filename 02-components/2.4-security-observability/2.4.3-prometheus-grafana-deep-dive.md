# 2.4.3 Prometheus & Grafana Deep Dive: Metrics Collection and Visualization

## Intuitive Explanation

Imagine you're running a factory with 100 machines. You need to know:
- **Which machines are running?** (health)
- **How fast are they producing?** (throughput)
- **Are any machines overheating?** (alerts)

**Prometheus** is like having sensors on every machine that continuously measure and record data. **Grafana** is like a control room dashboard that displays all these measurements in beautiful charts, so you can see trends, spot problems, and get alerts.

**In distributed systems:**
- **Prometheus:** Collects metrics from services (CPU, memory, request rate, latency)
- **Grafana:** Visualizes metrics in dashboards, creates alerts
- **Goal:** Understand system health, performance, and troubleshoot issues
- **Benefit:** Proactive monitoring, data-driven decisions, faster incident response

---

## In-Depth Analysis

### 1. What is Prometheus?

**Prometheus** is an open-source monitoring and alerting toolkit designed for reliability and scalability.

**Key Features:**
- **Pull-Based Scraping:** Prometheus pulls metrics from services (HTTP endpoints)
- **Time-Series Database:** Stores metrics with timestamps
- **PromQL:** Powerful query language for metrics
- **Multi-Dimensional Data Model:** Metrics identified by name + labels
- **Service Discovery:** Automatically discovers services to monitor

**Architecture:**
```
Prometheus Server:
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ Scrape Targets   ‚îÇ (HTTP endpoints)
  ‚îÇ Service Discovery‚îÇ (Kubernetes, Consul)
  ‚îÇ Time-Series DB   ‚îÇ (Local storage)
  ‚îÇ PromQL Engine    ‚îÇ (Query evaluation)
  ‚îÇ Alertmanager     ‚îÇ (Alert routing)
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 2. Prometheus Data Model

**Metrics Structure:**
```
Metric Name: http_requests_total
Labels: {method="GET", status="200", endpoint="/api/users"}
Value: 12345
Timestamp: 1699123456
```

**Metric Types:**

**Counter:**
```
http_requests_total{method="GET", status="200"} 12345
‚Üí Only increases (monotonically)
‚Üí Use case: Total requests, errors, bytes transferred
```

**Gauge:**
```
cpu_usage_percent{instance="server1"} 75.5
‚Üí Can go up or down
‚Üí Use case: CPU usage, memory, queue size, active connections
```

**Histogram:**
```
http_request_duration_seconds_bucket{le="0.1"} 1000
http_request_duration_seconds_bucket{le="0.5"} 5000
http_request_duration_seconds_bucket{le="1.0"} 8000
http_request_duration_seconds_bucket{le="+Inf"} 10000
‚Üí Buckets for distribution
‚Üí Use case: Latency distribution, request sizes
```

**Summary:**
```
http_request_duration_seconds{quantile="0.5"} 0.1
http_request_duration_seconds{quantile="0.95"} 0.5
http_request_duration_seconds{quantile="0.99"} 1.0
‚Üí Pre-calculated quantiles
‚Üí Use case: Latency percentiles (p50, p95, p99)
```

### 3. Prometheus Scraping

**Pull Model:**
```
Prometheus ‚Üí HTTP GET /metrics ‚Üí Service
‚Üí Service exposes metrics endpoint
‚Üí Prometheus scrapes at configured interval (15s default)
```

**Service Discovery:**
```
Kubernetes:
  - Prometheus discovers pods via Kubernetes API
  - Automatically scrapes new pods
  - Removes stopped pods

Consul:
  - Prometheus queries Consul for services
  - Scrapes services based on tags
```

**Scrape Configuration:**
```yaml
scrape_configs:
  - job_name: 'user-service'
    kubernetes_sd_configs:
      - role: pod
    relabel_configs:
      - source_labels: [__meta_kubernetes_pod_label_app]
        regex: user-service
        action: keep
```

### 4. PromQL (Prometheus Query Language)

**Basic Queries:**
```
# Get current CPU usage
cpu_usage_percent{instance="server1"}

# Get requests per second (rate)
rate(http_requests_total[5m])

# Get 95th percentile latency
histogram_quantile(0.95, http_request_duration_seconds_bucket)
```

**Aggregation:**
```
# Sum requests across all instances
sum(http_requests_total)

# Average CPU across all servers
avg(cpu_usage_percent)

# Count unique services
count(http_requests_total) by (service)
```

**Functions:**
```
# Rate of change
rate(http_requests_total[5m])

# Increase over time window
increase(http_requests_total[1h])

# Percentile calculation
histogram_quantile(0.95, http_request_duration_seconds_bucket)
```

### 5. Prometheus Storage

**Local Storage:**
```
Time-Series Database:
  - Efficient compression (1.3 bytes per sample)
  - Retention: 15 days default (configurable)
  - Blocks: Data organized in 2-hour blocks
  - WAL: Write-Ahead Log for durability
```

**Remote Storage:**
```
Long-Term Storage:
  - Prometheus ‚Üí Remote Write ‚Üí InfluxDB, TimescaleDB, Thanos
  - Extends retention beyond local storage
  - Enables historical analysis
```

**Storage Optimization:**
```
Recording Rules:
  - Pre-compute expensive queries
  - Store results as new metrics
  - Reduces query load

Example:
  - Rule: avg_over_time(cpu_usage[5m])
  - Result: New metric cpu_usage_5m_avg
```

### 6. Alertmanager

**Alert Rules:**
```yaml
groups:
  - name: high_cpu
    rules:
      - alert: HighCPUUsage
        expr: cpu_usage_percent > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High CPU usage on {{ $labels.instance }}"
```

**Alert Flow:**
```
1. Prometheus evaluates alert rules
2. Alert fires (condition met for duration)
3. Alertmanager receives alert
4. Alertmanager routes to channels (email, Slack, PagerDuty)
5. Engineer receives notification
```

**Alert Routing:**
```yaml
route:
  group_by: ['alertname', 'cluster']
  receiver: 'default'
  routes:
    - match:
        severity: critical
      receiver: 'pagerduty'
    - match:
        severity: warning
      receiver: 'slack'
```

### 7. What is Grafana?

**Grafana** is an open-source analytics and visualization platform for metrics, logs, and traces.

**Key Features:**
- **Dashboards:** Create visual dashboards from metrics
- **Data Sources:** Connect to Prometheus, InfluxDB, Elasticsearch, etc.
- **Alerting:** Create alerts from dashboard queries
- **Templating:** Dynamic dashboards with variables
- **Annotations:** Mark events on graphs

**Architecture:**
```
Grafana:
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ Data Sources    ‚îÇ (Prometheus, InfluxDB, etc.)
  ‚îÇ Dashboard Engine‚îÇ (Renders panels)
  ‚îÇ Alerting Engine ‚îÇ (Evaluates alert rules)
  ‚îÇ User Interface  ‚îÇ (Web UI)
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 8. Grafana Dashboards

**Panel Types:**

**Time Series:**
```
Shows metrics over time
‚Üí Line graphs, area charts
‚Üí Use case: Request rate, latency trends
```

**Stat:**
```
Single value display
‚Üí Current value, change from previous period
‚Üí Use case: Total requests, error rate
```

**Table:**
```
Tabular data
‚Üí Sortable columns, filtering
‚Üí Use case: Top endpoints by latency, service health
```

**Gauge:**
```
Circular gauge display
‚Üí Shows value within range
‚Üí Use case: CPU usage, memory usage
```

**Heatmap:**
```
2D visualization
‚Üí X-axis: time, Y-axis: buckets, Color: frequency
‚Üí Use case: Latency distribution over time
```

### 9. Grafana Querying

**Prometheus Queries in Grafana:**
```
# Query
rate(http_requests_total[5m])

# Legend
{{method}} - {{status}}

# Format
Time series
```

**Transformations:**
```
# Calculate difference
A - B

# Calculate percentage
(A / B) * 100

# Group by
Group by: service, method
```

### 10. Grafana Alerting

**Alert Rules:**
```
Condition:
  WHEN last() OF query(A, 5m, now) IS ABOVE 1000

Evaluation:
  - Every: 30s
  - For: 5m

Notifications:
  - Email
  - Slack
  - PagerDuty
```

**Alert States:**
```
- OK: Condition not met
- Pending: Condition met, waiting for duration
- Alerting: Condition met for duration
- NoData: No data available
- Error: Query error
```

### 11. Prometheus & Grafana Integration

**Complete Stack:**
```
Services ‚Üí Expose /metrics endpoint
    ‚Üì
Prometheus ‚Üí Scrapes metrics (every 15s)
    ‚Üì
Prometheus ‚Üí Stores in time-series DB
    ‚Üì
Grafana ‚Üí Queries Prometheus via PromQL
    ‚Üì
Grafana ‚Üí Displays in dashboards
    ‚Üì
Alertmanager ‚Üí Routes alerts
```

**Best Practices:**
```
1. Consistent Labeling:
   - Use standard labels (service, environment, instance)
   - Avoid high cardinality (too many unique label combinations)

2. Recording Rules:
   - Pre-compute expensive queries
   - Reduce load on Prometheus

3. Dashboard Organization:
   - Group related metrics
   - Use templating for multi-service dashboards

4. Alert Tuning:
   - Avoid alert fatigue
   - Use severity levels
   - Set appropriate thresholds
```

---

## When to Use Prometheus & Grafana

### ‚úÖ Use Prometheus & Grafana When:

1. **Metrics Monitoring:** Need to monitor system metrics (CPU, memory, latency)
2. **Time-Series Data:** Need to track metrics over time
3. **Alerting:** Need to alert on metric thresholds
4. **Multi-Service:** Monitoring multiple services/microservices
5. **Kubernetes:** Running containerized workloads (native integration)
6. **Cost-Effective:** Need open-source solution (vs. Datadog, New Relic)

### ‚ùå Don't Use Prometheus & Grafana When:

1. **Logs Only:** Need log aggregation (use ELK stack, Loki)
2. **Tracing Only:** Need distributed tracing (use Jaeger, Zipkin)
3. **Simple Monitoring:** Single server, basic monitoring (use simpler tools)
4. **Managed Solution:** Prefer fully managed (use Datadog, New Relic, CloudWatch)

---

## Real-World Examples

### Kubernetes Monitoring

**Use Case:** Monitor Kubernetes cluster and applications

**Architecture:**
- Prometheus Operator (manages Prometheus in Kubernetes)
- Service discovery (automatic pod discovery)
- Grafana dashboards (cluster metrics, pod metrics)

**Metrics:**
- Cluster: CPU, memory, pod count
- Applications: Request rate, latency, error rate

### Microservices Monitoring

**Use Case:** Monitor 50+ microservices

**Architecture:**
- Each service exposes /metrics endpoint
- Prometheus scrapes all services
- Grafana dashboards per service + aggregate

**Metrics:**
- Per-service: QPS, latency, error rate
- Aggregate: Total requests, system health

---

## Prometheus & Grafana vs. Other Solutions

| Solution | Best For | Cost | Complexity | Features |
|----------|----------|------|------------|----------|
| **Prometheus + Grafana** | Open-source, Kubernetes | Free | Medium | Metrics, alerting |
| **Datadog** | Managed, full-stack | Paid | Low | Metrics, logs, traces, APM |
| **New Relic** | Managed, APM | Paid | Low | Metrics, logs, traces, APM |
| **CloudWatch** | AWS-native | Paid | Low | Metrics, logs (AWS only) |
| **InfluxDB + Grafana** | Time-series focus | Free/Paid | Medium | Time-series, metrics |

---

## Common Anti-Patterns

### ‚ùå **1. High Cardinality Labels**

**Problem:** Too many unique label combinations

**Solution:** Limit label cardinality

```
‚ùå Bad:
http_requests_total{user_id="123", request_id="abc", timestamp="..."}
‚Üí Millions of unique time series

‚úÖ Good:
http_requests_total{method="GET", status="200", endpoint="/api/users"}
‚Üí Limited, meaningful labels
```

### ‚ùå **2. No Recording Rules**

**Problem:** Expensive queries run repeatedly

**Solution:** Use recording rules

```
‚ùå Bad:
Dashboard queries: rate(http_requests_total[5m]) * 60
‚Üí Calculated on every dashboard load

‚úÖ Good:
Recording rule: requests_per_minute = rate(http_requests_total[5m]) * 60
Dashboard queries: requests_per_minute
‚Üí Pre-computed, faster
```

### ‚ùå **3. Alert Fatigue**

**Problem:** Too many alerts, all ignored

**Solution:** Tune alert thresholds, use severity levels

```
‚ùå Bad:
Alert: CPU > 50% (fires constantly)
Alert: CPU > 60% (fires constantly)
Alert: CPU > 70% (fires constantly)

‚úÖ Good:
Alert: CPU > 80% for 5m (warning)
Alert: CPU > 90% for 2m (critical)
‚Üí Meaningful thresholds, different severities
```

---

## Trade-offs Summary

| Aspect | What You Gain | What You Sacrifice |
|--------|---------------|-------------------|
| **Pull Model** | Simple, reliable | Can't monitor ephemeral services easily |
| **Local Storage** | Fast queries | Limited retention (15 days default) |
| **Open Source** | Free, customizable | Operational overhead |
| **PromQL** | Powerful queries | Learning curve |
| **Grafana Dashboards** | Rich visualization | Dashboard creation time |

---

## References

- **Prometheus Documentation:** [https://prometheus.io/docs/](https://prometheus.io/docs/)
- **Grafana Documentation:** [https://grafana.com/docs/](https://grafana.com/docs/)
- **PromQL Guide:** [https://prometheus.io/docs/prometheus/latest/querying/basics/](https://prometheus.io/docs/prometheus/latest/querying/basics/)
- **Related Chapters:**
  - [2.4.2 Observability](./2.4.2-observability.md) - High-level observability concepts
  - [2.1.17 Time Series Databases Deep Dive](../2.1-databases/2.1.17-time-series-databases-deep-dive.md) - Time-series storage

---

## ‚úèÔ∏è Design Challenge

### Problem

You are designing a monitoring system for a microservices platform with 50 services that must:

1. **Collect metrics** from 50 services (CPU, memory, request rate, latency)
2. **Store metrics** for 90 days (long-term analysis)
3. **Visualize metrics** in dashboards (per-service and aggregate)
4. **Alert on anomalies** (high CPU, high latency, high error rate)
5. **Handle 1M metrics/minute** (high volume)
6. **Support service discovery** (services start/stop frequently)

**Constraints:**
- Services run in Kubernetes
- Services expose /metrics endpoint (Prometheus format)
- Need sub-second query latency for dashboards
- Alert latency: <1 minute (detect issues quickly)

Design a Prometheus & Grafana strategy that:
- Handles metric collection at scale
- Stores metrics efficiently
- Provides fast dashboard queries
- Enables effective alerting
- Supports service discovery

### Solution

#### üß© Scenario

- **Services:** 50 microservices
- **Metrics:** 1M metrics/minute = 16,667 metrics/second
- **Retention:** 90 days
- **Services:** Kubernetes pods (dynamic)

**Calculations:**
- **Metrics per Service:** 1M / 50 = 20K metrics/minute per service
- **Storage:** 1M metrics/min √ó 90 days √ó 1.3 bytes = ~170 GB (compressed)
- **Scrape Interval:** 15 seconds (standard)

#### ‚úÖ Step 1: Prometheus Architecture

**Choice: Prometheus Federation + Remote Storage**

**Why:**
- **Federation:** Distributes load across multiple Prometheus instances
- **Remote Storage:** Extends retention beyond local storage
- **High Availability:** Multiple Prometheus instances (no single point of failure)

**Architecture:**
```
Prometheus Cluster:
  - 3 Prometheus Instances (federation)
  - Each scrapes 17 services (50 / 3)
  - Remote write to Thanos (long-term storage)
```

#### ‚úÖ Step 2: Service Discovery

**Kubernetes Service Discovery:**
```yaml
scrape_configs:
  - job_name: 'kubernetes-pods'
    kubernetes_sd_configs:
      - role: pod
    relabel_configs:
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        regex: "true"
        action: keep
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
        regex: (.+)
        target_label: __metrics_path__
      - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        target_label: __address__
```

**Automatic Discovery:**
```
Kubernetes Pods:
  - Annotation: prometheus.io/scrape: "true"
  - Annotation: prometheus.io/port: "8080"
  - Annotation: prometheus.io/path: "/metrics"
  
Prometheus:
  - Discovers pods automatically
  - Scrapes /metrics endpoint
  - Removes stopped pods automatically
```

#### ‚úÖ Step 3: Metric Collection

**Scrape Configuration:**
```
Scrape Interval: 15 seconds
Scrape Timeout: 10 seconds
Targets: 50 services (discovered automatically)

Per Service:
  - CPU usage
  - Memory usage
  - Request rate (QPS)
  - Request latency (p50, p95, p99)
  - Error rate
```

**Metric Cardinality:**
```
Per Service Metrics:
  - http_requests_total{method, status, endpoint} (~100 unique combinations)
  - http_request_duration_seconds{method, endpoint} (~50 unique combinations)
  - cpu_usage_percent (1)
  - memory_usage_bytes (1)
  
Total: ~150 time series per service
50 services √ó 150 = 7,500 active time series
```

#### ‚úÖ Step 4: Storage Strategy

**Local Storage (Prometheus):**
```
Retention: 15 days (default)
Compression: 1.3 bytes per sample
Storage: ~50 GB (15 days of data)

Purpose: Fast queries for recent data
```

**Remote Storage (Thanos):**
```
Thanos Architecture:
  - Thanos Sidecar (attached to each Prometheus)
  - Thanos Store (long-term storage, S3)
  - Thanos Query (unified query interface)

Retention: 90 days
Storage: ~170 GB (S3)

Purpose: Long-term retention, historical analysis
```

**Storage Optimization:**
```
Recording Rules:
  - Pre-compute expensive queries
  - Store as new metrics
  - Reduces query load

Examples:
  - requests_per_second = rate(http_requests_total[5m])
  - error_rate = rate(http_requests_total{status="5xx"}[5m]) / rate(http_requests_total[5m])
```

#### ‚úÖ Step 5: Grafana Dashboards

**Dashboard Structure:**
```
1. System Overview Dashboard:
   - Total requests/second (all services)
   - Total error rate
   - System health (up/down services)

2. Per-Service Dashboards:
   - Request rate (QPS)
   - Latency (p50, p95, p99)
   - Error rate
   - CPU/Memory usage

3. Alert Dashboard:
   - Active alerts
   - Alert history
   - Alert trends
```

**Dashboard Templating:**
```
Variables:
  - service: Dropdown (all services)
  - environment: Dropdown (dev, staging, prod)

Queries:
  - rate(http_requests_total{service="$service"}[5m])
  - histogram_quantile(0.95, http_request_duration_seconds_bucket{service="$service"})
```

#### ‚úÖ Step 6: Alerting

**Alert Rules:**
```yaml
groups:
  - name: service_alerts
    rules:
      - alert: HighErrorRate
        expr: |
          rate(http_requests_total{status=~"5.."}[5m]) / 
          rate(http_requests_total[5m]) > 0.05
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High error rate on {{ $labels.service }}"
          
      - alert: HighLatency
        expr: |
          histogram_quantile(0.95, 
            http_request_duration_seconds_bucket{service="{{ $labels.service }}"}) > 1
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High latency on {{ $labels.service }}"
          
      - alert: ServiceDown
        expr: up{job="kubernetes-pods"} == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Service {{ $labels.service }} is down"
```

**Alert Routing:**
```yaml
route:
  group_by: ['alertname', 'service', 'severity']
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 12h
  receiver: 'default'
  routes:
    - match:
        severity: critical
      receiver: 'pagerduty'
    - match:
        severity: warning
      receiver: 'slack'
```

#### ‚úÖ Step 7: Performance Optimization

**Query Optimization:**
```
Recording Rules:
  - Pre-compute: rate(http_requests_total[5m])
  - Store as: http_requests_per_second
  - Dashboard queries: http_requests_per_second (faster)

Caching:
  - Grafana query caching (30s TTL)
  - Prometheus query caching (for repeated queries)
```

**Scrape Optimization:**
```
Scrape Interval: 15s (standard)
Scrape Timeout: 10s
Parallel Scrapes: 50 (one per service)

Total Scrape Time: <10s (all services scraped in parallel)
```

#### ‚úÖ Complete Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ          Kubernetes Cluster (50 Services)               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ Service 1    ‚îÇ  ‚îÇ Service 2    ‚îÇ  ‚îÇ Service 50   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ /metrics     ‚îÇ  ‚îÇ /metrics     ‚îÇ  ‚îÇ /metrics     ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ                  ‚îÇ                  ‚îÇ
          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ   Prometheus Cluster (3 instances)      ‚îÇ
        ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
        ‚îÇ  ‚îÇ Prometheus 1 (17 services)        ‚îÇ  ‚îÇ
        ‚îÇ  ‚îÇ Prometheus 2 (17 services)        ‚îÇ  ‚îÇ
        ‚îÇ  ‚îÇ Prometheus 3 (16 services)        ‚îÇ  ‚îÇ
        ‚îÇ  ‚îÇ - Service Discovery (K8s)         ‚îÇ  ‚îÇ
        ‚îÇ  ‚îÇ - Scraping (15s interval)        ‚îÇ  ‚îÇ
        ‚îÇ  ‚îÇ - Local Storage (15 days)        ‚îÇ  ‚îÇ
        ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ   Thanos (Long-Term Storage)            ‚îÇ
        ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
        ‚îÇ  ‚îÇ Thanos Sidecar (per Prometheus)  ‚îÇ  ‚îÇ
        ‚îÇ  ‚îÇ Thanos Store (S3 backend)        ‚îÇ  ‚îÇ
        ‚îÇ  ‚îÇ Thanos Query (unified queries)   ‚îÇ  ‚îÇ
        ‚îÇ  ‚îÇ Retention: 90 days               ‚îÇ  ‚îÇ
        ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ   Grafana                                ‚îÇ
        ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
        ‚îÇ  ‚îÇ Data Sources:                     ‚îÇ  ‚îÇ
        ‚îÇ  ‚îÇ - Prometheus (recent data)        ‚îÇ  ‚îÇ
        ‚îÇ  ‚îÇ - Thanos Query (historical data) ‚îÇ  ‚îÇ
        ‚îÇ  ‚îÇ Dashboards:                       ‚îÇ  ‚îÇ
        ‚îÇ  ‚îÇ - System Overview                 ‚îÇ  ‚îÇ
        ‚îÇ  ‚îÇ - Per-Service                     ‚îÇ  ‚îÇ
        ‚îÇ  ‚îÇ - Alerts                          ‚îÇ  ‚îÇ
        ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ   Alertmanager                          ‚îÇ
        ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
        ‚îÇ  ‚îÇ Alert Rules (Prometheus)          ‚îÇ  ‚îÇ
        ‚îÇ  ‚îÇ Alert Routing                     ‚îÇ  ‚îÇ
        ‚îÇ  ‚îÇ - Critical ‚Üí PagerDuty            ‚îÇ  ‚îÇ
        ‚îÇ  ‚îÇ - Warning ‚Üí Slack                 ‚îÇ  ‚îÇ
        ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Query Flow:**
```
1. Grafana Dashboard ‚Üí Query: rate(http_requests_total[5m])
2. Grafana ‚Üí Prometheus (if <15 days) OR Thanos Query (if >15 days)
3. Prometheus/Thanos ‚Üí Evaluates PromQL query
4. Returns time series data
5. Grafana ‚Üí Renders graph
```

#### ‚öñÔ∏è Trade-offs Summary

| Decision | What We Gain | What We Sacrifice |
|----------|--------------|-------------------|
| **Prometheus Federation** | Load distribution, HA | More complex architecture |
| **Thanos Remote Storage** | 90-day retention | Additional infrastructure |
| **15s Scrape Interval** | Real-time monitoring | Higher storage/network usage |
| **Recording Rules** | Faster queries | Additional storage |
| **Kubernetes Service Discovery** | Automatic monitoring | Kubernetes dependency |

#### ‚úÖ Final Summary

**Prometheus & Grafana Strategy:**
- **Prometheus:** 3 instances (federation, HA)
- **Service Discovery:** Kubernetes (automatic)
- **Storage:** Local (15 days) + Thanos (90 days, S3)
- **Grafana:** Dashboards (system, per-service, alerts)
- **Alerting:** Alertmanager (PagerDuty, Slack)

**Performance:**
- **Metric Collection:** 1M metrics/minute (handled by 3 Prometheus instances)
- **Query Latency:** <1 second (recording rules, caching)
- **Alert Latency:** <1 minute (15s scrape + 5m alert duration)
- **Storage:** ~170 GB (90 days, compressed)

**Result:**
- ‚úÖ Collects metrics from 50 services
- ‚úÖ Stores metrics for 90 days
- ‚úÖ Fast dashboard queries (<1s)
- ‚úÖ Effective alerting (<1m latency)
- ‚úÖ Automatic service discovery
- ‚úÖ Handles 1M metrics/minute

