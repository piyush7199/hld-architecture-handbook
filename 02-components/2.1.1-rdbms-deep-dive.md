# 2.1.1 RDBMS Deep Dive: SQL and the ACID Guarantees

## Intuitive Explanation

Relational Database Management Systems (RDBMS) like PostgreSQL and MySQL are like a highly organized, trustworthy
accountant's ledger. They use tables (like spreadsheets) linked by relationships. Their core promise is trust and
consistency, enforced by the ACID properties.

- SQL (Structured Query Language): The universal language for talking to these databases.
- ACID: The absolute guarantee that your data is always valid, even if the power goes out mid-transaction. This makes
  them ideal for financial systems and inventory management.

---

## In-Depth Analysis

### 1. Relational Model and Joins

RDBMS stores data in normalized tables, meaning data is split into multiple tables to eliminate redundancy (e.g.,
separating user details from order details).

- **Normalization:** Reduces data duplication, saving space and ensuring that updates only need to happen in one place.
- **Joins:** Used to reconstruct the complete data (User + Order + Item) by linking tables using Foreign Keys. Joins are
  powerful but can become very slow at scale.

### 2. The ACID Properties (The Trust Contract)

ACID is an acronym defining the properties of database transactions, ensuring data remains valid despite errors,
failures, or concurrent access.

| Property    | Definition                                                                                                                              | Intuition                                                                                                                  |
|-------------|-----------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------|
| Atomicity   | A transaction is treated as a single, indivisible unit. Either all of the changes succeed, or none of them do.                          | If you transfer money, the deduction and the credit must both succeed; otherwise, the whole transfer is cancelled.         |
| Consistency | The database must move from one valid state to another valid state. Transactions that violate database rules (constraints) are aborted. | You cannot write a negative number into an inventory column that is defined as unsigned.                                   |
| Isolation   | Concurrent transactions must appear to execute sequentially. One transaction's intermediate state is hidden from others.                | Two users trying to buy the last iPhone see the stock count of 1 sequentially, not simultaneously, preventing overselling. |
| Durability  | Once a transaction is committed, it remains permanently recorded, even if the system crashes or loses power immediately after.          | The confirmation receipt you get for your order means the change is written to persistent storage (usually the disk).      |

### 3. Transactions and Isolation Levels

A Transaction is a sequence of database operations grouped together as a single logical unit of work. Isolation is
enforced through Isolation Levels:

- **Read Uncommitted**:
    - **Definition**: Transactions can read uncommitted changes from other transactions.
    - **How It Works**: Allows "`dirty reads`", where a transaction sees uncommitted data that may be rolled back.
    - **Examples**: Rarely used; supported in MySQL, SQL Server.
    - **Use Cases**: Non-critical reporting where speed is prioritized.
    - **Issues**: High risk of inconsistent data.
    - **Trade-offs**:
        - **Pros**: Minimal locking, high concurrency.
        - **Cons**: Risk of reading incorrect data.

- **Read Committed**:
    - **Definition**: Transactions only read committed data, but non-repeatable reads and phantom reads are possible.
    - **How It Works**: Ensures no dirty reads, but data may change between reads within a transaction.
    - **Examples**: Default in PostgreSQL, SQL Server, Oracle.
    - **Use Cases**: General-purpose applications (e.g., user profile updates).
    - **Issues**:
        - **Non-repeatable reads**: Data read twice in a transaction may differ.
        - **Phantom reads**: New rows may appear during a transaction.
    - **Trade-offs**:
        - **Pros**: Balances consistency and performance.
        - **Cons**: Inconsistent reads within a transaction.

- **Repeatable Read**:
    - **Definition**: Ensures consistent reads within a transaction, but phantom reads are possible.
    - **How It Works**: Locks read data to prevent updates by other transactions.
    - **Examples**: Default in MySQL (InnoDB); supported in PostgreSQL.
    - **Use Cases**: Applications needing stable reads (e.g., financial reports).
    - **Issues**: Phantom reads (new rows inserted by other transactions).
    - **Trade-offs**:
        - **Pros**: Stronger consistency than Read Committed.
        - **Cons**: Increased locking, potential deadlocks.

- **Serializable**:
    - **Definition**: Transactions execute as if run sequentially, preventing all anomalies.
    - **How It Works**: Uses strict locking or multiversion concurrency control (MVCC) to ensure complete isolation.
    - **Examples**: Supported in PostgreSQL, MySQL, Oracle.
    - **Use Cases**: Critical systems (e.g., banking, inventory control).
    - **Issues**: Lowest concurrency due to heavy locking.
    - **Trade-offs**:
        - **Pros**: Maximum consistency, no anomalies.
        - **Cons**: High latency, risk of deadlocks, reduced throughput.

### 4. Common Anomalies

- **Dirty Reads**: Reading uncommitted data that may be rolled back.
- **Non-repeatable Reads**: Data changes between reads within a transaction.
- **Phantom Reads**: New rows appear during a transaction due to inserts by other transactions.
- **Lost Updates**: Concurrent transactions overwrite each other’s changes.

### 5. Implementation Techniques

- **Locking**: Uses shared (read) or exclusive (write) locks to enforce isolation.
    - Example: MySQL’s Repeatable Read uses row-level locks.
- **Multiversion Concurrency Control (MVCC)**:
    - Creates data snapshots for transactions to read consistent versions.
    - Example: PostgreSQL uses MVCC for Read Committed and Repeatable Read.
- **Trade-offs**:
    - Locking: Simple but risks deadlocks and contention.
    - MVCC: Improves concurrency but increases storage for versioned data.

---

## 🔒 Concurrency Control: Optimistic vs Pessimistic Locking

### Intuitive Explanation

Imagine two people trying to edit the same Google Doc:
- **Pessimistic Locking**: Lock the document when you start editing. Others can't edit until you're done.
- **Optimistic Locking**: Everyone can edit. When you save, check if someone else changed it. If yes, resolve the conflict.

### Pessimistic Locking (Lock-Based)

**How It Works:**
- Acquire a lock **before** reading/modifying data
- Hold the lock during the entire transaction
- Release the lock when done
- Other transactions wait if they need the same data

**Database Implementation:**
```sql
-- PostgreSQL: SELECT FOR UPDATE
BEGIN;
SELECT * FROM inventory 
WHERE product_id = 123 
FOR UPDATE;  -- Acquires exclusive lock

-- Do some processing (lock is held)
UPDATE inventory 
SET quantity = quantity - 1 
WHERE product_id = 123;

COMMIT;  -- Lock released
```

**When to Use:**
- High contention (many concurrent updates to same data)
- Critical data that must be consistent (bank accounts, inventory)
- Read-modify-write operations
- Short transactions (< 100ms)

**Pros:**
- ✅ Prevents conflicts (no lost updates)
- ✅ Simpler to reason about
- ✅ Guaranteed consistency

**Cons:**
- ❌ Reduced concurrency (blocking)
- ❌ Risk of deadlocks
- ❌ Performance bottleneck under high load
- ❌ Can cause cascading waits

---

### Optimistic Locking (Version-Based)

**How It Works:**
- Read data **without** acquiring a lock
- Include a version number or timestamp
- When updating, check if version changed
- If unchanged, update and increment version
- If changed, transaction fails (retry or abort)

**Database Implementation:**

**Option 1: Version Column**
```sql
-- Read with current version
SELECT product_id, quantity, version 
FROM inventory 
WHERE product_id = 123;
-- Result: quantity = 10, version = 5

-- Later, update only if version unchanged
UPDATE inventory 
SET quantity = 9, version = version + 1
WHERE product_id = 123 AND version = 5;

-- Check affected rows
-- If 0 rows affected → someone else updated it → conflict!
-- If 1 row affected → success!
```

**Option 2: Timestamp Column**
```sql
SELECT product_id, quantity, updated_at 
FROM inventory 
WHERE product_id = 123;

UPDATE inventory 
SET quantity = 9, updated_at = NOW()
WHERE product_id = 123 
  AND updated_at = :previous_timestamp;
```

**Application-Level Check:**
```python
# Read data
product = db.execute("SELECT * FROM inventory WHERE id = 123")
original_version = product['version']

# Do some business logic (no lock held)
new_quantity = calculate_new_quantity(product['quantity'])

# Try to update with version check
result = db.execute("""
    UPDATE inventory 
    SET quantity = ?, version = version + 1
    WHERE id = 123 AND version = ?
""", new_quantity, original_version)

if result.rowcount == 0:
    # Conflict! Someone else updated it
    # Retry or return error to user
    raise ConflictError("Product was modified by another transaction")
else:
    # Success!
    db.commit()
```

**When to Use:**
- Low to medium contention
- Long-running transactions (user thinking time)
- Read-heavy workloads
- Distributed systems (no centralized lock manager)
- Web applications (stateless HTTP)

**Pros:**
- ✅ Higher concurrency (no blocking)
- ✅ No deadlocks
- ✅ Better scalability
- ✅ Works well with stateless protocols (HTTP)

**Cons:**
- ❌ Conflicts possible (retry logic needed)
- ❌ Wasted work if conflict occurs
- ❌ Not suitable for high contention
- ❌ User experience issue (save fails)

---

### Detailed Comparison Matrix

| Aspect | Pessimistic Locking | Optimistic Locking |
|--------|-------------------|-------------------|
| **Lock Acquisition** | Before read/update | No lock (version check at update) |
| **Blocking** | Yes (other transactions wait) | No (concurrent reads) |
| **Concurrency** | Low (serialized access) | High (parallel reads) |
| **Deadlock Risk** | Yes | No |
| **Best For** | High contention, short transactions | Low contention, long transactions |
| **Conflict Handling** | Prevented (blocking) | Detected (retry) |
| **Performance** | Good for high contention | Good for low contention |
| **User Experience** | Slower (waiting for lock) | Fast (but may fail on save) |
| **Implementation** | `SELECT FOR UPDATE` | Version column + conditional update |
| **Typical Use Cases** | Banking, inventory, booking systems | CMS, e-commerce carts, user profiles |

---

### Isolation Levels and Locking Behavior

| Isolation Level | Default Locking Behavior | Read Locks | Write Locks | Anomalies Prevented |
|----------------|-------------------------|-----------|------------|---------------------|
| **Read Uncommitted** | Minimal locking | None | Short duration | None (dirty reads possible) |
| **Read Committed** | Statement-level locking | Released after statement | Held until commit | Dirty reads |
| **Repeatable Read** | Transaction-level locking | Held until commit | Held until commit | Dirty reads, non-repeatable reads |
| **Serializable** | Full serialization | Held until commit (range locks) | Held until commit | All anomalies |

---

### Real-World Examples

#### Example 1: E-commerce Inventory (Pessimistic)

**Problem:** Multiple users buying last item in stock

```sql
-- User A and User B both want to buy product 123
-- Only 1 item left in stock

-- User A's transaction (gets lock first)
BEGIN;
SELECT quantity FROM inventory 
WHERE product_id = 123 
FOR UPDATE;  -- Acquires lock, quantity = 1

-- User B's transaction (must wait)
BEGIN;
SELECT quantity FROM inventory 
WHERE product_id = 123 
FOR UPDATE;  -- Blocked! Waiting for User A...

-- User A completes purchase
UPDATE inventory SET quantity = 0 WHERE product_id = 123;
COMMIT;  -- Lock released

-- Now User B's SELECT returns
-- quantity = 0, so purchase fails (out of stock)
ROLLBACK;
```

**Result:** No overselling! Pessimistic locking prevents race condition.

---

#### Example 2: Blog Post Editing (Optimistic)

**Problem:** Two editors updating same blog post

```python
# Editor A
post = db.query("SELECT * FROM posts WHERE id = 1")
# post = {id: 1, title: "Hello", content: "...", version: 5}

# Editor A makes changes (takes 10 minutes)
new_title = "Hello World"

# Meanwhile, Editor B also edits
post_b = db.query("SELECT * FROM posts WHERE id = 1")
# post_b = {id: 1, title: "Hello", content: "...", version: 5}

# Editor B saves first (changes content)
db.execute("""
    UPDATE posts 
    SET content = 'New content', version = 6
    WHERE id = 1 AND version = 5
""")
# Success! version is now 6

# Editor A tries to save (changes title)
result = db.execute("""
    UPDATE posts 
    SET title = 'Hello World', version = 6
    WHERE id = 1 AND version = 5
""")
# Fails! 0 rows affected (version is now 6, not 5)

# Application shows error:
# "This post was modified by another user. Please refresh and try again."
```

**Result:** Conflict detected! Editor A must refresh and re-apply changes.

---

#### Example 3: Hybrid Approach (Pessimistic for Critical, Optimistic for Others)

```python
# E-commerce order processing
def process_order(order_id, product_id, quantity):
    # Use pessimistic locking for inventory (critical!)
    db.execute("BEGIN")
    
    inventory = db.execute("""
        SELECT quantity FROM inventory 
        WHERE product_id = ? 
        FOR UPDATE
    """, product_id)
    
    if inventory['quantity'] < quantity:
        db.execute("ROLLBACK")
        raise OutOfStockError()
    
    # Deduct inventory (lock held)
    db.execute("""
        UPDATE inventory 
        SET quantity = quantity - ?
        WHERE product_id = ?
    """, quantity, product_id)
    
    # Use optimistic locking for user profile (less critical)
    user = db.execute("""
        SELECT loyalty_points, version 
        FROM users 
        WHERE user_id = ?
    """, order['user_id'])
    
    # Add loyalty points
    result = db.execute("""
        UPDATE users 
        SET loyalty_points = loyalty_points + ?,
            version = version + 1
        WHERE user_id = ? AND version = ?
    """, points, order['user_id'], user['version'])
    
    if result.rowcount == 0:
        # Conflict on user profile - acceptable, just skip points
        log.warning("Loyalty points conflict, skipping")
    
    db.execute("COMMIT")
```

---

### Choosing Between Optimistic and Pessimistic Locking

```
                    High Contention?
                          |
                    Yes ──┴── No
                    |            |
             Pessimistic    Low Contention?
              Locking            |
                           Yes ──┴── No
                           |            |
                      Optimistic   Consider Both
                       Locking
                       
Short Transaction? (<100ms)
├─ Yes → Pessimistic (lock overhead minimal)
└─ No → Optimistic (don't hold locks long)

Critical Data? (money, inventory)
├─ Yes → Pessimistic (prevent conflicts)
└─ No → Optimistic (accept occasional conflicts)

Distributed System?
├─ Yes → Optimistic (no central lock manager)
└─ No → Either works
```

---

### Lock Types in SQL

| Lock Type | SQL Syntax | Purpose | Scope |
|----------|-----------|---------|-------|
| **Shared Lock (S)** | `SELECT ... FOR SHARE` | Read lock, allows other reads | Row/Table |
| **Exclusive Lock (X)** | `SELECT ... FOR UPDATE` | Write lock, blocks all other access | Row/Table |
| **Update Lock (U)** | Automatic in some DBs | Intent to update, prevents deadlocks | Row |
| **Intent Locks** | Automatic | Indicates intention to lock at finer granularity | Table/Page |
| **Row-Level Lock** | `FOR UPDATE` | Locks specific rows | Row |
| **Table-Level Lock** | `LOCK TABLE` | Locks entire table | Table |

---

### Deadlock Example and Prevention

**Deadlock Scenario:**
```sql
-- Transaction 1
BEGIN;
UPDATE accounts SET balance = balance - 100 WHERE id = 1;  -- Locks account 1
-- ... waiting ...
UPDATE accounts SET balance = balance + 100 WHERE id = 2;  -- Needs lock on account 2

-- Transaction 2 (concurrent)
BEGIN;
UPDATE accounts SET balance = balance - 50 WHERE id = 2;   -- Locks account 2
-- ... waiting ...
UPDATE accounts SET balance = balance + 50 WHERE id = 1;   -- Needs lock on account 1

-- DEADLOCK! Each transaction waiting for the other
```

**Prevention Strategies:**
1. **Lock Ordering**: Always acquire locks in same order (e.g., by account ID)
```sql
-- Both transactions lock in ascending ID order
UPDATE accounts SET balance = ... WHERE id = 1;  -- Lock 1 first
UPDATE accounts SET balance = ... WHERE id = 2;  -- Then lock 2
```

2. **Lock Timeout**: Set timeout for lock acquisition
```sql
SET lock_timeout = '5s';
-- Transaction aborts if can't acquire lock in 5 seconds
```

3. **Deadlock Detection**: DB automatically detects and aborts one transaction

---

## 💡 Use Cases and Scaling Trade-offs

| Use Case                    | Rationale                                                               | Scaling Challenge                                                                                                                               |
|-----------------------------|-------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------|
| Financial/Banking Ledgers   | Requires absolute ACID compliance (especially Atomicity and Isolation). | **High Write Contention:** Concurrent updates to bank balances or inventory require heavy locking, severely limiting write throughput.          |
| User Authentication/Profile | Strong consistency is required for password and core profile data.      | **Vertical Scaling:** Traditional RDBMS scales primarily by getting a bigger server (vertical scaling), which has physical and cost limits.**** |

---

## 🗄️ Popular RDBMS Comparison Matrix

| Feature | PostgreSQL | MySQL | Oracle | SQL Server | SQLite |
|---------|-----------|-------|--------|------------|--------|
| **License** | Open Source (MIT) | Open Source (GPL) / Commercial | Commercial | Commercial (Express Free) | Public Domain |
| **ACID Compliance** | Full | Full | Full | Full | Full |
| **Default Isolation** | Read Committed | Repeatable Read | Read Committed | Read Committed | Serializable |
| **MVCC** | Yes | Yes (InnoDB) | Yes | Yes | No |
| **JSON Support** | Excellent (JSONB) | Good | Good | Good | Good |
| **Full-Text Search** | Excellent | Good | Excellent | Excellent | Limited |
| **Replication** | Streaming, Logical | Binary Log, GTID | Advanced | Always On, Mirroring | None built-in |
| **Partitioning** | Range, List, Hash | Range, List, Hash, Key | Range, List, Hash, Composite | Range, List, Hash | None |
| **Window Functions** | Yes | Yes (8.0+) | Yes | Yes | Yes (3.25+) |
| **Scalability** | Good | Good | Excellent | Good | Single-user |
| **Performance** | Excellent (complex queries) | Excellent (reads) | Excellent | Good | Excellent (embedded) |
| **Best For** | Complex queries, JSON, GIS | Web apps, read-heavy | Enterprise, mission-critical | Microsoft stack | Embedded, mobile, testing |
| **Cost** | Free | Free (Community) | Very High | Medium-High | Free |

### When to Choose Each RDBMS

| Use Case | Best Choice | Why |
|----------|------------|-----|
| **Web Application** | MySQL / PostgreSQL | Free, proven, large community |
| **Complex Analytics** | PostgreSQL | Best query optimizer, advanced features |
| **Enterprise Mission-Critical** | Oracle | Best support, advanced features, proven at scale |
| **Microsoft Ecosystem** | SQL Server | Native .NET integration, Azure support |
| **Mobile/Embedded Apps** | SQLite | Zero-config, serverless, single file |
| **GIS/Geospatial** | PostgreSQL (PostGIS) | Best geospatial support |
| **High Concurrency Reads** | MySQL | Optimized for read-heavy workloads |
| **JSON Document Store** | PostgreSQL (JSONB) | Better JSON indexing and queries than NoSQL |
| **Time Series** | TimescaleDB (PostgreSQL) | Built on PostgreSQL, SQL-compatible |

---

## ⚠️ Common RDBMS Anti-Patterns

### Anti-Pattern 1: SELECT * FROM Everything

**Problem:**
```sql
-- Fetching all columns when only need a few
SELECT * FROM users WHERE id = 123;
-- Returns 50 columns, 10 KB of data
-- But you only need name and email!
```

**Why It's Wrong:**
- Wastes bandwidth and memory
- Slower query execution
- Breaks code when schema changes
- Can't use covering indexes effectively

**Better Approach:**
```sql
-- Only select what you need
SELECT id, name, email FROM users WHERE id = 123;
-- Returns 3 columns, 100 bytes
```

---

### Anti-Pattern 2: N+1 Query Problem

**Problem:**
```python
# Fetch all users
users = db.query("SELECT * FROM users LIMIT 100")

# Then fetch orders for each user
for user in users:
    orders = db.query("SELECT * FROM orders WHERE user_id = ?", user.id)
    # 100 separate queries!
# Total: 101 queries (1 + 100)
```

**Why It's Wrong:**
- Massive number of round-trips to database
- Each query has latency overhead
- Scales terribly (1000 users = 1001 queries)

**Better Approach:**
```python
# Single query with JOIN
results = db.query("""
    SELECT u.*, o.*
    FROM users u
    LEFT JOIN orders o ON u.id = o.user_id
    WHERE u.id IN (...)
""")
# 1 query instead of 101!

# Or use IN clause
user_ids = [u.id for u in users]
orders = db.query("""
    SELECT * FROM orders 
    WHERE user_id IN (?)
""", user_ids)
# 2 queries instead of 101
```

---

### Anti-Pattern 3: Using GUID/UUID as Primary Key

**Problem:**
```sql
CREATE TABLE users (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    name VARCHAR(255)
);
-- UUIDs are 128-bit random values
```

**Why It's Wrong:**
- **Index fragmentation**: Random UUIDs cause B-tree splits
- **Larger indexes**: 16 bytes vs 4-8 bytes for integers
- **Poor cache locality**: Random values don't cluster
- **Slower inserts**: More page splits and reorganization

**Better Approach:**
```sql
-- Option 1: Sequential integer
CREATE TABLE users (
    id BIGSERIAL PRIMARY KEY,  -- PostgreSQL
    -- or AUTO_INCREMENT in MySQL
    name VARCHAR(255)
);

-- Option 2: ULID (sortable UUID alternative)
-- Time-sorted, better for B-trees
CREATE TABLE users (
    id CHAR(26) PRIMARY KEY,  -- ULID format
    name VARCHAR(255)
);

-- Option 3: UUID v7 (time-ordered)
-- Better than random UUIDs, preserves some locality
```

**When UUIDs ARE Good:**
- Distributed systems (no central ID generator)
- Merging data from multiple sources
- Security (IDs aren't predictable)

---

### Anti-Pattern 4: Not Using Prepared Statements

**Problem:**
```python
# String interpolation - SQL INJECTION RISK!
username = request.get('username')
query = f"SELECT * FROM users WHERE username = '{username}'"
db.execute(query)

# If username = "admin' OR '1'='1"
# Query becomes: SELECT * FROM users WHERE username = 'admin' OR '1'='1'
# Returns ALL users!
```

**Why It's Wrong:**
- **SQL Injection vulnerability** (catastrophic security risk)
- Can't cache query plans
- Slower performance

**Better Approach:**
```python
# Parameterized query (prepared statement)
username = request.get('username')
query = "SELECT * FROM users WHERE username = ?"
db.execute(query, [username])

# Database treats ? as DATA, not SQL code
# Safe from SQL injection
# Query plan cached for reuse
```

---

### Anti-Pattern 5: Storing Images/Files in Database

**Problem:**
```sql
CREATE TABLE products (
    id INT PRIMARY KEY,
    name VARCHAR(255),
    image BYTEA  -- Storing 5 MB image
);
```

**Why It's Wrong:**
- **Bloated database**: Images fill up expensive DB storage
- **Slow queries**: Large BLOBs slow down scans
- **Poor caching**: Can't use CDN for images
- **Backup overhead**: Database backups become huge

**Better Approach:**
```sql
CREATE TABLE products (
    id INT PRIMARY KEY,
    name VARCHAR(255),
    image_url VARCHAR(500)  -- Store URL, not image
);

-- Store images in:
-- - Object storage (S3, GCS, Azure Blob)
-- - CDN for fast delivery
-- - Database only stores reference URL
```

**When storing in DB IS OK:**
- Small files (<100 KB)
- Transactional integrity critical
- Files need ACID guarantees

---

### Anti-Pattern 6: Using Nullable Foreign Keys Incorrectly

**Problem:**
```sql
CREATE TABLE orders (
    id INT PRIMARY KEY,
    user_id INT,  -- No NOT NULL constraint!
    product_id INT,
    FOREIGN KEY (user_id) REFERENCES users(id)
);

-- Allows orphan orders with no user
INSERT INTO orders (id, product_id) VALUES (1, 100);
```

**Why It's Wrong:**
- **Data integrity issues**: Orders without users
- **Join complexity**: Have to handle NULL cases
- **Application bugs**: Code assumes user_id exists

**Better Approach:**
```sql
CREATE TABLE orders (
    id INT PRIMARY KEY,
    user_id INT NOT NULL,  -- Must have a user
    product_id INT NOT NULL,
    FOREIGN KEY (user_id) REFERENCES users(id),
    FOREIGN KEY (product_id) REFERENCES products(id)
);

-- If you need "guest orders", create a guest user
INSERT INTO users (id, name) VALUES (0, 'Guest');
```

---

### Anti-Pattern 7: Not Using Transactions

**Problem:**
```python
# Transfer money between accounts
db.execute("UPDATE accounts SET balance = balance - 100 WHERE id = 1")
# ... app crashes here! ...
db.execute("UPDATE accounts SET balance = balance + 100 WHERE id = 2")
# Money deducted but never credited!
```

**Why It's Wrong:**
- **Data inconsistency**: Partial updates
- **Lost money** in this example
- No atomicity guarantee

**Better Approach:**
```python
# Use transaction
try:
    db.begin_transaction()
    db.execute("UPDATE accounts SET balance = balance - 100 WHERE id = 1")
    db.execute("UPDATE accounts SET balance = balance + 100 WHERE id = 2")
    db.commit()
except Exception as e:
    db.rollback()  # Both changes rolled back
    raise e
```

---

### Anti-Pattern 8: Ignoring Connection Pooling

**Problem:**
```python
# Creating new connection per request
def handle_request():
    conn = psycopg2.connect("dbname=mydb")
    cursor = conn.cursor()
    cursor.execute("SELECT * FROM users")
    conn.close()
    
# 10,000 requests = 10,000 new connections!
# Connection creation is SLOW (10-50ms)
```

**Why It's Wrong:**
- Connection creation overhead (handshake, auth)
- Database connection limits exhausted
- Poor performance

**Better Approach:**
```python
from psycopg2.pool import ThreadedConnectionPool

# Create pool on startup
pool = ThreadedConnectionPool(
    minconn=10,
    maxconn=100,
    dbname="mydb"
)

def handle_request():
    conn = pool.getconn()  # Reuse connection
    try:
        cursor = conn.cursor()
        cursor.execute("SELECT * FROM users")
    finally:
        pool.putconn(conn)  # Return to pool

# 10,000 requests share 100 connections
```

---

### Anti-Pattern 9: Not Using Indexes

**Problem:**
```sql
-- No index on email
CREATE TABLE users (
    id INT PRIMARY KEY,
    email VARCHAR(255),
    name VARCHAR(255)
);

-- This query scans ALL rows
SELECT * FROM users WHERE email = 'alice@example.com';
-- O(n) time complexity!
```

**Why It's Wrong:**
- Full table scan on every query
- Slow as table grows
- Database can't optimize

**Better Approach:**
```sql
-- Add index on email
CREATE UNIQUE INDEX idx_users_email ON users(email);

-- Now query is O(log n)
SELECT * FROM users WHERE email = 'alice@example.com';

-- Or compound index for common query patterns
CREATE INDEX idx_users_country_created 
ON users(country, created_at DESC);
```

**Index Guidelines:**
- Index foreign keys
- Index columns in WHERE clauses
- Index columns in JOIN conditions
- Index columns in ORDER BY
- Don't over-index (slows writes)

---

### Anti-Pattern 10: Long-Running Transactions

**Problem:**
```python
db.begin_transaction()
db.execute("SELECT * FROM orders FOR UPDATE")  # Locks rows

# ... 5 minutes of processing ...
# Send emails, call external APIs, etc.

db.commit()  # Finally release locks
```

**Why It's Wrong:**
- **Locks held too long**: Blocks other transactions
- **Deadlock risk**: Increases chance of deadlocks
- **Poor concurrency**: Other users wait
- **Connection exhaustion**: Holds connections

**Better Approach:**
```python
# Keep transactions SHORT
db.begin_transaction()
db.execute("SELECT * FROM orders FOR UPDATE")
orders = db.fetchall()
db.commit()  # Release locks quickly

# Do slow operations OUTSIDE transaction
for order in orders:
    send_email(order)  # No locks held
    call_api(order)

# Only open transaction when needed
db.begin_transaction()
db.execute("UPDATE orders SET status = 'processed'")
db.commit()
```

**Rule of thumb:** Transactions should be < 100ms

---

## 💡 RDBMS Best Practices

| Practice | Description | Benefit |
|----------|-------------|---------|
| **Use indexes wisely** | Index WHERE/JOIN/ORDER BY columns | O(log n) vs O(n) queries |
| **Always use prepared statements** | Parameterized queries | Prevent SQL injection, better performance |
| **Keep transactions short** | < 100ms duration | Better concurrency, avoid deadlocks |
| **Use connection pooling** | Reuse connections | Reduce overhead, better performance |
| **Select only needed columns** | No SELECT * | Less bandwidth, faster queries |
| **Set appropriate isolation level** | Match to use case | Balance consistency and performance |
| **Use foreign keys** | Enforce referential integrity | Data consistency |
| **Regular VACUUM/ANALYZE** | Maintain statistics (PostgreSQL) | Query optimizer accuracy |
| **Monitor slow queries** | Log queries > 100ms | Identify optimization opportunities |
| **Use appropriate data types** | INT vs BIGINT, VARCHAR length | Storage efficiency |

---

## ✏️ Design Challenge

### Problem

You are designing the back-end for an auction site where millions of users might bid on a single item. Every bid is a
database write. If you use a traditional RDBMS for the bidding ledger, which Isolation Level would you choose to prevent
bids from disappearing or conflicting, and what is the direct performance cost of this choice?

### Solution

#### 🧩 Scenario

- System: Auction site
- Action: Millions of concurrent bids on a single item
- Requirement:
    - Every bid must be recorded (no lost updates or conflicts)
    - Prevent inconsistent or missing data in the bidding ledger

#### ✅ Goal

Prevent:

- Lost updates (two concurrent bids overwriting each other)
- Phantom reads / write conflicts
- Duplicate or missing bids

So we need strong consistency in the write path.

#### ⚙️ Isolation Level Choice: SERIALIZABLE

Why SERIALIZABLE

- It’s the strongest isolation level — transactions behave as if executed one at a time (serially), even though they
  actually run concurrently.
  This ensures:
- Every bid is fully isolated.
- No two concurrent bids will see or overwrite each other’s intermediate state.
- The bidding ledger remains consistent and linearizable — no disappearing bids or race conditions.

#### ⚠️ Performance Cost: Concurrency & Throughput Drop

| Impact                     | Description                                                                                           |
|----------------------------|-------------------------------------------------------------------------------------------------------|
| **Lock contention**        | SERIALIZABLE isolation requires strict read/write locks or validation, blocking concurrent bidders.   |
| **Transaction retries**    | Many transactions will abort and retry due to serialization conflicts.                                |
| **Reduced throughput**     | Heavy contention (millions of bids) → major latency increase and lower QPS.                           |
| **Scalability bottleneck** | Traditional RDBMS might not horizontally scale under this isolation level for such high write volume. |

#### 🧠 Practical Real-World Approach

In real systems (like eBay or Amazon Auctions), they often:

- Use Optimistic Concurrency Control (OCC) or
- A **message queue + event** sourcing architecture,

so that bids are serialized by design without blocking all transactions.

#### ✅ Final Answer

| Aspect               | Decision                                                                                                        |
|----------------------|-----------------------------------------------------------------------------------------------------------------|
| **Isolation Level**  | **SERIALIZABLE**                                                                                                |
| **Why**              | Ensures no lost or conflicting bids — provides full transaction correctness.                                    |
| **Performance Cost** | Severe — high lock contention, frequent rollbacks, and reduced throughput due to serialization of transactions. |
