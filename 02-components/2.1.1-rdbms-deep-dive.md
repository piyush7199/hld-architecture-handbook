# 2.1.1 RDBMS Deep Dive: SQL and the ACID Guarantees

## Intuitive Explanation

Relational Database Management Systems (RDBMS) like PostgreSQL and MySQL are like a highly organized, trustworthy
accountant's ledger. They use tables (like spreadsheets) linked by relationships. Their core promise is trust and
consistency, enforced by the ACID properties.

- SQL (Structured Query Language): The universal language for talking to these databases.
- ACID: The absolute guarantee that your data is always valid, even if the power goes out mid-transaction. This makes
  them ideal for financial systems and inventory management.

---

## In-Depth Analysis

### 1. Relational Model and Joins

RDBMS stores data in normalized tables, meaning data is split into multiple tables to eliminate redundancy (e.g.,
separating user details from order details).

- **Normalization:** Reduces data duplication, saving space and ensuring that updates only need to happen in one place.
- **Joins:** Used to reconstruct the complete data (User + Order + Item) by linking tables using Foreign Keys. Joins are
  powerful but can become very slow at scale.

### 2. The ACID Properties (The Trust Contract)

ACID is an acronym defining the properties of database transactions, ensuring data remains valid despite errors,
failures, or concurrent access.

| Property    | Definition                                                                                                                              | Intuition                                                                                                                  |
|-------------|-----------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------|
| Atomicity   | A transaction is treated as a single, indivisible unit. Either all of the changes succeed, or none of them do.                          | If you transfer money, the deduction and the credit must both succeed; otherwise, the whole transfer is cancelled.         |
| Consistency | The database must move from one valid state to another valid state. Transactions that violate database rules (constraints) are aborted. | You cannot write a negative number into an inventory column that is defined as unsigned.                                   |
| Isolation   | Concurrent transactions must appear to execute sequentially. One transaction's intermediate state is hidden from others.                | Two users trying to buy the last iPhone see the stock count of 1 sequentially, not simultaneously, preventing overselling. |
| Durability  | Once a transaction is committed, it remains permanently recorded, even if the system crashes or loses power immediately after.          | The confirmation receipt you get for your order means the change is written to persistent storage (usually the disk).      |

### 3. Transactions and Isolation Levels

A Transaction is a sequence of database operations grouped together as a single logical unit of work. Isolation is
enforced through Isolation Levels:

- **Read Uncommitted**:
    - **Definition**: Transactions can read uncommitted changes from other transactions.
    - **How It Works**: Allows "`dirty reads`", where a transaction sees uncommitted data that may be rolled back.
    - **Examples**: Rarely used; supported in MySQL, SQL Server.
    - **Use Cases**: Non-critical reporting where speed is prioritized.
    - **Issues**: High risk of inconsistent data.
    - **Trade-offs**:
        - **Pros**: Minimal locking, high concurrency.
        - **Cons**: Risk of reading incorrect data.

- **Read Committed**:
    - **Definition**: Transactions only read committed data, but non-repeatable reads and phantom reads are possible.
    - **How It Works**: Ensures no dirty reads, but data may change between reads within a transaction.
    - **Examples**: Default in PostgreSQL, SQL Server, Oracle.
    - **Use Cases**: General-purpose applications (e.g., user profile updates).
    - **Issues**:
        - **Non-repeatable reads**: Data read twice in a transaction may differ.
        - **Phantom reads**: New rows may appear during a transaction.
    - **Trade-offs**:
        - **Pros**: Balances consistency and performance.
        - **Cons**: Inconsistent reads within a transaction.

- **Repeatable Read**:
    - **Definition**: Ensures consistent reads within a transaction, but phantom reads are possible.
    - **How It Works**: Locks read data to prevent updates by other transactions.
    - **Examples**: Default in MySQL (InnoDB); supported in PostgreSQL.
    - **Use Cases**: Applications needing stable reads (e.g., financial reports).
    - **Issues**: Phantom reads (new rows inserted by other transactions).
    - **Trade-offs**:
        - **Pros**: Stronger consistency than Read Committed.
        - **Cons**: Increased locking, potential deadlocks.

- **Serializable**:
    - **Definition**: Transactions execute as if run sequentially, preventing all anomalies.
    - **How It Works**: Uses strict locking or multiversion concurrency control (MVCC) to ensure complete isolation.
    - **Examples**: Supported in PostgreSQL, MySQL, Oracle.
    - **Use Cases**: Critical systems (e.g., banking, inventory control).
    - **Issues**: Lowest concurrency due to heavy locking.
    - **Trade-offs**:
        - **Pros**: Maximum consistency, no anomalies.
        - **Cons**: High latency, risk of deadlocks, reduced throughput.

### 4. Common Anomalies

- **Dirty Reads**: Reading uncommitted data that may be rolled back.
- **Non-repeatable Reads**: Data changes between reads within a transaction.
- **Phantom Reads**: New rows appear during a transaction due to inserts by other transactions.
- **Lost Updates**: Concurrent transactions overwrite each other‚Äôs changes.

### 5. Implementation Techniques

- **Locking**: Uses shared (read) or exclusive (write) locks to enforce isolation.
    - Example: MySQL‚Äôs Repeatable Read uses row-level locks.
- **Multiversion Concurrency Control (MVCC)**:
    - Creates data snapshots for transactions to read consistent versions.
    - Example: PostgreSQL uses MVCC for Read Committed and Repeatable Read.
- **Trade-offs**:
    - Locking: Simple but risks deadlocks and contention.
    - MVCC: Improves concurrency but increases storage for versioned data.

---

## üîí Concurrency Control: Optimistic vs Pessimistic Locking

### Intuitive Explanation

Imagine two people trying to edit the same Google Doc:
- **Pessimistic Locking**: Lock the document when you start editing. Others can't edit until you're done.
- **Optimistic Locking**: Everyone can edit. When you save, check if someone else changed it. If yes, resolve the conflict.

### Pessimistic Locking (Lock-Based)

**How It Works:**
- Acquire a lock **before** reading/modifying data
- Hold the lock during the entire transaction
- Release the lock when done
- Other transactions wait if they need the same data

**Database Implementation:**
```sql
-- PostgreSQL: SELECT FOR UPDATE
BEGIN;
SELECT * FROM inventory 
WHERE product_id = 123 
FOR UPDATE;  -- Acquires exclusive lock

-- Do some processing (lock is held)
UPDATE inventory 
SET quantity = quantity - 1 
WHERE product_id = 123;

COMMIT;  -- Lock released
```

**When to Use:**
- High contention (many concurrent updates to same data)
- Critical data that must be consistent (bank accounts, inventory)
- Read-modify-write operations
- Short transactions (< 100ms)

**Pros:**
- ‚úÖ Prevents conflicts (no lost updates)
- ‚úÖ Simpler to reason about
- ‚úÖ Guaranteed consistency

**Cons:**
- ‚ùå Reduced concurrency (blocking)
- ‚ùå Risk of deadlocks
- ‚ùå Performance bottleneck under high load
- ‚ùå Can cause cascading waits

---

### Optimistic Locking (Version-Based)

**How It Works:**
- Read data **without** acquiring a lock
- Include a version number or timestamp
- When updating, check if version changed
- If unchanged, update and increment version
- If changed, transaction fails (retry or abort)

**Database Implementation:**

**Option 1: Version Column**
```sql
-- Read with current version
SELECT product_id, quantity, version 
FROM inventory 
WHERE product_id = 123;
-- Result: quantity = 10, version = 5

-- Later, update only if version unchanged
UPDATE inventory 
SET quantity = 9, version = version + 1
WHERE product_id = 123 AND version = 5;

-- Check affected rows
-- If 0 rows affected ‚Üí someone else updated it ‚Üí conflict!
-- If 1 row affected ‚Üí success!
```

**Option 2: Timestamp Column**
```sql
SELECT product_id, quantity, updated_at 
FROM inventory 
WHERE product_id = 123;

UPDATE inventory 
SET quantity = 9, updated_at = NOW()
WHERE product_id = 123 
  AND updated_at = :previous_timestamp;
```

**Application-Level Check:**
```python
# Read data
product = db.execute("SELECT * FROM inventory WHERE id = 123")
original_version = product['version']

# Do some business logic (no lock held)
new_quantity = calculate_new_quantity(product['quantity'])

# Try to update with version check
result = db.execute("""
    UPDATE inventory 
    SET quantity = ?, version = version + 1
    WHERE id = 123 AND version = ?
""", new_quantity, original_version)

if result.rowcount == 0:
    # Conflict! Someone else updated it
    # Retry or return error to user
    raise ConflictError("Product was modified by another transaction")
else:
    # Success!
    db.commit()
```

**When to Use:**
- Low to medium contention
- Long-running transactions (user thinking time)
- Read-heavy workloads
- Distributed systems (no centralized lock manager)
- Web applications (stateless HTTP)

**Pros:**
- ‚úÖ Higher concurrency (no blocking)
- ‚úÖ No deadlocks
- ‚úÖ Better scalability
- ‚úÖ Works well with stateless protocols (HTTP)

**Cons:**
- ‚ùå Conflicts possible (retry logic needed)
- ‚ùå Wasted work if conflict occurs
- ‚ùå Not suitable for high contention
- ‚ùå User experience issue (save fails)

---

### Detailed Comparison Matrix

| Aspect | Pessimistic Locking | Optimistic Locking |
|--------|-------------------|-------------------|
| **Lock Acquisition** | Before read/update | No lock (version check at update) |
| **Blocking** | Yes (other transactions wait) | No (concurrent reads) |
| **Concurrency** | Low (serialized access) | High (parallel reads) |
| **Deadlock Risk** | Yes | No |
| **Best For** | High contention, short transactions | Low contention, long transactions |
| **Conflict Handling** | Prevented (blocking) | Detected (retry) |
| **Performance** | Good for high contention | Good for low contention |
| **User Experience** | Slower (waiting for lock) | Fast (but may fail on save) |
| **Implementation** | `SELECT FOR UPDATE` | Version column + conditional update |
| **Typical Use Cases** | Banking, inventory, booking systems | CMS, e-commerce carts, user profiles |

---

### Isolation Levels and Locking Behavior

| Isolation Level | Default Locking Behavior | Read Locks | Write Locks | Anomalies Prevented |
|----------------|-------------------------|-----------|------------|---------------------|
| **Read Uncommitted** | Minimal locking | None | Short duration | None (dirty reads possible) |
| **Read Committed** | Statement-level locking | Released after statement | Held until commit | Dirty reads |
| **Repeatable Read** | Transaction-level locking | Held until commit | Held until commit | Dirty reads, non-repeatable reads |
| **Serializable** | Full serialization | Held until commit (range locks) | Held until commit | All anomalies |

---

### Real-World Examples

#### Example 1: E-commerce Inventory (Pessimistic)

**Problem:** Multiple users buying last item in stock

```sql
-- User A and User B both want to buy product 123
-- Only 1 item left in stock

-- User A's transaction (gets lock first)
BEGIN;
SELECT quantity FROM inventory 
WHERE product_id = 123 
FOR UPDATE;  -- Acquires lock, quantity = 1

-- User B's transaction (must wait)
BEGIN;
SELECT quantity FROM inventory 
WHERE product_id = 123 
FOR UPDATE;  -- Blocked! Waiting for User A...

-- User A completes purchase
UPDATE inventory SET quantity = 0 WHERE product_id = 123;
COMMIT;  -- Lock released

-- Now User B's SELECT returns
-- quantity = 0, so purchase fails (out of stock)
ROLLBACK;
```

**Result:** No overselling! Pessimistic locking prevents race condition.

---

#### Example 2: Blog Post Editing (Optimistic)

**Problem:** Two editors updating same blog post

```python
# Editor A
post = db.query("SELECT * FROM posts WHERE id = 1")
# post = {id: 1, title: "Hello", content: "...", version: 5}

# Editor A makes changes (takes 10 minutes)
new_title = "Hello World"

# Meanwhile, Editor B also edits
post_b = db.query("SELECT * FROM posts WHERE id = 1")
# post_b = {id: 1, title: "Hello", content: "...", version: 5}

# Editor B saves first (changes content)
db.execute("""
    UPDATE posts 
    SET content = 'New content', version = 6
    WHERE id = 1 AND version = 5
""")
# Success! version is now 6

# Editor A tries to save (changes title)
result = db.execute("""
    UPDATE posts 
    SET title = 'Hello World', version = 6
    WHERE id = 1 AND version = 5
""")
# Fails! 0 rows affected (version is now 6, not 5)

# Application shows error:
# "This post was modified by another user. Please refresh and try again."
```

**Result:** Conflict detected! Editor A must refresh and re-apply changes.

---

#### Example 3: Hybrid Approach (Pessimistic for Critical, Optimistic for Others)

```python
# E-commerce order processing
def process_order(order_id, product_id, quantity):
    # Use pessimistic locking for inventory (critical!)
    db.execute("BEGIN")
    
    inventory = db.execute("""
        SELECT quantity FROM inventory 
        WHERE product_id = ? 
        FOR UPDATE
    """, product_id)
    
    if inventory['quantity'] < quantity:
        db.execute("ROLLBACK")
        raise OutOfStockError()
    
    # Deduct inventory (lock held)
    db.execute("""
        UPDATE inventory 
        SET quantity = quantity - ?
        WHERE product_id = ?
    """, quantity, product_id)
    
    # Use optimistic locking for user profile (less critical)
    user = db.execute("""
        SELECT loyalty_points, version 
        FROM users 
        WHERE user_id = ?
    """, order['user_id'])
    
    # Add loyalty points
    result = db.execute("""
        UPDATE users 
        SET loyalty_points = loyalty_points + ?,
            version = version + 1
        WHERE user_id = ? AND version = ?
    """, points, order['user_id'], user['version'])
    
    if result.rowcount == 0:
        # Conflict on user profile - acceptable, just skip points
        log.warning("Loyalty points conflict, skipping")
    
    db.execute("COMMIT")
```

---

### Choosing Between Optimistic and Pessimistic Locking

```
                    High Contention?
                          |
                    Yes ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ No
                    |            |
             Pessimistic    Low Contention?
              Locking            |
                           Yes ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ No
                           |            |
                      Optimistic   Consider Both
                       Locking
                       
Short Transaction? (<100ms)
‚îú‚îÄ Yes ‚Üí Pessimistic (lock overhead minimal)
‚îî‚îÄ No ‚Üí Optimistic (don't hold locks long)

Critical Data? (money, inventory)
‚îú‚îÄ Yes ‚Üí Pessimistic (prevent conflicts)
‚îî‚îÄ No ‚Üí Optimistic (accept occasional conflicts)

Distributed System?
‚îú‚îÄ Yes ‚Üí Optimistic (no central lock manager)
‚îî‚îÄ No ‚Üí Either works
```

---

### Lock Types in SQL

| Lock Type | SQL Syntax | Purpose | Scope |
|----------|-----------|---------|-------|
| **Shared Lock (S)** | `SELECT ... FOR SHARE` | Read lock, allows other reads | Row/Table |
| **Exclusive Lock (X)** | `SELECT ... FOR UPDATE` | Write lock, blocks all other access | Row/Table |
| **Update Lock (U)** | Automatic in some DBs | Intent to update, prevents deadlocks | Row |
| **Intent Locks** | Automatic | Indicates intention to lock at finer granularity | Table/Page |
| **Row-Level Lock** | `FOR UPDATE` | Locks specific rows | Row |
| **Table-Level Lock** | `LOCK TABLE` | Locks entire table | Table |

---

### Deadlock Example and Prevention

**Deadlock Scenario:**
```sql
-- Transaction 1
BEGIN;
UPDATE accounts SET balance = balance - 100 WHERE id = 1;  -- Locks account 1
-- ... waiting ...
UPDATE accounts SET balance = balance + 100 WHERE id = 2;  -- Needs lock on account 2

-- Transaction 2 (concurrent)
BEGIN;
UPDATE accounts SET balance = balance - 50 WHERE id = 2;   -- Locks account 2
-- ... waiting ...
UPDATE accounts SET balance = balance + 50 WHERE id = 1;   -- Needs lock on account 1

-- DEADLOCK! Each transaction waiting for the other
```

**Prevention Strategies:**
1. **Lock Ordering**: Always acquire locks in same order (e.g., by account ID)
```sql
-- Both transactions lock in ascending ID order
UPDATE accounts SET balance = ... WHERE id = 1;  -- Lock 1 first
UPDATE accounts SET balance = ... WHERE id = 2;  -- Then lock 2
```

2. **Lock Timeout**: Set timeout for lock acquisition
```sql
SET lock_timeout = '5s';
-- Transaction aborts if can't acquire lock in 5 seconds
```

3. **Deadlock Detection**: DB automatically detects and aborts one transaction

---

## üí° Use Cases and Scaling Trade-offs

| Use Case                    | Rationale                                                               | Scaling Challenge                                                                                                                               |
|-----------------------------|-------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------|
| Financial/Banking Ledgers   | Requires absolute ACID compliance (especially Atomicity and Isolation). | **High Write Contention:** Concurrent updates to bank balances or inventory require heavy locking, severely limiting write throughput.          |
| User Authentication/Profile | Strong consistency is required for password and core profile data.      | **Vertical Scaling:** Traditional RDBMS scales primarily by getting a bigger server (vertical scaling), which has physical and cost limits.**** |

---

## üóÑÔ∏è Popular RDBMS Comparison Matrix

| Feature | PostgreSQL | MySQL | Oracle | SQL Server | SQLite |
|---------|-----------|-------|--------|------------|--------|
| **License** | Open Source (MIT) | Open Source (GPL) / Commercial | Commercial | Commercial (Express Free) | Public Domain |
| **ACID Compliance** | Full | Full | Full | Full | Full |
| **Default Isolation** | Read Committed | Repeatable Read | Read Committed | Read Committed | Serializable |
| **MVCC** | Yes | Yes (InnoDB) | Yes | Yes | No |
| **JSON Support** | Excellent (JSONB) | Good | Good | Good | Good |
| **Full-Text Search** | Excellent | Good | Excellent | Excellent | Limited |
| **Replication** | Streaming, Logical | Binary Log, GTID | Advanced | Always On, Mirroring | None built-in |
| **Partitioning** | Range, List, Hash | Range, List, Hash, Key | Range, List, Hash, Composite | Range, List, Hash | None |
| **Window Functions** | Yes | Yes (8.0+) | Yes | Yes | Yes (3.25+) |
| **Scalability** | Good | Good | Excellent | Good | Single-user |
| **Performance** | Excellent (complex queries) | Excellent (reads) | Excellent | Good | Excellent (embedded) |
| **Best For** | Complex queries, JSON, GIS | Web apps, read-heavy | Enterprise, mission-critical | Microsoft stack | Embedded, mobile, testing |
| **Cost** | Free | Free (Community) | Very High | Medium-High | Free |

### When to Choose Each RDBMS

| Use Case | Best Choice | Why |
|----------|------------|-----|
| **Web Application** | MySQL / PostgreSQL | Free, proven, large community |
| **Complex Analytics** | PostgreSQL | Best query optimizer, advanced features |
| **Enterprise Mission-Critical** | Oracle | Best support, advanced features, proven at scale |
| **Microsoft Ecosystem** | SQL Server | Native .NET integration, Azure support |
| **Mobile/Embedded Apps** | SQLite | Zero-config, serverless, single file |
| **GIS/Geospatial** | PostgreSQL (PostGIS) | Best geospatial support |
| **High Concurrency Reads** | MySQL | Optimized for read-heavy workloads |
| **JSON Document Store** | PostgreSQL (JSONB) | Better JSON indexing and queries than NoSQL |
| **Time Series** | TimescaleDB (PostgreSQL) | Built on PostgreSQL, SQL-compatible |

---

## ‚ö†Ô∏è Common RDBMS Anti-Patterns

### Anti-Pattern 1: SELECT * FROM Everything

**Problem:**
```sql
-- Fetching all columns when only need a few
SELECT * FROM users WHERE id = 123;
-- Returns 50 columns, 10 KB of data
-- But you only need name and email!
```

**Why It's Wrong:**
- Wastes bandwidth and memory
- Slower query execution
- Breaks code when schema changes
- Can't use covering indexes effectively

**Better Approach:**
```sql
-- Only select what you need
SELECT id, name, email FROM users WHERE id = 123;
-- Returns 3 columns, 100 bytes
```

---

### Anti-Pattern 2: N+1 Query Problem

**Problem:**
```python
# Fetch all users
users = db.query("SELECT * FROM users LIMIT 100")

# Then fetch orders for each user
for user in users:
    orders = db.query("SELECT * FROM orders WHERE user_id = ?", user.id)
    # 100 separate queries!
# Total: 101 queries (1 + 100)
```

**Why It's Wrong:**
- Massive number of round-trips to database
- Each query has latency overhead
- Scales terribly (1000 users = 1001 queries)

**Better Approach:**
```python
# Single query with JOIN
results = db.query("""
    SELECT u.*, o.*
    FROM users u
    LEFT JOIN orders o ON u.id = o.user_id
    WHERE u.id IN (...)
""")
# 1 query instead of 101!

# Or use IN clause
user_ids = [u.id for u in users]
orders = db.query("""
    SELECT * FROM orders 
    WHERE user_id IN (?)
""", user_ids)
# 2 queries instead of 101
```

---

### Anti-Pattern 3: Using GUID/UUID as Primary Key

**Problem:**
```sql
CREATE TABLE users (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    name VARCHAR(255)
);
-- UUIDs are 128-bit random values
```

**Why It's Wrong:**
- **Index fragmentation**: Random UUIDs cause B-tree splits
- **Larger indexes**: 16 bytes vs 4-8 bytes for integers
- **Poor cache locality**: Random values don't cluster
- **Slower inserts**: More page splits and reorganization

**Better Approach:**
```sql
-- Option 1: Sequential integer
CREATE TABLE users (
    id BIGSERIAL PRIMARY KEY,  -- PostgreSQL
    -- or AUTO_INCREMENT in MySQL
    name VARCHAR(255)
);

-- Option 2: ULID (sortable UUID alternative)
-- Time-sorted, better for B-trees
CREATE TABLE users (
    id CHAR(26) PRIMARY KEY,  -- ULID format
    name VARCHAR(255)
);

-- Option 3: UUID v7 (time-ordered)
-- Better than random UUIDs, preserves some locality
```

**When UUIDs ARE Good:**
- Distributed systems (no central ID generator)
- Merging data from multiple sources
- Security (IDs aren't predictable)

---

### Anti-Pattern 4: Not Using Prepared Statements

**Problem:**
```python
# String interpolation - SQL INJECTION RISK!
username = request.get('username')
query = f"SELECT * FROM users WHERE username = '{username}'"
db.execute(query)

# If username = "admin' OR '1'='1"
# Query becomes: SELECT * FROM users WHERE username = 'admin' OR '1'='1'
# Returns ALL users!
```

**Why It's Wrong:**
- **SQL Injection vulnerability** (catastrophic security risk)
- Can't cache query plans
- Slower performance

**Better Approach:**
```python
# Parameterized query (prepared statement)
username = request.get('username')
query = "SELECT * FROM users WHERE username = ?"
db.execute(query, [username])

# Database treats ? as DATA, not SQL code
# Safe from SQL injection
# Query plan cached for reuse
```

---

### Anti-Pattern 5: Storing Images/Files in Database

**Problem:**
```sql
CREATE TABLE products (
    id INT PRIMARY KEY,
    name VARCHAR(255),
    image BYTEA  -- Storing 5 MB image
);
```

**Why It's Wrong:**
- **Bloated database**: Images fill up expensive DB storage
- **Slow queries**: Large BLOBs slow down scans
- **Poor caching**: Can't use CDN for images
- **Backup overhead**: Database backups become huge

**Better Approach:**
```sql
CREATE TABLE products (
    id INT PRIMARY KEY,
    name VARCHAR(255),
    image_url VARCHAR(500)  -- Store URL, not image
);

-- Store images in:
-- - Object storage (S3, GCS, Azure Blob)
-- - CDN for fast delivery
-- - Database only stores reference URL
```

**When storing in DB IS OK:**
- Small files (<100 KB)
- Transactional integrity critical
- Files need ACID guarantees

---

### Anti-Pattern 6: Using Nullable Foreign Keys Incorrectly

**Problem:**
```sql
CREATE TABLE orders (
    id INT PRIMARY KEY,
    user_id INT,  -- No NOT NULL constraint!
    product_id INT,
    FOREIGN KEY (user_id) REFERENCES users(id)
);

-- Allows orphan orders with no user
INSERT INTO orders (id, product_id) VALUES (1, 100);
```

**Why It's Wrong:**
- **Data integrity issues**: Orders without users
- **Join complexity**: Have to handle NULL cases
- **Application bugs**: Code assumes user_id exists

**Better Approach:**
```sql
CREATE TABLE orders (
    id INT PRIMARY KEY,
    user_id INT NOT NULL,  -- Must have a user
    product_id INT NOT NULL,
    FOREIGN KEY (user_id) REFERENCES users(id),
    FOREIGN KEY (product_id) REFERENCES products(id)
);

-- If you need "guest orders", create a guest user
INSERT INTO users (id, name) VALUES (0, 'Guest');
```

---

### Anti-Pattern 7: Not Using Transactions

**Problem:**
```python
# Transfer money between accounts
db.execute("UPDATE accounts SET balance = balance - 100 WHERE id = 1")
# ... app crashes here! ...
db.execute("UPDATE accounts SET balance = balance + 100 WHERE id = 2")
# Money deducted but never credited!
```

**Why It's Wrong:**
- **Data inconsistency**: Partial updates
- **Lost money** in this example
- No atomicity guarantee

**Better Approach:**
```python
# Use transaction
try:
    db.begin_transaction()
    db.execute("UPDATE accounts SET balance = balance - 100 WHERE id = 1")
    db.execute("UPDATE accounts SET balance = balance + 100 WHERE id = 2")
    db.commit()
except Exception as e:
    db.rollback()  # Both changes rolled back
    raise e
```

---

### Anti-Pattern 8: Ignoring Connection Pooling

**Problem:**
```python
# Creating new connection per request
def handle_request():
    conn = psycopg2.connect("dbname=mydb")
    cursor = conn.cursor()
    cursor.execute("SELECT * FROM users")
    conn.close()
    
# 10,000 requests = 10,000 new connections!
# Connection creation is SLOW (10-50ms)
```

**Why It's Wrong:**
- Connection creation overhead (handshake, auth)
- Database connection limits exhausted
- Poor performance

**Better Approach:**
```python
from psycopg2.pool import ThreadedConnectionPool

# Create pool on startup
pool = ThreadedConnectionPool(
    minconn=10,
    maxconn=100,
    dbname="mydb"
)

def handle_request():
    conn = pool.getconn()  # Reuse connection
    try:
        cursor = conn.cursor()
        cursor.execute("SELECT * FROM users")
    finally:
        pool.putconn(conn)  # Return to pool

# 10,000 requests share 100 connections
```

---

### Anti-Pattern 9: Not Using Indexes

**Problem:**
```sql
-- No index on email
CREATE TABLE users (
    id INT PRIMARY KEY,
    email VARCHAR(255),
    name VARCHAR(255)
);

-- This query scans ALL rows
SELECT * FROM users WHERE email = 'alice@example.com';
-- O(n) time complexity!
```

**Why It's Wrong:**
- Full table scan on every query
- Slow as table grows
- Database can't optimize

**Better Approach:**
```sql
-- Add index on email
CREATE UNIQUE INDEX idx_users_email ON users(email);

-- Now query is O(log n)
SELECT * FROM users WHERE email = 'alice@example.com';

-- Or compound index for common query patterns
CREATE INDEX idx_users_country_created 
ON users(country, created_at DESC);
```

**Index Guidelines:**
- Index foreign keys
- Index columns in WHERE clauses
- Index columns in JOIN conditions
- Index columns in ORDER BY
- Don't over-index (slows writes)

---

### Anti-Pattern 10: Long-Running Transactions

**Problem:**
```python
db.begin_transaction()
db.execute("SELECT * FROM orders FOR UPDATE")  # Locks rows

# ... 5 minutes of processing ...
# Send emails, call external APIs, etc.

db.commit()  # Finally release locks
```

**Why It's Wrong:**
- **Locks held too long**: Blocks other transactions
- **Deadlock risk**: Increases chance of deadlocks
- **Poor concurrency**: Other users wait
- **Connection exhaustion**: Holds connections

**Better Approach:**
```python
# Keep transactions SHORT
db.begin_transaction()
db.execute("SELECT * FROM orders FOR UPDATE")
orders = db.fetchall()
db.commit()  # Release locks quickly

# Do slow operations OUTSIDE transaction
for order in orders:
    send_email(order)  # No locks held
    call_api(order)

# Only open transaction when needed
db.begin_transaction()
db.execute("UPDATE orders SET status = 'processed'")
db.commit()
```

**Rule of thumb:** Transactions should be < 100ms

---

## üí° RDBMS Best Practices

| Practice | Description | Benefit |
|----------|-------------|---------|
| **Use indexes wisely** | Index WHERE/JOIN/ORDER BY columns | O(log n) vs O(n) queries |
| **Always use prepared statements** | Parameterized queries | Prevent SQL injection, better performance |
| **Keep transactions short** | < 100ms duration | Better concurrency, avoid deadlocks |
| **Use connection pooling** | Reuse connections | Reduce overhead, better performance |
| **Select only needed columns** | No SELECT * | Less bandwidth, faster queries |
| **Set appropriate isolation level** | Match to use case | Balance consistency and performance |
| **Use foreign keys** | Enforce referential integrity | Data consistency |
| **Regular VACUUM/ANALYZE** | Maintain statistics (PostgreSQL) | Query optimizer accuracy |
| **Monitor slow queries** | Log queries > 100ms | Identify optimization opportunities |
| **Use appropriate data types** | INT vs BIGINT, VARCHAR length | Storage efficiency |

---

## ‚úèÔ∏è Design Challenge

### Problem

You are designing the back-end for an auction site where millions of users might bid on a single item. Every bid is a
database write. If you use a traditional RDBMS for the bidding ledger, which Isolation Level would you choose to prevent
bids from disappearing or conflicting, and what is the direct performance cost of this choice?

### Solution

#### üß© Scenario

- System: Auction site
- Action: Millions of concurrent bids on a single item
- Requirement:
    - Every bid must be recorded (no lost updates or conflicts)
    - Prevent inconsistent or missing data in the bidding ledger

#### ‚úÖ Goal

Prevent:

- Lost updates (two concurrent bids overwriting each other)
- Phantom reads / write conflicts
- Duplicate or missing bids

So we need strong consistency in the write path.

#### ‚öôÔ∏è Isolation Level Choice: SERIALIZABLE

Why SERIALIZABLE

- It‚Äôs the strongest isolation level ‚Äî transactions behave as if executed one at a time (serially), even though they
  actually run concurrently.
  This ensures:
- Every bid is fully isolated.
- No two concurrent bids will see or overwrite each other‚Äôs intermediate state.
- The bidding ledger remains consistent and linearizable ‚Äî no disappearing bids or race conditions.

#### ‚ö†Ô∏è Performance Cost: Concurrency & Throughput Drop

| Impact                     | Description                                                                                           |
|----------------------------|-------------------------------------------------------------------------------------------------------|
| **Lock contention**        | SERIALIZABLE isolation requires strict read/write locks or validation, blocking concurrent bidders.   |
| **Transaction retries**    | Many transactions will abort and retry due to serialization conflicts.                                |
| **Reduced throughput**     | Heavy contention (millions of bids) ‚Üí major latency increase and lower QPS.                           |
| **Scalability bottleneck** | Traditional RDBMS might not horizontally scale under this isolation level for such high write volume. |

#### üß† Practical Real-World Approach

In real systems (like eBay or Amazon Auctions), they often:

- Use Optimistic Concurrency Control (OCC) or
- A **message queue + event** sourcing architecture,

so that bids are serialized by design without blocking all transactions.

#### ‚úÖ Final Answer

| Aspect               | Decision                                                                                                        |
|----------------------|-----------------------------------------------------------------------------------------------------------------|
| **Isolation Level**  | **SERIALIZABLE**                                                                                                |
| **Why**              | Ensures no lost or conflicting bids ‚Äî provides full transaction correctness.                                    |
| **Performance Cost** | Severe ‚Äî high lock contention, frequent rollbacks, and reduced throughput due to serialization of transactions. |
