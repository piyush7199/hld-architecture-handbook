# 2.3.2 Kafka Deep Dive: Partitions, Offsets, and Log Compaction

## Intuitive Explanation

Apache Kafka is the de facto standard for distributed message **streaming**. Think of it as a huge, durable,
highly-available commit log for your entire business. Every event (user click, order placed, price change) is written to
this log.

Kafka's strength is that it treats data as an **immutable, replayable sequence of events**, enabling services to read
data
in real-time or revisit history.

---

## In-Depth Analysis

### 1. Key Components

- **Broker:** A single server running Kafka. A Kafka cluster is composed of multiple brokers working together.
- **Topic:** A category or feed name to which records are published (e.g., `user_events`, `payment_logs`).
- **Producer:** The application that writes data (records/events) to topics.
- **Consumer:** The application that reads data from topics.

### 2. Partitions (The Key to Scale)

A **Topic** is split into multiple ordered, immutable logs called **Partitions**.

- **Parallelism:** Partitions are the unit of parallelism. Kafka can handle a high volume of writes because different
  producers can write to different partitions simultaneously.
- **Order Guarantee:** Kafka only guarantees the order of messages **within a single partition**, not across the entire
  topic.
- **Keying:** Producers assign a **Key** to each message (e.g., a user_id). All messages with the same key are
  guaranteed to land on the **same partition.** This is essential for processing related events in order (e.g., all
  banking
  deposits for a single account).

### 3. Consumer Groups and Offsets

- **Consumer Group:** A set of consumers that cooperate to consume data from a single topic. All messages from a given
  partition are delivered to only one consumer within the group. This ensures load balancing.
- **Offset:** The unique ID or index of a record within a partition. Consumers track their last processed message using
  this offset.
    - **Benefit:** If a consumer crashes, it restarts by reading from its last recorded offset, ensuring no data is
      missed or duplicated.

### 4. Log Compaction

Kafka partitions store all data indefinitely (or for a long retention period). Log Compaction is a feature that cleans
up the log by only retaining the **latest value** for any given key.

- **Use Case:** Ideal for storing system state where you only care about the current snapshot (e.g., the last known
  location of a device, the current stock count of an item). It efficiently converts a stream of updates into a
  continuously updated database.

---

## ‚úèÔ∏è Design Challenge

### Problem

You are building a live inventory counter for a highly contested item. The inventory updates frequently, and you need to
ensure all updates for a specific item (identified by its SKU) are processed in the correct order.

How would you use Kafka Partitions and Message Keys to guarantee this strict ordering for all events related to that
single SKU, and what is the trade-off of this approach?

### Solution

#### üß© Scenario Summary

| Aspect          | Description                                                    |
|-----------------|----------------------------------------------------------------|
| **System**      | Live inventory counter                                         |
| **Data Flow**   | Multiple producers ‚Üí Kafka ‚Üí Consumer(s) updating counts       |
| **Requirement** | Events for the same SKU must be processed **in order**         |
| **Challenge**   | Kafka is distributed ‚Äî messages may go to different partitions |

#### ‚úÖ Step 1: Ordering in Kafka ‚Äî The Key Rule

Kafka guarantees message order only within a single partition, not across partitions.

Therefore:

üëâ To preserve ordering for a given SKU, all its events must land in the same partition.

#### ‚úÖ Step 2: Use Message Keys to Route by SKU

Each message produced to Kafka should include a message key = the item‚Äôs SKU.

```java
ProducerRecord<String, InventoryEvent> record =
        new ProducerRecord<>("inventory-events", sku, event);
```

Kafka‚Äôs partitioner will use this key to ensure:

```
partition = hash(key) % numPartitions
```

‚û°Ô∏è **All events with the same SKU** will always hash to the same partition, preserving strict order for that SKU.

| SKU  | Key    | Partition | Ordering Guarantee |
|------|--------|-----------|--------------------|
| A123 | "A123" | 4         | ‚úÖ Ordered          |
| A123 | "A123" | 4         | ‚úÖ Ordered          |
| B456 | "B456" | 7         | ‚úÖ Ordered          |
| C789 | "C789" | 2         | ‚úÖ Ordered          |

#### ‚úÖ Step 3: Consumers and Ordering

Kafka consumers read from partitions sequentially.
If each partition is consumed by exactly one consumer thread:

- All events for a given SKU are processed in the same order they were written.
- Parallelism is still achieved across partitions (different SKUs).

#### ‚úÖ Step 4: Trade-off ‚Äî Parallelism vs Ordering

While this design guarantees **per-SKU ordering**, it introduces an important trade-off:

| Aspect                  | Impact                                                                      |
|-------------------------|-----------------------------------------------------------------------------|
| **Ordering**            | ‚úÖ Guaranteed for each SKU (all in same partition)                           |
| **Parallelism per SKU** | ‚ùå Lost (only one consumer handles that SKU‚Äôs partition)                     |
| **Throughput scaling**  | Limited by number of partitions ‚Äî you can‚Äôt scale within a single partition |
| **Hotspot risk**        | If one SKU is ‚Äúhot‚Äù (many updates), that partition becomes a **bottleneck** |
| **Balancing**           | Skewed load possible if some SKUs dominate traffic                          |

#### ‚úÖ Step 5: Optimization Techniques

To reduce skew or improve throughput:

- **Increase partition count** ‚Üí spreads SKUs across more partitions.
- **Use composite keys** like (SKU, region) if ordering can be relaxed per region.
- **Shard hot SKUs** into multiple logical SKUs (e.g., SKU#1, SKU#2) if allowed.

#### ‚úÖ Final Summary

| Aspect                 | Design Decision                               | Reason                                          |
|------------------------|-----------------------------------------------|-------------------------------------------------|
| **Kafka topic**        | `inventory-events`                            | Stores all inventory update events              |
| **Message key**        | SKU ID                                        | Ensures deterministic routing to same partition |
| **Ordering guarantee** | Per-partition (per SKU)                       | Kafka guarantees order within a partition       |
| **Consumer model**     | 1 consumer per partition                      | Maintains sequential processing                 |
| **Trade-off**          | Reduced parallelism, potential hot partitions | Necessary for ordering consistency              |
