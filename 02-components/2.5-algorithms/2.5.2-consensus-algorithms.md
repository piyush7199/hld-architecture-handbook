# 2.5.2 Consensus Algorithms: Solving the Agreement Problem

## Intuitive Explanation

In a distributed system, **Consensus** is the challenge of getting all participating servers to agree on a single value
or decision (like which server is the leader, or whether a commit is successful).

- **Problem**: Servers can fail, network messages can be delayed, and partitions can
  occur ($\text{CAP}$ $\text{Theorem}$).
- **Goal**: To ensure the system remains correct (Consistent) even when these failures happen.

---

## In-Depth Analysis

### 1. Consensus Algorithms (Solving the Leader Problem)

Algorithms like **Paxos** and **Raft** are designed to maintain a consistent log or state machine across a cluster of
independent nodes, ensuring agreement even if a minority of nodes fail.

- **Raft (The Simplified Algorithm):** Raft is often used instead of $\text{Paxos}$ because it is easier to understand
  and implement.
    - **Mechanism:** It relies on a majority vote. The cluster elects a single Leader. All writes must go through the
      **Leader**, who replicates the change to its followers. The Leader only commits a change when a majority of
      followers confirm the write.
    - **Tools**: Used by $\text{etcd}$ and $\text{ZooKeeper}$ for cluster coordination.

- **Quorum:** The minimum number of votes (or replicas) required for a distributed operation to be considered
  successful. Usually, this is the majority ($\text{N}/2 + 1$).

### Key Concepts / Tradeoffs

| Feature         | Description                                                                                                                                                                                                                          | Trade-off / Scalability Issue                                                                                            |
|-----------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------|
| Split Brain     | A dangerous condition where a cluster is partitioned, and two nodes incorrectly believe they are the **Leader**, leading to conflicting, inconsistent writes.                                                                        | Consensus algorithms ($\text{Raft}$/$\text{Paxos}$) prevent this by requiring a **Quorum** (majority) to commit a write. |
| Consensus & CAP | Consensus systems prioritize **Consistency** ($\text{C}$) over $\text{Availability}$ ($\text{A}$). If a quorum cannot be reached (e.g., during a network partition), the system must stop accepting writes to guarantee correctness. | This is an explicit, necessary violation of the $\text{Availability}$ guarantee of the $\text{CAP}$ theorem.             |

---

## ✏️ Design Challenge

### Problem

You are building a distributed configuration service ($\text{Config}$ $\text{Store}$) that holds critical flags (e.g.,
Feature $\text{Toggle}$ for a major launch). Explain why this service must be built on a Consensus Algorithm and what
the user experience would be if a network partition occurred (i.e., if it traded $\text{A}$ for $\text{C}$).

### Solution

#### 🧩 Scenario Summary

| Requirement          | Description                                           |
|----------------------|-------------------------------------------------------|
| **System type**      | Distributed Config Store                              |
| **Data type**        | Critical feature flags / system configs               |
| **Consistency need** | Strong — everyone must see the same config            |
| **Risk**             | Network partition or node failure                     |
| **Trade-off**        | Consistency (C) vs Availability (A) under CAP theorem |

#### ✅ Step 1: Why It Must Use a Consensus Algorithm

To keep configurations **strongly consistent** across replicas, you need all nodes to **agree on a single authoritative
value** before any node reports success.

This requires a **Consensus Algorithm** such as:

* **Raft**,
* **Paxos**, or
* **Zab** (used in ZooKeeper).

These algorithms ensure that:

- Only **one leader** can accept writes.
- Updates (e.g., feature toggle ON/OFF) are **replicated** in order across all nodes.
- Once a value is committed, it’s durable and consistent cluster-wide.

> ✅ Reason: You cannot risk half your services thinking a feature is ON and others thinking it’s OFF — that would cause
> production chaos.

#### ✅ Step 2: CAP Theorem Trade-off

Under a **network partition**, the system must choose between:

| Option               | Meaning                                       | Implication                                                    |
|----------------------|-----------------------------------------------|----------------------------------------------------------------|
| **A (Availability)** | Every node keeps responding, even if isolated | Could serve stale or conflicting configs                       |
| **C (Consistency)**  | Only nodes in majority quorum respond         | Some regions become unavailable, but all responses are correct |

For a Config Store, correctness is more important than uptime — you’d rather return _no answer_ than a wrong answer.

> ⚖️ **Choice:** Trade Availability (A) for Consistency (C).

#### ✅ Step 3: What Happens During a Network Partition

When a network partition occurs (say, between east and west data centers):

- The **majority partition** (e.g., 2 out of 3 nodes) **elects a leader** and continues to serve reads and writes.
- The **minority partition** cannot reach quorum → becomes **read-only** or unavailable until reconnection.

This ensures only **one source of truth** exists, avoiding config drift.

#### ✅ Step 4: User Experience During Partition

| User Segment                              | Experience                                                       | Reason                                  |
|-------------------------------------------|------------------------------------------------------------------|-----------------------------------------|
| **Users connected to majority partition** | ✅ Continue seeing consistent, correct config                     | Cluster quorum is available             |
| **Users connected to minority partition** | 🚫 Requests may fail or return old config                        | No quorum → cannot confirm latest state |
| **Global impact**                         | Temporary reduced availability, but **no inconsistent behavior** | System favors correctness               |

> 🔒 This guarantees safety for critical flags — better a brief outage than a split-brain feature rollout.

#### ✅ Step 5: Why Consensus Matters for Config Stores

| Property               | Achieved via Consensus                   |
|------------------------|------------------------------------------|
| **Strong Consistency** | Only committed updates are visible       |
| **Leader Election**    | Ensures single writer, avoids conflicts  |
| **Durability**         | Once quorum commits, value persists      |
| **Fault Tolerance**    | Survives minority node or region failure |

Systems like **etcd, Consul,** and **ZooKeeper** are built this way for exactly this reason — they hold the source of
truth for
distributed configuration.

#### ✅ Final Summary

| Aspect                 | Decision                                                          | Reason                                  |
|------------------------|-------------------------------------------------------------------|-----------------------------------------|
| **Algorithm**          | Consensus (Raft/Paxos/Zab)                                        | Guarantees one consistent config source |
| **CAP Trade-off**      | **CP** (Consistency + Partition tolerance)                        | Avoids conflicting configs              |
| **Partition behavior** | Majority side continues; minority read-only/unavailable           | Ensures single truth                    |
| **User experience**    | Some users see temporary unavailability, but **never wrong data** | Safety > uptime for configs             |
