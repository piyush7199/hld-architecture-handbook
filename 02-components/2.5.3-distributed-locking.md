# 2.5.3 Distributed Locking: Ensuring Mutual Exclusion

## Intuitive Explanation

Distributed Locking is a mechanism that ensures only **one process** across a distributed cluster can execute a critical
section of code at a time.

- **Problem:** If multiple servers simultaneously try to update the last piece of inventory or the same user's credit
  balance, they will cause a race condition, leading to incorrect state.
- **Goal:** Use a highly available, shared service (like ZooKeeper, etcd, or Redis) to manage a lock, guaranteeing
  **mutual exclusion** in a non-transactional environment.

---

## In-Depth Analysis

### 1. The Need for Distributed Locks

In a single-threaded application, you use a mutex or semaphore. In a distributed environment, you need a shared, highly
available service to manage the lock's state.

- **Use Case:** Critical resources like atomic inventory stock decrement, database schema migration execution, or
  ensuring a batch job runs exactly once.

### 2. Implementing a Distributed Lock (The Redis Example)

While dedicated tools are safer, Redis is often used for high-performance locking due to its speed and atomic
operations.

1. **Acquire:** Process A attempts to set a key in Redis (`SET lock:resource_id A EX 10 NX`).
    - $\text{EX}$ $\text{10}$: Sets a $\text{Time}$-$\text{To}$-$\text{Live}$ (TTL) of 10 seconds. **Crucial** to
      prevent a process crash from holding the lock indefinitely.
    - $\text{NX}$: Ensures the lock is set only if the key **Does Not Exist** ($\text{Atomicity}$).
2. **Execute:** If Process A successfully sets the key, it executes the critical code.
3. **Release:** Process A deletes the key (only if the key is still owned by A).
4. **Trade-off:** Distributed locks are inherently slow because they require a network round-trip to a centralized
   service. They trade performance for absolute correctness ($\text{Isolation}$).

### 3. Fencing and Safety (Preventing Stale Writes)

- **Fencing Token:** A constantly increasing, unique number issued with every lock acquisition. If a process holding an
  old, stale lock tries to write after the lock has expired and been reacquired by another process, its operation is
  rejected if its fencing token is lower than the current one. This ensures only the current lock holder can execute the
  critical section.

#### Key Concepts / Tradeoffs

| Feature             | Description                                                                                                                                                                                                                                                                   | Trade-off / Scalability Issue                                                                                                                |
|---------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------|
| **Lock Complexity** | Implementing a safe $\text{Distributed}$ $\text{Lock}$ is extremely difficult due to network latency, clock drift, and process crashes. It is often recommended to use mature libraries/services (ZooKeeper/etcd) rather than building it yourself with plain $\text{Redis}$. | High reliance on the stability and clock synchronization of the locking service.                                                             |
| **TTL Necessity**   | Time-To-Live prevents a crashed process from indefinitely holding a lock, ensuring system availability.                                                                                                                                                                       | A too-short $\text{TTL}$ leads to the lock expiring while a process is still working; a too-long $\text{TTL}$ blocks recovery after a crash. |

---

## ‚úèÔ∏è Design Challenge

### Problem

You are running a critical task scheduler where only one server among five should execute the weekly backup script.
Explain why using a $\text{database}$ $\text{table}$ to manage the lock (checking a row and then updating it) is
dangerous compared to using a dedicated $\text{Consensus}$ service like $\text{etcd}$ or $\text{ZooKeeper}$ for
distributed locking.

### Solution

#### ‚öôÔ∏è Option 1 ‚Äî Using a Database Table as a Lock

A naive approach might be:

```sql
SELECT * FROM locks WHERE task = 'weekly_backup';
-- If not locked
UPDATE locks SET locked_by = 'server_1', timestamp = NOW() WHERE task = 'weekly_backup';
```

At first glance, this seems simple ‚Äî but it‚Äôs **dangerous** in distributed systems.

#### üö® Why Database-Based Locking Is Dangerous

**1. No Strong Leader Election or Consensus**

Databases are not built to handle **distributed coordination** across multiple nodes.
If two servers check the lock at nearly the same time:

- Both may **read ‚Äúunlocked‚Äù** before either writes.
- Both then **update** the lock row ‚Äî resulting in duplicate execution of your backup task.

This is known as a **race condition**, and it happens because relational DBs do not provide atomic distributed
coordination semantics.

**2. No Automatic Lock Expiration (Deadlocks)**

If the server holding the lock crashes mid-task, the row remains ‚Äúlocked forever.‚Äù
Other servers can‚Äôt safely determine whether:

- The task is truly running, or
- The lock is just stale.

You‚Äôd need a manual cleanup or custom ‚Äúheartbeat‚Äù logic, which becomes fragile.

**3. Split-Brain and Network Partition Issues**

During a network partition, two groups of servers might connect to different replicas of the database:

- Each sees the lock as available,
- Each proceeds to acquire it,
- Leading to two active backups running simultaneously.

Databases typically **prioritize availability**, not **distributed consensus**, so they can‚Äôt guarantee a single,
consistent
lock owner.

**4. No Lease-Based Safety**

Databases lack **ephemeral sessions** ‚Äî they can‚Äôt automatically release a lock if the holder goes down.
This can lead to:

- Locks stuck forever,
- Or locks being overwritten unsafely if the system attempts a forced unlock.

#### ‚úÖ Option 2 ‚Äî Using a Consensus Service (e.g., etcd, ZooKeeper, Consul)

Consensus systems are **explicitly designed** for distributed coordination and locking.

**How It Works**

- Each node tries to create an ephemeral lock node (e.g., `/locks/weekly_backup`).
- The service (built on Raft/Paxos) ensures only one node successfully acquires the lock.
- If the node holding the lock dies or disconnects, the session expires ‚Üí lock is automatically released.

#### ‚úÖ Advantages of Consensus-Based Locking

| Property                               | etcd / ZooKeeper / Consul                 | Database                          |
|----------------------------------------|-------------------------------------------|-----------------------------------|
| **Atomic lock acquisition**            | ‚úÖ Guaranteed by consensus (single leader) | ‚ùå Race conditions possible        |
| **Automatic lock release (ephemeral)** | ‚úÖ On crash or disconnect                  | ‚ùå Stale locks persist             |
| **Partition safety**                   | ‚úÖ Only majority partition can hold lock   | ‚ùå Possible double locking         |
| **Built-in TTL/lease**                 | ‚úÖ Supported                               | ‚ùå Must be custom built            |
| **High availability via Raft/Paxos**   | ‚úÖ                                         | ‚ö†Ô∏è Depends on DB replication mode |

#### ‚öñÔ∏è Summary

| Aspect                   | Database Lock                   | Consensus Lock (etcd/ZooKeeper)           |
|--------------------------|---------------------------------|-------------------------------------------|
| Safety under concurrency | ‚ùå Race conditions               | ‚úÖ Strong atomicity                        |
| Fault tolerance          | ‚ùå Stale locks                   | ‚úÖ Auto-release via session                |
| Partition tolerance      | ‚ùå Split-brain risk              | ‚úÖ Quorum ensures one leader               |
| Ease of maintenance      | ‚ö†Ô∏è Custom logic required        | ‚úÖ Built-in APIs for locking               |
| Use case                 | Simple single-node coordination | Distributed coordination (critical tasks) |
