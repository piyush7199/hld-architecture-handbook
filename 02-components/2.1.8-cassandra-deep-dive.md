# 2.1.8 Cassandra Deep Dive: The Masterless Distributed Database

## Intuitive Explanation

Apache Cassandra is a **distributed NoSQL database** designed for massive scale and zero downtime. Unlike traditional databases with a single leader, Cassandra has **no master node** — every node is equal. This makes it incredibly resilient: if one server dies, the system keeps running without interruption. It's like having a team where everyone knows the job, so losing one person doesn't break the system.

- **Masterless Architecture:** No single point of failure. All nodes can accept reads and writes.
- **Linear Scalability:** Add more servers = proportionally more throughput.
- **Always-On:** Designed for 99.99% uptime with multi-datacenter replication.
- **Use Cases:** Time-series data (IoT sensors), messaging apps (WhatsApp), high-write workloads (Netflix viewing history).

---

## In-Depth Analysis

### 1. Cassandra Architecture: The Ring

Cassandra organizes nodes in a **ring topology** using **consistent hashing**:

```
              ┌─────────────────────────┐
              │  Token Range: 0 - 2^63  │
              └─────────────────────────┘
                          │
         ┌────────────────┼────────────────┐
         │                │                │
    ┌────▼────┐     ┌────▼────┐     ┌────▼────┐
    │ Node A  │     │ Node B  │     │ Node C  │
    │ Token:  │     │ Token:  │     │ Token:  │
    │   42    │     │   130   │     │   210   │
    └─────────┘     └─────────┘     └─────────┘
         │                │                │
         └────────────────┴────────────────┘
              Gossip Protocol (peer-to-peer)
```

**Key Concepts:**

| Concept | Explanation |
|---------|-------------|
| **Partition Key** | Determines which node stores the data (via consistent hashing). |
| **Token** | Each node owns a range of tokens (hash values). |
| **Virtual Nodes (vnodes)** | Each physical node handles multiple token ranges (better load balancing). |
| **Gossip Protocol** | Nodes exchange state information every second (peer-to-peer heartbeat). |
| **Replication Factor (RF)** | Number of copies of each data row (e.g., RF=3 means 3 copies). |
| **Consistency Level** | Tunable trade-off between consistency and availability. |

**Example: Data Distribution**

```
Row Key: "user:12345"
Hash(user:12345) = Token 178

Token Ranges:
- Node A: 0 - 100
- Node B: 101 - 200  ← Token 178 falls here (Primary Replica)
- Node C: 201 - 300

With RF=3:
- Primary: Node B
- Replicas: Node C, Node A (next two nodes in the ring)
```

---

### 2. Data Model: Wide-Column Store

Cassandra is a **wide-column store**, meaning data is organized differently than SQL or document databases:

#### **2.1 Terminology Mapping**

| Cassandra | SQL Equivalent | Description |
|-----------|----------------|-------------|
| **Keyspace** | Database | Top-level container |
| **Table** | Table | Collection of rows |
| **Partition Key** | Primary Key (part 1) | Determines data distribution across nodes |
| **Clustering Key** | Primary Key (part 2) | Determines sort order within a partition |
| **Column** | Column | Individual data field |

#### **2.2 Primary Key Structure**

```sql
PRIMARY KEY ((partition_key), clustering_key1, clustering_key2)
```

**Example: User Timeline Table**

```sql
CREATE TABLE user_timeline (
    user_id UUID,
    tweet_id TIMEUUID,
    content TEXT,
    created_at TIMESTAMP,
    PRIMARY KEY (user_id, tweet_id)
) WITH CLUSTERING ORDER BY (tweet_id DESC);
```

**Breakdown:**

- **Partition Key:** `user_id` — All tweets for a user are stored on the same node.
- **Clustering Key:** `tweet_id` — Tweets are sorted by time (TIMEUUID is time-ordered).
- **Query Pattern:** `SELECT * FROM user_timeline WHERE user_id = ? ORDER BY tweet_id DESC LIMIT 10;` (single-partition query, very fast).

#### **2.3 Partition Design (Critical)**

**The Golden Rule:** **Design your table around your queries.**

| Good Design | Bad Design |
|-------------|------------|
| One partition = One user's timeline (bounded size) | One partition = All tweets (unbounded, causes hotspots) |
| Single-partition queries (`WHERE user_id = ?`) | Multi-partition queries (`WHERE content CONTAINS 'hello'`) |
| Partition size < 100MB | Partition size > 1GB (wide partition problem) |

**Wide Partition Problem:**

If one partition grows too large (e.g., a celebrity with millions of followers), that single node becomes a **hotspot** (all queries for that celebrity hit one node).

**Solution: Bucketing**

```sql
-- Bad: One partition per user (unbounded)
PRIMARY KEY (user_id, tweet_id)

-- Good: Multiple partitions per user (bounded by month)
PRIMARY KEY ((user_id, bucket_month), tweet_id)

-- Query
SELECT * FROM user_timeline
WHERE user_id = ? AND bucket_month = '2024-01'
ORDER BY tweet_id DESC;
```

---

### 3. Consistency Levels: Tunable CAP Trade-offs

Cassandra lets you **tune consistency per query** (unlike most databases):

#### **3.1 Consistency Levels**

| Level | Behavior | Latency | Availability | Use Case |
|-------|----------|---------|--------------|----------|
| **ONE** | 1 replica responds | Fastest | Highest | Analytics, caching (eventual consistency OK) |
| **QUORUM** | Majority (RF/2 + 1) responds | Medium | High | Default for most apps (balance) |
| **ALL** | All replicas respond | Slowest | Lowest | Critical reads (strong consistency) |
| **LOCAL_QUORUM** | Quorum within local DC | Medium | High | Multi-DC setup (avoids cross-DC latency) |
| **EACH_QUORUM** | Quorum in each DC | High | Medium | Strong consistency across DCs |

**Example: RF=3**

```
Consistency Level = QUORUM
- Writes: Wait for 2 out of 3 replicas to ACK.
- Reads: Query 2 out of 3 replicas and return most recent value.
```

**Strong Consistency Formula:**

```
R + W > RF  (R = read replicas, W = write replicas)

Example:
- RF = 3
- Write at QUORUM (W=2)
- Read at QUORUM (R=2)
- 2 + 2 > 3 ✅ (Guarantees you always read latest write)
```

#### **3.2 Eventual Consistency (ONE/TWO)**

- **How:** Writes go to one node, replicas sync in the background.
- **Risk:** Reads might return stale data temporarily.
- **Mitigation:** Use **read repair** and **anti-entropy** (background reconciliation).

**Example:**

```
Time    | Node A | Node B | Node C
--------|--------|--------|--------
T0      | v1     | v1     | v1
T1      | v2     | v1     | v1    (Write at ONE to Node A)
T2      | v2     | v2     | v1    (Background replication)
T3      | v2     | v2     | v2    (Eventually consistent)
```

---

### 4. Write Path: Fast by Design

Cassandra is **optimized for writes** (much faster than reads):

#### **4.1 Write Path (In-Memory)**

```
Write Request
    │
    ▼
┌─────────────────────┐
│  1. Write to        │
│  CommitLog (disk)   │  ← Append-only (sequential writes, very fast)
└──────────┬──────────┘
           │
           ▼
┌─────────────────────┐
│  2. Write to        │
│  Memtable (memory)  │  ← In-memory sorted structure
└──────────┬──────────┘
           │
           ▼
┌─────────────────────┐
│  3. Return ACK      │
└─────────────────────┘
```

**Why So Fast?**

- ✅ **Sequential disk writes** (CommitLog is append-only).
- ✅ **No read-before-write** (no need to check if row exists).
- ✅ **No locks** (writes never block reads or other writes).

#### **4.2 Flush to SSTable (Background)**

When Memtable fills up (e.g., 128MB):

```
Memtable (memory)
    │
    ▼
┌─────────────────────┐
│  Flush to SSTable   │  ← Immutable file on disk (sorted)
└──────────┬──────────┘
           │
           ▼
┌─────────────────────┐
│  SSTables (disk)    │
│  - SSTable 1 (old)  │
│  - SSTable 2 (old)  │
│  - SSTable 3 (new)  │
└─────────────────────┘
           │
           ▼
┌─────────────────────┐
│  Compaction         │  ← Merge SSTables, remove tombstones
└─────────────────────┘
```

**SSTable (Sorted String Table):**

- **Immutable:** Once written, never modified.
- **Sorted:** Rows sorted by primary key.
- **Indexed:** Includes partition index, bloom filter.

---

### 5. Read Path: More Complex

Reads are **slower than writes** because data might be scattered across multiple SSTables:

#### **5.1 Read Path (Multi-Level)**

```
Read Request
    │
    ▼
┌─────────────────────┐
│  1. Check Memtable  │  ← In-memory (fastest)
└──────────┬──────────┘
           │
           ▼
┌─────────────────────┐
│  2. Check Row Cache │  ← Optional (caches frequently read rows)
└──────────┬──────────┘
           │
           ▼
┌─────────────────────┐
│  3. Bloom Filter    │  ← Quickly check if SSTable might have the row
└──────────┬──────────┘
           │
           ▼ (if bloom says "maybe")
┌─────────────────────┐
│  4. Partition Index │  ← Find offset in SSTable
└──────────┬──────────┘
           │
           ▼
┌─────────────────────┐
│  5. Read SSTable    │  ← Disk read (slowest)
└──────────┬──────────┘
           │
           ▼
┌─────────────────────┐
│  6. Merge Results   │  ← Merge from multiple SSTables (timestamp-based)
└──────────┬──────────┘
           │
           ▼
┌─────────────────────┐
│  Return Result      │
└─────────────────────┘
```

**Why Reads Are Slower:**

- ❌ **Multiple SSTables:** Must check all SSTables (can be 10+).
- ❌ **Disk I/O:** SSTables are on disk (unless cached).
- ❌ **Merge overhead:** Must merge rows from multiple SSTables by timestamp.

**Optimization: Compaction**

- **Purpose:** Merge SSTables to reduce read amplification.
- **Strategy:** Size-Tiered (STCS), Leveled (LCS), Time-Window (TWCS).
- **Trade-off:** Compaction uses CPU/disk I/O (background task).

---

### 6. Replication and Multi-Datacenter Setup

#### **6.1 Replication Strategy**

**Simple Strategy (Single DC):**

```sql
CREATE KEYSPACE my_keyspace
WITH REPLICATION = {
    'class': 'SimpleStrategy',
    'replication_factor': 3
};
```

**Network Topology Strategy (Multi-DC):**

```sql
CREATE KEYSPACE my_keyspace
WITH REPLICATION = {
    'class': 'NetworkTopologyStrategy',
    'DC1': 3,  -- 3 replicas in DC1
    'DC2': 2   -- 2 replicas in DC2
};
```

**Topology:**

```
┌──────────────────────────────────────────────────────┐
│                   Cassandra Ring                      │
├──────────────────────────────────────────────────────┤
│  DC1 (US-East)              DC2 (EU-West)            │
│    ┌──────┐                   ┌──────┐              │
│    │Node 1│                   │Node 4│              │
│    └──────┘                   └──────┘              │
│    ┌──────┐                   ┌──────┐              │
│    │Node 2│                   │Node 5│              │
│    └──────┘                   └──────┘              │
│    ┌──────┐                                          │
│    │Node 3│                                          │
│    └──────┘                                          │
└──────────────────────────────────────────────────────┘
       │                             │
       └─────────Async Replication────┘
```

**Writes with LOCAL_QUORUM:**

1. Write arrives at coordinator in DC1.
2. Coordinator writes to 2 out of 3 local replicas (QUORUM in DC1).
3. Coordinator returns ACK to client (fast, no cross-DC latency).
4. Background async replication to DC2 (eventual consistency across DCs).

**Benefits:**

- ✅ **Low latency:** LOCAL_QUORUM avoids cross-region latency.
- ✅ **Disaster recovery:** If DC1 fails, DC2 can serve traffic.
- ✅ **Geo-distribution:** Serve users from nearest DC.

---

### 7. Failure Handling: Hinted Handoff & Repair

#### **7.1 Hinted Handoff**

**Problem:** Node C is temporarily down. Write targeted at C needs to succeed.

**Solution:**

```
Write Request (RF=3: A, B, C)
    │
    ▼
Coordinator (Node A)
    ├─> Node A (OK)
    ├─> Node B (OK)
    └─> Node C (DOWN)
          │
          ▼
    Node A stores "hint" for C
    (writes to local disk: "C owes this write")
    │
    ▼
Node C comes back online
    │
    ▼
Node A replays hints to C
```

**Hinted Handoff Window:** Default 3 hours. If node is down longer, hints are dropped (must run repair).

#### **7.2 Read Repair**

**Problem:** Replicas are inconsistent (e.g., Node B missed a write).

**Solution:**

```
Read at QUORUM (RF=3)
    │
    ▼
Coordinator queries 2 replicas:
- Node A: version 5
- Node B: version 4  ← Stale

Coordinator detects mismatch:
    │
    ▼
Return version 5 to client (latest)
    │
    ▼
Background: Write version 5 to Node B (repair)
```

#### **7.3 Anti-Entropy Repair (Manual)**

**Purpose:** Full reconciliation across all replicas (run periodically).

**Command:**

```bash
nodetool repair -pr  # Repair primary range only
```

**When to Run:**

- After node failure/recovery.
- Periodically (weekly/monthly) to ensure consistency.
- After exceeding hinted handoff window.

---

### 8. Data Modeling Best Practices

#### **8.1 Query-Driven Design**

**Rule:** Model data around queries, not entities.

**Example: Social Media App**

**Queries Needed:**

1. Get user's timeline (latest 20 posts).
2. Get post details by post_id.
3. Get all comments for a post.

**Bad Design (RDBMS-style):**

```sql
-- Single posts table (requires scatter-gather queries)
CREATE TABLE posts (
    post_id UUID PRIMARY KEY,
    user_id UUID,
    content TEXT,
    created_at TIMESTAMP
);

-- Query requires full table scan (slow)
SELECT * FROM posts WHERE user_id = ? ORDER BY created_at DESC LIMIT 20;
```

**Good Design (Cassandra-style):**

```sql
-- Table 1: User timeline (optimized for Query 1)
CREATE TABLE user_timeline (
    user_id UUID,
    post_id TIMEUUID,
    content TEXT,
    created_at TIMESTAMP,
    PRIMARY KEY (user_id, post_id)
) WITH CLUSTERING ORDER BY (post_id DESC);

-- Table 2: Post details (optimized for Query 2)
CREATE TABLE posts_by_id (
    post_id UUID PRIMARY KEY,
    user_id UUID,
    content TEXT,
    created_at TIMESTAMP
);

-- Table 3: Comments (optimized for Query 3)
CREATE TABLE post_comments (
    post_id UUID,
    comment_id TIMEUUID,
    user_id UUID,
    content TEXT,
    PRIMARY KEY (post_id, comment_id)
) WITH CLUSTERING ORDER BY (comment_id DESC);
```

**Data Duplication:**

- ✅ **Accepted trade-off** in Cassandra (denormalization).
- ✅ **Faster reads** (single-partition queries).
- ❌ **More storage** (but storage is cheap).
- ❌ **Write overhead** (must update multiple tables).

#### **8.2 Avoid Anti-Patterns**

| Anti-Pattern | Why Bad | Solution |
|--------------|---------|----------|
| **Secondary indexes** | Requires scatter-gather (queries all nodes) | Use materialized views or separate tables |
| **ALLOW FILTERING** | Full table scan (extremely slow) | Redesign table to match query |
| **Large partitions (>100MB)** | Hotspots, slow queries | Use bucketing (e.g., partition by month) |
| **Unbounded partitions** | Partition grows forever (celebrity problem) | Time-bucket or limit partition size |
| **Multi-partition queries** | Slow (must query many nodes) | Single-partition queries only |

**Example: Fixing Secondary Index**

```sql
-- Bad: Secondary index
CREATE TABLE users (
    user_id UUID PRIMARY KEY,
    email TEXT,
    name TEXT
);
CREATE INDEX ON users(email);  -- Queries all nodes!

-- Good: Separate table
CREATE TABLE users_by_email (
    email TEXT PRIMARY KEY,
    user_id UUID,
    name TEXT
);
```

---

### 9. Performance Tuning

#### **9.1 Compaction Strategy**

| Strategy | Best For | Trade-offs |
|----------|----------|------------|
| **Size-Tiered (STCS)** | Write-heavy workloads | ❌ Read amplification (many SSTables) |
| **Leveled (LCS)** | Read-heavy workloads | ❌ Write amplification (more compaction) |
| **Time-Window (TWCS)** | Time-series data (TTL) | ✅ Efficient TTL expiry (drops entire SSTable) |

**Example: Time-Series Data**

```sql
CREATE TABLE sensor_data (
    sensor_id UUID,
    timestamp TIMESTAMP,
    value DOUBLE,
    PRIMARY KEY (sensor_id, timestamp)
) WITH CLUSTERING ORDER BY (timestamp DESC)
  AND compaction = {'class': 'TimeWindowCompactionStrategy', 'compaction_window_unit': 'DAYS', 'compaction_window_size': 1}
  AND default_time_to_live = 2592000;  -- 30 days
```

#### **9.2 Read Optimization**

**Row Cache:**

- Caches entire rows in memory (like Redis).
- Use for hot rows (e.g., celebrity profiles).

```sql
ALTER TABLE users WITH caching = {'keys': 'ALL', 'rows_per_partition': '100'};
```

**Key Cache:**

- Caches partition key offsets (always enabled by default).

**Partition Index:**

- Index inside each SSTable (automatically managed).

#### **9.3 Monitoring Key Metrics**

| Metric | What to Monitor | Threshold |
|--------|-----------------|-----------|
| **Write latency** | p99 latency | < 10ms |
| **Read latency** | p99 latency | < 50ms |
| **Compaction pending** | Number of pending compaction tasks | < 10 |
| **Memtable flush** | Flush rate | Should be steady |
| **Gossip state** | Node UP/DOWN status | All nodes UP |
| **Replication lag** | Hinted handoff queue size | < 1000 |

**Tools:**

- **nodetool:** CLI for ops (`nodetool status`, `nodetool tpstats`).
- **Prometheus + Cassandra Exporter:** Metrics scraping.
- **Grafana:** Visualization.

---

### 10. When to Use Cassandra

#### **✅ Use Cassandra When:**

1. **High write throughput** — Logs, time-series data, IoT sensors (millions of writes/sec).
2. **Linear scalability** — Need to scale horizontally (add nodes for more capacity).
3. **No single point of failure** — Always-on, multi-datacenter replication.
4. **Predictable query patterns** — Know your queries upfront (single-partition queries).
5. **Time-series data with TTL** — Automatic expiration (TWCS compaction).
6. **Geo-distributed users** — Multi-region, low-latency reads via LOCAL_QUORUM.

#### **❌ Don't Use Cassandra When:**

1. **Complex joins and transactions** — Use PostgreSQL or MySQL instead.
2. **Ad-hoc queries** — Cassandra requires query-driven design upfront.
3. **Strong consistency required** — ACID guarantees easier in RDBMS.
4. **Small dataset (< 1TB)** — Overhead of Cassandra not worth it (use PostgreSQL).
5. **Heavy updates/deletes** — Tombstone overhead (eventual consistency issues).

---

### 11. Real-World Examples

| Company | Use Case | Why Cassandra? |
|---------|----------|----------------|
| **Netflix** | Viewing history, recommendations | 2.5 trillion writes/day, multi-region, always-on |
| **Apple** | iCloud backend | Multi-DC, billions of users, high availability |
| **Instagram** | User feeds, DMs | Time-series data, high write throughput |
| **Discord** | Message history | Handles spikes (millions of messages/sec during events) |
| **Uber** | Trip history, location tracking | Time-series data, geo-distributed |
| **Spotify** | User playlists, listening history | High write throughput, multi-region |

---

### 12. Cassandra vs. Other Databases

| Feature | Cassandra | PostgreSQL | MongoDB | DynamoDB |
|---------|-----------|------------|---------|----------|
| **Architecture** | Masterless (peer-to-peer) | Leader-follower | Leader-follower | Managed (AWS) |
| **Consistency** | Tunable (eventual to strong) | Strong (ACID) | Eventual (default) | Eventual (default) |
| **Scalability** | Linear (horizontal) | Vertical (limited horizontal) | Horizontal | Fully managed |
| **Writes** | Very fast (O(1)) | Moderate | Fast | Fast |
| **Reads** | Slower (multi-SSTable) | Fast (indexes) | Fast | Fast |
| **Joins** | Not supported | Supported | Limited | Not supported |
| **Use Case** | High-write, time-series | ACID, relational | Flexible documents | Serverless, AWS-native |

---

### 13. Common Anti-Patterns

#### ❌ **1. Using SELECT * or ALLOW FILTERING**

**Problem:** Full table scan (queries all partitions).

```sql
-- Bad (scans all nodes)
SELECT * FROM users WHERE age > 25 ALLOW FILTERING;

-- Good (single-partition query)
SELECT * FROM users WHERE user_id = ?;
```

#### ❌ **2. Unbounded Partitions**

**Problem:** One partition grows forever (hotspot).

**Solution:** Time-bucket partitions.

```sql
-- Bad
PRIMARY KEY (user_id, event_time)

-- Good
PRIMARY KEY ((user_id, bucket_day), event_time)
```

#### ❌ **3. Using Secondary Indexes**

**Problem:** Queries all nodes (slow).

**Solution:** Materialized view or separate table.

#### ❌ **4. Not Running Repair**

**Problem:** Replicas drift over time.

**Solution:** Run `nodetool repair` weekly.

---

### 14. Summary

| What Cassandra Does Best | What Cassandra Struggles With |
|---------------------------|------------------------------|
| ✅ Massive write throughput (millions/sec) | ❌ Complex queries (joins, aggregations) |
| ✅ Linear scalability (just add nodes) | ❌ Strong consistency (eventual by default) |
| ✅ No single point of failure (masterless) | ❌ Ad-hoc queries (requires upfront design) |
| ✅ Multi-datacenter replication | ❌ Heavy updates/deletes (tombstones) |
| ✅ Time-series data with TTL | ❌ Small datasets (overkill) |

---

### 15. References

- **Apache Cassandra Documentation:** [https://cassandra.apache.org/doc/](https://cassandra.apache.org/doc/)
- **DataStax Academy (Free Courses):** [https://academy.datastax.com/](https://academy.datastax.com/)
- **Cassandra: The Definitive Guide (Book):** By Jeff Carpenter & Eben Hewitt
- **Related Chapters:**
  - [2.1.2 NoSQL Deep Dive](./2.1.2-no-sql-deep-dive.md) — BASE principles
  - [2.2.2 Consistent Hashing](./2.2.2-consistent-hashing.md) — How Cassandra distributes data
  - [2.1.4 Database Scaling](./2.1.4-database-scaling.md) — Horizontal scaling strategies
  - [2.5.2 Consensus Algorithms](./2.5.2-consensus-algorithms.md) — Paxos in Cassandra

---

## ✏️ Design Challenge

### Problem

You're designing a **time-series IoT platform** that collects sensor data from **10 million devices** worldwide. Requirements:

1. **Write throughput:** 100,000 writes/sec (each device reports every 10 seconds)
2. **Data retention:** 90 days (auto-delete old data)
3. **Query pattern:** Fetch last 24 hours of data for a specific device
4. **High availability:** No downtime during datacenter failures
5. **Multi-region:** Devices spread across US, EU, Asia

Your team is considering **Cassandra vs. PostgreSQL with TimescaleDB**. 

**Question:** Which database would you choose, what partition key design would you use, and how would you handle the time-series nature of the data?

### Solution

#### 🧩 Scenario

- **System:** IoT sensor data collection
- **Scale:** 10M devices × 100 bytes/reading × 6 readings/min = **60GB/hour**
- **Write load:** 100,000 writes/sec
- **Query pattern:** `SELECT * FROM sensor_data WHERE device_id = ? AND timestamp >= now() - 24h`
- **Retention:** 90 days (auto-delete)
- **Geographic distribution:** Multi-region (US, EU, Asia)

#### ✅ Goal

- Handle massive write throughput without throttling
- Efficient time-range queries (last 24 hours)
- Automatic data expiration (TTL)
- High availability across regions
- Linear scalability (add nodes = more capacity)

#### ⚙️ Solution: Cassandra (Clear Winner for This Use Case)

**Why Cassandra?**

| Requirement | Cassandra | PostgreSQL + TimescaleDB |
|-------------|-----------|--------------------------|
| **Write throughput** | ✅ Excellent (100k+ writes/sec per node) | ⚠️ Limited (10k-20k writes/sec on single master) |
| **Linear scaling** | ✅ Add nodes = more capacity | ❌ Write bottleneck on master |
| **TTL** | ✅ Built-in (automatic deletion) | ⚠️ Manual DELETE or partition drops |
| **Multi-region** | ✅ Native multi-DC replication | ⚠️ Complex (streaming replication) |
| **High availability** | ✅ Masterless (no single point of failure) | ⚠️ Master failover required |
| **Time-series queries** | ✅ Excellent (with proper partition key) | ✅ Excellent (TimescaleDB hypertables) |

**Recommended Data Model:**

```sql
CREATE TABLE sensor_data (
    device_id UUID,
    bucket_day TEXT,       -- e.g., "2024-01-15" (prevents wide partitions)
    timestamp TIMESTAMP,
    temperature FLOAT,
    humidity FLOAT,
    battery_level INT,
    PRIMARY KEY ((device_id, bucket_day), timestamp)
) WITH CLUSTERING ORDER BY (timestamp DESC)
  AND compaction = {'class': 'TimeWindowCompactionStrategy', 'compaction_window_unit': 'DAYS', 'compaction_window_size': 1}
  AND default_time_to_live = 7776000;  -- 90 days in seconds
```

**Key Design Decisions:**

1. **Partition Key: `(device_id, bucket_day)`**
   - **Why composite:** Prevents unbounded partitions (one device = 90 days of data)
   - **Bucketing by day:** Each partition contains 24 hours of data (~8,640 readings)
   - **Partition size:** 8,640 readings × 100 bytes = ~864KB (well under 100MB limit)

2. **Clustering Key: `timestamp DESC`**
   - **Why DESC:** Most recent data first (common query pattern)
   - **Benefit:** `SELECT * WHERE device_id = ? AND bucket_day = '2024-01-15' LIMIT 100` is $\text{O}(1)$

3. **TTL: 90 days**
   - **Automatic deletion:** No manual cleanup jobs
   - **Efficient:** TWCS compaction drops entire SSTables (no tombstone overhead)

4. **Time-Window Compaction (TWCS)**
   - **Optimized for time-series:** Groups data by time windows
   - **Fast expiration:** Drops entire SSTable when all data expires (no compaction overhead)

**Example Queries:**

```sql
-- Query last 24 hours for device
SELECT * FROM sensor_data
WHERE device_id = 123e4567-e89b-12d3-a456-426614174000
  AND bucket_day IN ('2024-01-15', '2024-01-14')
  AND timestamp >= '2024-01-14 10:00:00'
ORDER BY timestamp DESC;

-- Query last hour (single partition)
SELECT * FROM sensor_data
WHERE device_id = 123e4567-e89b-12d3-a456-426614174000
  AND bucket_day = '2024-01-15'
  AND timestamp >= '2024-01-15 09:00:00'
ORDER BY timestamp DESC;
```

#### ⚠️ Trade-offs and Challenges

**Challenge 1: Query Spans Multiple Days**

**Problem:** Query for "last 24 hours" might span 2 bucket_days.

**Solution:**
```python
# Application logic
today = datetime.now().strftime('%Y-%m-%d')
yesterday = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')

# Query both partitions
query = """
    SELECT * FROM sensor_data
    WHERE device_id = ?
      AND bucket_day IN (?, ?)
      AND timestamp >= ?
    ORDER BY timestamp DESC
"""
# Cassandra driver merges and sorts results
```

**Challenge 2: Hot Partitions (Celebrity Device)**

**Problem:** One device generates 10x more data (e.g., high-frequency sensor).

**Solution:** Add sub-bucketing by hour:
```sql
PRIMARY KEY ((device_id, bucket_day, bucket_hour), timestamp)
```

**Challenge 3: Consistency for Recent Data**

**Problem:** Eventual consistency might show stale data.

**Solution:** Use **LOCAL_QUORUM** for reads/writes:
```python
# Write with LOCAL_QUORUM (2 out of 3 local replicas)
session.execute(query, consistency_level=ConsistencyLevel.LOCAL_QUORUM)

# Read with LOCAL_QUORUM (guaranteed to see recent writes)
session.execute(query, consistency_level=ConsistencyLevel.LOCAL_QUORUM)
```

#### 🧠 Multi-Region Architecture

```
Region: US-East (Primary)
  ├─ Node 1, Node 2, Node 3 (RF=3)
  └─ LOCAL_QUORUM writes/reads

Region: EU-West (Replica)
  ├─ Node 4, Node 5 (RF=2)
  └─ LOCAL_QUORUM writes/reads

Region: Asia-Pacific (Replica)
  ├─ Node 6, Node 7 (RF=2)
  └─ LOCAL_QUORUM writes/reads

Cross-DC Replication: Async (eventual consistency)
Benefit: Each region serves local devices with low latency
```

**Configuration:**
```sql
CREATE KEYSPACE iot_data WITH REPLICATION = {
  'class': 'NetworkTopologyStrategy',
  'us-east': 3,
  'eu-west': 2,
  'asia-pacific': 2
};
```

#### ✅ Final Answer

| Aspect | Decision | Reason |
|--------|----------|--------|
| **Database** | **Cassandra** | Designed for high-write workloads, linear scaling, masterless HA |
| **Partition Key** | `(device_id, bucket_day)` | Bounded partitions (~864KB/day), prevents hotspots |
| **Clustering Key** | `timestamp DESC` | Recent data first, efficient range queries |
| **TTL** | 90 days | Automatic deletion, no manual cleanup |
| **Compaction** | Time-Window (TWCS) | Optimized for time-series, fast expiration |
| **Consistency** | LOCAL_QUORUM | Balance between consistency and latency |
| **Replication** | Multi-DC (RF=3 US, RF=2 EU/Asia) | High availability, low latency per region |
| **Trade-off Accepted** | Eventual consistency across regions | Gain: High availability, no single point of failure |

**Key Metrics:**
- **Write latency:** < 5ms (p99) per region
- **Query latency:** < 10ms (p99) for single-day queries
- **Storage:** ~5.4TB for 90 days (60GB/hour × 24 × 90 days)
- **Cost:** ~$5,000/month (9 nodes × $500/node, reserved instances)

**Why NOT PostgreSQL:**
- Single-master write bottleneck (cannot handle 100k writes/sec)
- Complex multi-region setup
- Manual TTL management (DELETE jobs = performance hit)
- Vertical scaling limits

**When to Reconsider:**
- If write volume drops below 10k writes/sec → PostgreSQL + TimescaleDB viable
- If need complex analytics (JOINs, aggregations) → Consider data warehouse (Redshift, ClickHouse)

