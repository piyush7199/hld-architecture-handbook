# 2.2.1 Caching Deep Dive: Speed, Cost, and Cache Invalidation

## Intuitive Explanation

Caching is the most effective way to improve the performance and reduce the cost of any large-scale system.

- **Cache:** A fast, temporary storage layer (usually $\text{RAM}$ or ultra-fast $\text{SSD}$) that stores copies of
  frequently accessed data.
- **The Goal:** Avoid expensive operations (database queries, external $\text{API}$ calls, complex calculations) by
  serving the result instantly from memory.
- **The Trade-off:** You exchange the performance gain for the risk of stale data (the cache might hold old data while
  the source data has been updated).

---

## In-Depth Analysis

### 1. Where to Place Caches

The closer the cache is to the user, the faster the response, but the harder the synchronization.

| Cache Level             | Location                                                                  | What it Caches                                                                                  | Use Case                                                                      |
|-------------------------|---------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------|
| Client/Browser          | User's browser memory or disk.                                            | Static assets, minor user data.                                                                 | Avoids network latency entirely ($\text{TTL}$ set via $\text{HTTP}$ headers). |
| CDN (Edge)              | Geographically distributed servers (Edge locations).                      | Images, $\text{CSS}$, $\text{JS}$ files, entire $\text{HTML}$ pages.                            | Reduces latency globally, dramatically lowers origin server load.             |
| API Gateway             | Proxy layer before the backend services.                                  | Authentication tokens, $\text{Rate}$ $\text{Limit}$ counters, expensive $\text{API}$ responses. | Protects backend services from redundant external calls.                      |
| Application (App-Level) | Dedicated cluster (e.g., Redis, Memcached) shared by application servers. | Database query results, session data, user profiles.                                            | Best performance for dynamic data reads.                                      |
| Database                | Built-in to the $\text{DB}$ engine (query result or row buffer).          | Recently accessed rows or query plans.                                                          | Least effective for distributed scaling, but useful for basic performance.    |

### 2. Cache Architectures (The Write Problem)

These patterns dictate how data is written, determining consistency and latency.

| Architecture               | Write Flow                                                                                                                                                                                     | Consistency / Stale Data                                                                                                | Best For                                                                                                            |
|----------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------|
| Cache-Aside (Lazy Loading) | $\text{Application}$ writes to $\text{DB}$. If data is needed later, $\text{Application}$ checks $\text{Cache}$. If miss, $\text{Application}$ reads $\text{DB}$ and populates $\text{Cache}$. | Low Write Latency. High risk of cache stampede or initial stale data after update.                                      | Read-heavy workloads where stale data is acceptable for a short time (e.g., blog posts, social feeds).              |
| Write-Through              | $\text{Application}$ writes to $\text{Cache}$ $\text{AND}$ $\text{DB}$ **synchronously**.                                                                                                      | High Consistency. Data is guaranteed to be in the cache immediately after the write.                                    | Small datasets where high consistency is mandatory (e.g., core user profile).                                       |
| Write-Back (Write-Behind)  | $\text{Application}$ writes to $\text{Cache}$. $\text{Cache}$ asynchronously writes to $\text{DB}$ in the background.                                                                          | Lowest Write Latency. High risk of data loss if the cache server fails before the data is persisted to the $\text{DB}$. | High-speed, high-volume logging or $\text{IoT}$ sensor data where speed is critical and durability can be eventual. |

### 3. Cache Invalidation and Eviction

- **Cache Invalidation:** The mechanism for removing stale data.
    - **TTL (Time-To-Live):** The most common method. Data expires after a set period. Simple and reliable.
    - **Write-Invalidate:** When the $\text{DB}$ is updated, a signal is sent to the cache to proactively delete the
      key. Used for higher consistency.

- **Eviction Policies:** What happens when the cache fills up (runs out of $\text{RAM}$)?
    - **LRU (Least Recently Used):** Removes the data that hasn't been accessed for the longest time. (Most common and
      effective).
    - **LFU (Least Frequently Used):** Removes the data that has the fewest accesses.
    - **FIFO (First-In, First-Out):** Removes the data that was added earliest.

### 4. Cache Issues at Scale

- **Cache Stampede:** When a highly requested key expires, thousands of concurrent requests all hit the database
  simultaneously, causing it to crash.
    - **Mitigation:** Use a Mutex Lock or $\text{Single}$ $\text{Flight}$ $\text{Request}$ pattern: only the first
      request is allowed to query the $\text{DB}$ and write back to the cache; all others wait for the result.
- **Cache Miss Ratio:** The ratio of requests that miss the cache and hit the database. A high ratio indicates poor
  performance or a misconfigured $\text{TTL}$.

### 5. Caching Strategy Comparison Matrix

| Strategy | Write Latency | Read Latency | Consistency | Complexity | Data Loss Risk | Best Use Case |
|----------|--------------|--------------|-------------|------------|----------------|---------------|
| **Cache-Aside** | Low (DB only) | Variable (cache hit: low, miss: high) | Eventually consistent | Low | None | Read-heavy, stale data OK (blogs, feeds) |
| **Write-Through** | High (cache + DB sync) | Low (always cached) | Strong | Medium | None | Small data, consistency critical (user profiles) |
| **Write-Back** | Very Low (cache only) | Low (always cached) | Weak | High | High (if cache crashes) | High write volume, eventual durability OK (IoT, logs) |
| **Refresh-Ahead** | N/A (background) | Low (predictive) | Predictably stale | High | None | Predictable access patterns (reports) |

### 6. Cache Eviction Policy Comparison

| Policy | How It Works | Best For | Pros | Cons |
|--------|-------------|----------|------|------|
| **LRU** | Evicts least recently accessed | General purpose | Fair, works well in most cases | Doesn't consider frequency |
| **LFU** | Evicts least frequently accessed | Hot data identification | Keeps popular data | Complex to implement, slow cold start |
| **FIFO** | Evicts oldest entry | Simple, predictable | Easy to implement | Ignores access patterns |
| **TTL** | Evicts after time expires | Time-sensitive data | Simple, predictable expiry | May evict still-useful data |
| **Random** | Evicts random entry | Low overhead needed | Very simple, low CPU | Poor cache hit rate |

---

## ⚠️ Common Caching Anti-Patterns

### Anti-Pattern 1: Caching Everything

**Problem:** "Let's cache every single database query!"

**Why It's Wrong:**
- Wastes cache memory on rarely-accessed data
- Reduces cache hit rate for hot data
- Increases cache invalidation complexity
- Higher infrastructure costs

**Better Approach:**
- Cache only frequently accessed data (80/20 rule)
- Monitor cache hit rates per key
- Set appropriate TTLs based on access patterns

**Example:**
```python
❌ Bad:
cache.set(f"user:{user_id}:last_login_ip", ip, ttl=3600)
cache.set(f"user:{user_id}:password_reset_token", token, ttl=3600)
# Rarely accessed data cluttering cache

✅ Good:
# Only cache hot data
cache.set(f"user:{user_id}:profile", profile, ttl=300)  # Frequently accessed
# Don't cache: password tokens, audit logs, etc.
```

---

### Anti-Pattern 2: No Cache Invalidation Strategy

**Problem:**
```python
# Set cache with infinite TTL
cache.set(f"product:{product_id}", product_data)

# Later: product price updated in DB
db.update("UPDATE products SET price = 99.99 WHERE id = ?", product_id)
# Cache still shows old price forever!
```

**Why It's Wrong:**
- Users see stale data indefinitely
- No way to update cached data
- Inconsistency between cache and database

**Better Approach:**
```python
# Always set TTL
cache.set(f"product:{product_id}", product_data, ttl=600)

# On update: invalidate cache
db.update("UPDATE products SET price = 99.99 WHERE id = ?", product_id)
cache.delete(f"product:{product_id}")  # Force refresh on next read
```

---

### Anti-Pattern 3: Cache Stampede (Thundering Herd)

**Problem:**
```python
# Popular key expires
# 10,000 concurrent requests all miss cache simultaneously
for i in range(10000):
    data = cache.get("popular_key")
    if data is None:
        data = expensive_db_query()  # 10,000 concurrent DB queries!
        cache.set("popular_key", data)
```

**Why It's Wrong:**
- Database gets overwhelmed with identical queries
- Can cause cascading failures
- Wastes resources

**Better Approach:**
```python
# Use mutex/lock pattern (single-flight)
import threading

locks = {}

def get_with_lock(key):
    data = cache.get(key)
    if data is not None:
        return data
    
    # Acquire lock for this key
    if key not in locks:
        locks[key] = threading.Lock()
    
    with locks[key]:
        # Double-check after acquiring lock
        data = cache.get(key)
        if data is not None:
            return data
        
        # Only one thread reaches here
        data = expensive_db_query()
        cache.set(key, data, ttl=600)
        return data
```

**Alternative: Probabilistic Early Expiration**
```python
import random

def get_with_early_expiration(key, ttl=600):
    data, expiry = cache.get_with_expiry(key)
    if data is None:
        return refresh_cache(key, ttl)
    
    # Refresh cache probabilistically before expiry
    time_to_expiry = expiry - time.now()
    if time_to_expiry < ttl * 0.1:  # Last 10% of TTL
        if random.random() < 0.1:  # 10% chance
            return refresh_cache(key, ttl)
    
    return data
```

---

### Anti-Pattern 4: Ignoring Cache Warming

**Problem:**
```python
# After deployment, cache is empty
# First wave of users all hit database (cold start)
```

**Why It's Wrong:**
- Poor user experience after deployment
- Database spike can cause outages
- Cache hit rate starts at 0%

**Better Approach:**
```python
# Warm cache on startup
def warm_cache():
    popular_products = db.query("SELECT * FROM products WHERE is_popular = true")
    for product in popular_products:
        cache.set(f"product:{product.id}", product, ttl=600)
    
    log.info(f"Warmed cache with {len(popular_products)} products")

# Run on app startup
warm_cache()
```

---

### Anti-Pattern 5: Caching Personalized Data Globally

**Problem:**
```python
# Caching user-specific data with global key
user_id = get_current_user()
recommendations = cache.get("recommendations")  # Same key for all users!
```

**Why It's Wrong:**
- User A sees User B's recommendations
- Security/privacy issue
- Incorrect data shown to users

**Better Approach:**
```python
# Include user ID in cache key
user_id = get_current_user()
cache_key = f"recommendations:user:{user_id}"
recommendations = cache.get(cache_key)
```

---

### Anti-Pattern 6: Not Monitoring Cache Performance

**Problem:** No visibility into cache effectiveness

**Why It's Wrong:**
- Can't identify problems (low hit rate, memory pressure)
- Can't optimize TTLs or cache size
- Wasting resources on ineffective caching

**Better Approach:**
```python
# Track cache metrics
from prometheus_client import Counter, Histogram

cache_hits = Counter('cache_hits', 'Cache hits', ['key_type'])
cache_misses = Counter('cache_misses', 'Cache misses', ['key_type'])
cache_latency = Histogram('cache_latency', 'Cache operation latency')

def get_from_cache(key, key_type="default"):
    with cache_latency.time():
        data = cache.get(key)
        if data is not None:
            cache_hits.labels(key_type=key_type).inc()
            return data
        else:
            cache_misses.labels(key_type=key_type).inc()
            return None

# Monitor: cache hit rate = hits / (hits + misses)
# Target: > 80% hit rate
```

---

### Anti-Pattern 7: Cache Key Collisions

**Problem:**
```python
# Poor key naming
cache.set(f"user:123", user_data)
cache.set(f"user:123", order_data)  # Overwrites user data!
```

**Why It's Wrong:**
- Different data types overwrite each other
- Data corruption
- Hard to debug

**Better Approach:**
```python
# Namespace your keys
cache.set(f"user:profile:123", user_data)
cache.set(f"user:orders:123", order_data)

# Or use key prefixes
class CacheKeys:
    USER_PROFILE = "user:profile:{user_id}"
    USER_ORDERS = "user:orders:{user_id}"
    PRODUCT = "product:{product_id}"
    
cache.set(CacheKeys.USER_PROFILE.format(user_id=123), user_data)
```

---

### Anti-Pattern 8: Large Cache Values

**Problem:**
```python
# Caching huge objects
user_with_everything = {
    "id": 123,
    "profile": {...},
    "orders": [... 10,000 orders ...],  # 50 MB!
    "activity_log": [... 100,000 events ...],  # 200 MB!
}
cache.set(f"user:123", user_with_everything)  # 250 MB per user!
```

**Why It's Wrong:**
- Wastes memory
- Slow serialization/deserialization
- Network transfer overhead
- Most of the data rarely accessed

**Better Approach:**
```python
# Cache granularly
cache.set(f"user:123:profile", user_profile)  # 1 KB
cache.set(f"user:123:recent_orders", recent_orders[:10])  # 10 KB
# Don't cache: full order history, full activity log

# Fetch only what you need
profile = cache.get(f"user:123:profile")
```

---

## 💡 Caching Best Practices

| Practice | Description | Benefit |
|----------|-------------|---------|
| **Set appropriate TTLs** | Match TTL to data change frequency | Balance freshness and hit rate |
| **Namespace keys** | Use prefixes like `user:`, `product:` | Avoid collisions |
| **Cache granularly** | Cache small, specific pieces of data | Reduce memory usage |
| **Warm cache proactively** | Pre-populate on startup | Better cold start performance |
| **Monitor hit rate** | Track cache effectiveness | Identify optimization opportunities |
| **Handle cache failures** | Gracefully degrade to DB | Maintain availability |
| **Use compression** | Compress large cached values | Save memory |
| **Implement circuit breakers** | Protect against cache outages | Prevent cascading failures |

---

## ✏️ Design Challenge

### Problem

You are serving the main user profile page of a social network. This data is read $1,000$ times more often than it is
written. If you update the $\text{DB}$, the profile must be visible to everyone within $10$ seconds. Which cache
architecture (Cache-Aside or Write-Through) would you choose, and what $\text{TTL}$ would you assign to balance
consistency and load reduction?

### Solution

#### 🧩 Scenario Summary

| Aspect               | Value                                                            |
|----------------------|------------------------------------------------------------------|
| **Workload Type**    | Read-heavy (1,000× reads vs writes)                              |
| **Consistency Need** | Moderate – visible to everyone within 10 seconds                 |
| **Primary Goal**     | Reduce DB load while keeping cache reasonably fresh              |
| **Constraint**       | Cache should not serve stale data for more than 10s after update |

#### ✅ Step 1: Choose the Cache Architecture

**Option 1 – Write-Through Cache**

- Every write goes through the cache first; the cache updates the DB.
- Keeps cache always fresh (strong consistency), but:
    - Adds write latency.
    - Increases coupling between app and cache.
    - Inefficient for a system where writes are rare (1/1000 ratio).

**Option 2 – Cache-Aside (Lazy Loading)**

- Application reads from cache:
    - If miss → fetch from DB, then cache it.
    - If hit → return directly.

- On write/update:
    - Update DB.
    - Invalidate or refresh the cache entry.

**✅ Best fit:**

- Read-heavy workloads.
- Occasional writes.
- Controlled staleness (via TTL or explicit invalidation).

Hence, **Cache-Aside** is the right choice.

#### ✅ Step 2: Handle Consistency Requirement (10-Second Visibility)

Since we can tolerate a 10-second lag, we can:

- **Invalidate** the cache on profile updates, forcing the next read to fetch from DB.
- Also set a **short TTL (≈10s)** to auto-refresh stale profiles if invalidation fails (e.g., missed cache clear).

#### ✅ Step 3: TTL Selection

| Parameter              | Decision              | Reason                                                                 |
|------------------------|-----------------------|------------------------------------------------------------------------|
| **TTL (Time-To-Live)** | **10 seconds**        | Guarantees stale data auto-refreshes within the 10s consistency window |
| **Cache invalidation** | On profile update     | Ensures updates become visible immediately for that user               |
| **Fallback**           | Expiry-driven refresh | Prevents serving stale cache beyond 10s even if invalidation fails     |

This balances load reduction and data freshness:

- 99.9% of requests hit cache (read-heavy).
- Cache entries auto-expire after 10s.
- DB load stays low because each user’s profile refreshes rarely.

#### ✅ Step 4: Data Flow Example

1. Read Request (Cache Hit):
    - Return profile from cache (fast, no DB hit).

2. Read Request (Cache Miss):
    - Fetch from DB → Populate cache (set TTL=10s).

3. Profile Update:
    - Update DB → Invalidate cache (remove entry immediately).
    - Next read repopulates cache with fresh data.

#### ⚠️ Trade-offs / Risks

| Risk                                                   | Mitigation                                            |
|--------------------------------------------------------|-------------------------------------------------------|
| Cache invalidation missed (stale data)                 | Short TTL (10s) ensures auto-refresh                  |
| Slight read latency spike after expiry                 | Acceptable for 1-in-1000 reads                        |
| Cache stampede (many users reloading same expired key) | Use jittered TTL or lock-based “refresh-once” pattern |

#### ✅ Final Summary

| Aspect                 | Design Decision                | Reason                                 |
|------------------------|--------------------------------|----------------------------------------|
| **Cache Strategy**     | **Cache-Aside (Lazy Loading)** | Ideal for read-heavy workloads         |
| **Consistency Target** | Update visible within 10s      | Business requirement                   |
| **TTL**                | **≈10 seconds**                | Ensures freshness with minimal DB load |
| **Update Handling**    | Invalidate cache on write      | Prevents stale data after updates      |
| **Trade-off**          | Possible brief stale reads     | Acceptable under 10s limit             |
