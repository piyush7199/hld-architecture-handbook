# 2.2.1 Caching Deep Dive: Speed, Cost, and Cache Invalidation

## Intuitive Explanation

Caching is the most effective way to improve the performance and reduce the cost of any large-scale system.

- **Cache:** A fast, temporary storage layer (usually $\text{RAM}$ or ultra-fast $\text{SSD}$) that stores copies of
  frequently accessed data.
- **The Goal:** Avoid expensive operations (database queries, external $\text{API}$ calls, complex calculations) by
  serving the result instantly from memory.
- **The Trade-off:** You exchange the performance gain for the risk of stale data (the cache might hold old data while
  the source data has been updated).

---

## In-Depth Analysis

### 1. Where to Place Caches

The closer the cache is to the user, the faster the response, but the harder the synchronization.

| Cache Level             | Location                                                                  | What it Caches                                                                                  | Use Case                                                                      |
|-------------------------|---------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------|
| Client/Browser          | User's browser memory or disk.                                            | Static assets, minor user data.                                                                 | Avoids network latency entirely ($\text{TTL}$ set via $\text{HTTP}$ headers). |
| CDN (Edge)              | Geographically distributed servers (Edge locations).                      | Images, $\text{CSS}$, $\text{JS}$ files, entire $\text{HTML}$ pages.                            | Reduces latency globally, dramatically lowers origin server load.             |
| API Gateway             | Proxy layer before the backend services.                                  | Authentication tokens, $\text{Rate}$ $\text{Limit}$ counters, expensive $\text{API}$ responses. | Protects backend services from redundant external calls.                      |
| Application (App-Level) | Dedicated cluster (e.g., Redis, Memcached) shared by application servers. | Database query results, session data, user profiles.                                            | Best performance for dynamic data reads.                                      |
| Database                | Built-in to the $\text{DB}$ engine (query result or row buffer).          | Recently accessed rows or query plans.                                                          | Least effective for distributed scaling, but useful for basic performance.    |

### 2. Cache Architectures (The Write Problem)

These patterns dictate how data is written, determining consistency and latency.

| Architecture               | Write Flow                                                                                                                                                                                     | Consistency / Stale Data                                                                                                | Best For                                                                                                            |
|----------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------|
| Cache-Aside (Lazy Loading) | $\text{Application}$ writes to $\text{DB}$. If data is needed later, $\text{Application}$ checks $\text{Cache}$. If miss, $\text{Application}$ reads $\text{DB}$ and populates $\text{Cache}$. | Low Write Latency. High risk of cache stampede or initial stale data after update.                                      | Read-heavy workloads where stale data is acceptable for a short time (e.g., blog posts, social feeds).              |
| Write-Through              | $\text{Application}$ writes to $\text{Cache}$ $\text{AND}$ $\text{DB}$ **synchronously**.                                                                                                      | High Consistency. Data is guaranteed to be in the cache immediately after the write.                                    | Small datasets where high consistency is mandatory (e.g., core user profile).                                       |
| Write-Back (Write-Behind)  | $\text{Application}$ writes to $\text{Cache}$. $\text{Cache}$ asynchronously writes to $\text{DB}$ in the background.                                                                          | Lowest Write Latency. High risk of data loss if the cache server fails before the data is persisted to the $\text{DB}$. | High-speed, high-volume logging or $\text{IoT}$ sensor data where speed is critical and durability can be eventual. |

### 3. Cache Invalidation and Eviction

- **Cache Invalidation:** The mechanism for removing stale data.
    - **TTL (Time-To-Live):** The most common method. Data expires after a set period. Simple and reliable.
    - **Write-Invalidate:** When the $\text{DB}$ is updated, a signal is sent to the cache to proactively delete the
      key. Used for higher consistency.

- **Eviction Policies:** What happens when the cache fills up (runs out of $\text{RAM}$)?
    - **LRU (Least Recently Used):** Removes the data that hasn't been accessed for the longest time. (Most common and
      effective).
    - **LFU (Least Frequently Used):** Removes the data that has the fewest accesses.
    - **FIFO (First-In, First-Out):** Removes the data that was added earliest.

### 4. Cache Issues at Scale

- **Cache Stampede:** When a highly requested key expires, thousands of concurrent requests all hit the database
  simultaneously, causing it to crash.
    - **Mitigation:** Use a Mutex Lock or $\text{Single}$ $\text{Flight}$ $\text{Request}$ pattern: only the first
      request is allowed to query the $\text{DB}$ and write back to the cache; all others wait for the result.
- **Cache Miss Ratio:** The ratio of requests that miss the cache and hit the database. A high ratio indicates poor
  performance or a misconfigured $\text{TTL}$.

---

## ‚úèÔ∏è Design Challenge

### Problem

You are serving the main user profile page of a social network. This data is read $1,000$ times more often than it is
written. If you update the $\text{DB}$, the profile must be visible to everyone within $10$ seconds. Which cache
architecture (Cache-Aside or Write-Through) would you choose, and what $\text{TTL}$ would you assign to balance
consistency and load reduction?

### Solution

#### üß© Scenario Summary

| Aspect               | Value                                                            |
|----------------------|------------------------------------------------------------------|
| **Workload Type**    | Read-heavy (1,000√ó reads vs writes)                              |
| **Consistency Need** | Moderate ‚Äì visible to everyone within 10 seconds                 |
| **Primary Goal**     | Reduce DB load while keeping cache reasonably fresh              |
| **Constraint**       | Cache should not serve stale data for more than 10s after update |

#### ‚úÖ Step 1: Choose the Cache Architecture

**Option 1 ‚Äì Write-Through Cache**

- Every write goes through the cache first; the cache updates the DB.
- Keeps cache always fresh (strong consistency), but:
    - Adds write latency.
    - Increases coupling between app and cache.
    - Inefficient for a system where writes are rare (1/1000 ratio).

**Option 2 ‚Äì Cache-Aside (Lazy Loading)**

- Application reads from cache:
    - If miss ‚Üí fetch from DB, then cache it.
    - If hit ‚Üí return directly.

- On write/update:
    - Update DB.
    - Invalidate or refresh the cache entry.

**‚úÖ Best fit:**

- Read-heavy workloads.
- Occasional writes.
- Controlled staleness (via TTL or explicit invalidation).

Hence, **Cache-Aside** is the right choice.

#### ‚úÖ Step 2: Handle Consistency Requirement (10-Second Visibility)

Since we can tolerate a 10-second lag, we can:

- **Invalidate** the cache on profile updates, forcing the next read to fetch from DB.
- Also set a **short TTL (‚âà10s)** to auto-refresh stale profiles if invalidation fails (e.g., missed cache clear).

#### ‚úÖ Step 3: TTL Selection

| Parameter              | Decision              | Reason                                                                 |
|------------------------|-----------------------|------------------------------------------------------------------------|
| **TTL (Time-To-Live)** | **10 seconds**        | Guarantees stale data auto-refreshes within the 10s consistency window |
| **Cache invalidation** | On profile update     | Ensures updates become visible immediately for that user               |
| **Fallback**           | Expiry-driven refresh | Prevents serving stale cache beyond 10s even if invalidation fails     |

This balances load reduction and data freshness:

- 99.9% of requests hit cache (read-heavy).
- Cache entries auto-expire after 10s.
- DB load stays low because each user‚Äôs profile refreshes rarely.

#### ‚úÖ Step 4: Data Flow Example

1. Read Request (Cache Hit):
    - Return profile from cache (fast, no DB hit).

2. Read Request (Cache Miss):
    - Fetch from DB ‚Üí Populate cache (set TTL=10s).

3. Profile Update:
    - Update DB ‚Üí Invalidate cache (remove entry immediately).
    - Next read repopulates cache with fresh data.

#### ‚ö†Ô∏è Trade-offs / Risks

| Risk                                                   | Mitigation                                            |
|--------------------------------------------------------|-------------------------------------------------------|
| Cache invalidation missed (stale data)                 | Short TTL (10s) ensures auto-refresh                  |
| Slight read latency spike after expiry                 | Acceptable for 1-in-1000 reads                        |
| Cache stampede (many users reloading same expired key) | Use jittered TTL or lock-based ‚Äúrefresh-once‚Äù pattern |

#### ‚úÖ Final Summary

| Aspect                 | Design Decision                | Reason                                 |
|------------------------|--------------------------------|----------------------------------------|
| **Cache Strategy**     | **Cache-Aside (Lazy Loading)** | Ideal for read-heavy workloads         |
| **Consistency Target** | Update visible within 10s      | Business requirement                   |
| **TTL**                | **‚âà10 seconds**                | Ensures freshness with minimal DB load |
| **Update Handling**    | Invalidate cache on write      | Prevents stale data after updates      |
| **Trade-off**          | Possible brief stale reads     | Acceptable under 10s limit             |
