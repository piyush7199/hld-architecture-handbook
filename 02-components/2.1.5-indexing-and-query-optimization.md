# 2.1.5 Indexing and Query Optimization: The Performance Engine

## Intuitive Explanation

Indexing is how a database finds data quickly. Imagine a massive, unorganized library (the table). Without an index, the
database must scan every single book ($\text{row}$) to find what you need. An index is the card catalog: a pre-sorted,
structured list that points directly to where the data lives.

- **Indexing:** Speeds up reads ($\text{select}$) massively, but slows down
  writes ($\text{insert}$, $\text{update}$, $\text{delete}$).
- **Query Optimization:** The process the database uses to find the most efficient way to execute your $\text{SQL}$
  query.

---

## In-Depth Analysis

### 1. Index Structures (B-Trees and LSM-Trees)

#### a. B-Trees (Balanced Tree)

The standard index structure for most $\text{RDBMS}$ (PostgreSQL, MySQL).

- **Mechanism:** Data is organized in a tree structure where all leaf nodes are at the same depth, ensuring that every
  search operation takes approximately the same amount of time ($\text{O}(\log N)$ lookup).
- **Benefit:** Excellent for read-heavy workloads. Supports sequential and range queries efficiently.
- **Trade-off:** Updates (writes) are slow because the tree must be constantly restructured to remain balanced.

#### b. LSM-Trees (Log-Structured Merge-Trees)

Used by modern $\text{NoSQL}$ databases (Cassandra, MongoDB, LevelDB) and optimized for high-volume writes.

- **Mechanism:** All incoming writes are first written sequentially to an in-memory structure ($\text{Memtable}$) and an
  on-disk $\text{Log}$. The $\text{Memtable}$ is periodically flushed, sorted, and written to immutable disk
  files ($\text{SSTables}$). Reads require checking multiple layers (memory $\rightarrow$ multiple $\text{SSTables}$).
- **Benefit: Extremely Fast Writes** (sequential appends are faster than random disk updates).
- Trade-off: Slower reads than B-Trees because a query might need to check multiple $\text{SSTables}$. Requires complex
  background merging processes (compaction).

### 2. Index Types

- **Primary Key Index:** A unique index that dictates the physical storage order of the data.
- **Secondary Index:** A separate data structure that points back to the primary key or the physical location of the
  row. This is what you create on columns like `last_name` or `email`.
- **Clustered Index:** The index where the data records themselves are stored in the order of the index (e.g., Primary
  Key in $\text{SQL}$ $\text{Server}$).
- **Covering Index:** An index that contains all the columns needed to satisfy a query, meaning the $\text{DB}$ never
  has to look up the main table (a massive performance win).

### 3. Query Optimization Basics

When you run a query, the **Query Planner/Optimizer** performs these steps:

- **Parsing:** Checks the query syntax.
- **Planning:** Generates several possible execution plans (e.g., "scan the whole table," "use Index $\text{A}$," "use
  Index $\text{B}$").
- **Cost Estimation:** Uses database statistics to estimate the $\text{CPU}$ and $\text{IO}$ cost of each plan.
- **Execution:** Selects and executes the lowest-cost plan.

---

## ‚úèÔ∏è Design Challenge

### Problem

You are building a high-scale logging service that records billions of ephemeral events per day. The data is rarely
updated, but frequently queried by time range. Should you choose a B-Tree or an $\text{LSM}$-Tree as your underlying
storage engine? Justify your choice by explaining the core trade-off between the two structures.

### Solution

#### üß© Scenario Summary

- **System:** Distributed logging/analytics pipeline (like ELK, Loki, or ClickHouse ingestion)
- Data characteristics:
    - **Write-heavy:** Billions of inserts per day
    - **Rare updates:** Data is immutable after ingestion
    - **Query pattern:** Time-range reads (append-only nature)

- **Goal:** Optimize for high write throughput and efficient time-range scans

#### ‚úÖ Step 1: Core Difference Between B-Tree and LSM-Tree

| Aspect             | **B-Tree**                              | **LSM-Tree (Log-Structured Merge Tree)**        |
|--------------------|-----------------------------------------|-------------------------------------------------|
| **Write Path**     | In-place updates on disk (random I/O)   | Sequential writes to disk (batched + compacted) |
| **Read Path**      | Single location lookup                  | May need to merge results across levels         |
| **Indexing Model** | Balanced hierarchical tree (disk pages) | Immutable SSTables + in-memory MemTable         |
| **Best for**       | Read-heavy, update-heavy workloads      | Write-heavy, append-only workloads              |

#### ‚úÖ Step 2: Choose ‚Äî LSM-Tree

**Why?** Logging workloads are:

- Append-only (no in-place updates)
- Massive write volume
- Time-ordered ingestion

**LSM-Trees** are optimized exactly for this:

- Data first written sequentially in memory (MemTable)
- Then flushed to disk as immutable SSTables (sorted by timestamp)
- Background compaction merges SSTables periodically

This makes writes O(1) amortized and disk-efficient, while still enabling range queries by timestamp.

#### ‚úÖ Step 3: Performance Trade-offs

| Operation                      | Winner         | Reason                                         |
|--------------------------------|----------------|------------------------------------------------|
| **Writes / Inserts**           | ‚úÖ **LSM-Tree** | Sequential disk writes, minimal random I/O     |
| **Updates / Deletes**          | ‚úÖ **LSM-Tree** | Uses tombstones, no in-place update            |
| **Reads (random lookups)**     | ‚ö†Ô∏è **B-Tree**  | Single-page lookup vs. multiple SSTable merges |
| **Range Queries (time-based)** | ‚úÖ **LSM-Tree** | Data naturally sorted by timestamp             |
| **Space Efficiency**           | ‚úÖ **LSM-Tree** | Compaction removes stale data                  |
| **Latency (point reads)**      | ‚ö†Ô∏è **B-Tree**  | LSM can have read amplification                |

#### ‚úÖ Step 4: Example Real-World Systems

| System                 | Storage Engine | Use Case                         |
|------------------------|----------------|----------------------------------|
| **Cassandra**          | LSM-Tree       | Time-series, logs, events        |
| **ScyllaDB**           | LSM-Tree       | High-ingest event data           |
| **LevelDB / RocksDB**  | LSM-Tree       | Write-heavy embedded stores      |
| **MySQL / PostgreSQL** | B-Tree         | OLTP workloads, frequent updates |

#### ‚ö†Ô∏è Trade-offs / Gotchas

- **Read amplification:** Querying across multiple SSTables can increase read latency.
    - Mitigated with Bloom filters and time-range partitioning.
- **Compaction overhead:** Background compaction uses I/O and CPU.
    - Must be tuned for throughput-heavy workloads.
- **Write amplification:** Compaction rewrites data multiple times (trade-off for space + sequential writes).

#### ‚úÖ Final Summary

| Aspect             | Design Decision                          | Reason                              |
|--------------------|------------------------------------------|-------------------------------------|
| **Storage Engine** | **LSM-Tree**                             | Optimized for sequential writes     |
| **Write Pattern**  | Append-only (MemTable ‚Üí SSTables)        | Perfect for immutable log ingestion |
| **Query Pattern**  | Time-range queries                       | Sorted data enables efficient scans |
| **Trade-off**      | Higher read latency, compaction overhead | Acceptable for log-style workloads  |
| **Real Systems**   | Cassandra, RocksDB, Bigtable             | Proven at scale                     |
