# 2.6.2 Configuration Management & Service Discovery Deep Dive: etcd, Consul, and Vault

## Intuitive Explanation

Imagine a large company with 1000 employees. Instead of each employee keeping their own phone directory, there's a *
*central directory (Configuration Management)** that everyone can access. When someone moves offices or changes their
number, they update the directory once, and everyone automatically gets the new information.

**In distributed systems:**

- **Configuration Management:** Centralized storage for configuration data (service locations, settings, secrets)
- **Service Discovery:** Services automatically find each other (no hardcoded IPs)
- **Leader Election:** Ensures only one service acts as leader (prevents conflicts)
- **Distributed Coordination:** Synchronizes actions across multiple services

**Key Tools:**

- **etcd:** Distributed key-value store (used by Kubernetes)
- **Consul:** Service discovery + configuration management
- **Vault:** Secrets management (passwords, API keys, certificates)

---

## In-Depth Analysis

### 1. What is Configuration Management?

**Configuration Management** is the process of storing, versioning, and distributing configuration data (settings,
service locations, feature flags) across distributed systems.

**Problems It Solves:**

- **Hardcoded Configuration:** IP addresses, ports hardcoded in code
- **Configuration Drift:** Different services have different configs
- **Secret Management:** Passwords, API keys stored in code/config files
- **Service Discovery:** Services need to find each other dynamically

### 2. etcd: Distributed Key-Value Store

**etcd** is a distributed, consistent key-value store used for configuration management and service discovery.

**Key Features:**

- **Raft Consensus:** Ensures consistency across nodes
- **Watch API:** Notify clients of changes (real-time updates)
- **TTL (Time-To-Live):** Automatic key expiration
- **Transactions:** Atomic operations (compare-and-swap)

**Architecture:**

```
etcd Cluster (3 nodes):
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Node 1  â”‚  â”‚ Node 2  â”‚  â”‚ Node 3  â”‚
  â”‚(Leader) â”‚  â”‚(Follower)â”‚ â”‚(Follower)â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚            â”‚            â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            Raft Consensus
```

**Use Cases:**

- **Kubernetes:** Stores cluster state, service endpoints
- **Service Discovery:** Stores service locations
- **Leader Election:** Distributed locks, leader selection
- **Configuration:** Application settings, feature flags

**Key Operations:**

```
PUT /v3/kv/put
  Key: /services/user-service
  Value: {"host": "10.0.1.5", "port": 8080}

GET /v3/kv/range
  Key: /services/user-service
  â†’ Returns: {"host": "10.0.1.5", "port": 8080}

WATCH /v3/watch
  Key: /services/user-service
  â†’ Notifies when value changes
```

### 3. Consul: Service Discovery & Configuration

**Consul** is a service mesh solution providing service discovery, configuration, and health checking.

**Key Features:**

- **Service Discovery:** Automatic service registration and discovery
- **Health Checking:** Monitors service health
- **Key-Value Store:** Configuration management
- **Multi-Datacenter:** Supports multiple data centers
- **Service Mesh:** Built-in service mesh (Connect)

**Architecture:**

```
Consul Cluster:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Consul Agentâ”‚ (on each service)
  â”‚ - Registers â”‚
  â”‚ - Discovers â”‚
  â”‚ - Health    â”‚
  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
         â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
  â”‚ Consul      â”‚
  â”‚ Server      â”‚ (3-5 nodes, Raft consensus)
  â”‚ - Stores    â”‚
  â”‚ - Coordinatesâ”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Service Discovery Flow:**

```
1. Service starts â†’ Registers with Consul Agent
2. Consul Agent â†’ Consul Server (registers service)
3. Other services query Consul: "Where is user-service?"
4. Consul returns: List of healthy instances
5. Service connects to instance
```

**Health Checking:**

```
Consul Health Checks:
  - HTTP: GET /health (every 10 seconds)
  - TCP: Connect to port (every 10 seconds)
  - Script: Run custom script (every 30 seconds)
  
If check fails:
  - Service marked as unhealthy
  - Removed from service discovery results
```

### 4. Vault: Secrets Management

**Vault** is a tool for securely storing and accessing secrets (passwords, API keys, certificates).

**Key Features:**

- **Secret Storage:** Encrypted at rest and in transit
- **Dynamic Secrets:** Generate secrets on-demand (database credentials)
- **Secret Rotation:** Automatic rotation of secrets
- **Access Control:** Fine-grained policies (who can access what)
- **Audit Logging:** Track all secret access

**Architecture:**

```
Vault Cluster:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Vault Serverâ”‚ (3-5 nodes, active-standby)
  â”‚ - Stores    â”‚
  â”‚ - Encrypts  â”‚
  â”‚ - Manages   â”‚
  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
         â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
  â”‚ Storage     â”‚ (Consul, etcd, S3, etc.)
  â”‚ Backend     â”‚ (encrypted secrets)
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Secret Types:**

```
Static Secrets:
  - Passwords
  - API keys
  - Certificates
  â†’ Stored in Vault, retrieved when needed

Dynamic Secrets:
  - Database credentials (generated on-demand)
  - AWS IAM roles (temporary credentials)
  â†’ Generated by Vault, auto-expire
```

**Access Flow:**

```
1. Application authenticates to Vault (AppRole, JWT)
2. Vault validates credentials
3. Application requests secret: "database-password"
4. Vault checks policy (does app have permission?)
5. Vault returns secret (decrypted)
6. Application uses secret
7. Vault logs access (audit trail)
```

### 5. Leader Election

**Problem:** Multiple services need to coordinate (only one should act as leader)

**Solution:** Distributed leader election using etcd/Consul

**etcd Leader Election:**

```
1. Service creates key: /leader/election
2. Service tries to acquire lock (TTL: 30 seconds)
3. If successful â†’ Service becomes leader
4. Leader renews TTL every 10 seconds (heartbeat)
5. If leader crashes â†’ TTL expires â†’ Other service becomes leader
```

**Consul Leader Election:**

```
1. Service creates session (TTL: 30 seconds)
2. Service acquires lock: /leader/election
3. If successful â†’ Service becomes leader
4. Leader renews session (heartbeat)
5. If leader crashes â†’ Session expires â†’ Lock released
```

**Use Cases:**

- **Master Election:** Only one service processes a queue
- **Coordination:** Only one service runs scheduled tasks
- **Resource Locking:** Only one service accesses shared resource

### 6. Watch API (Real-Time Updates)

**Problem:** Services need to know when configuration changes

**Solution:** Watch API (notify on changes)

**etcd Watch:**

```
Client: WATCH /services/user-service
etcd: Monitors key for changes

When key changes:
  etcd â†’ Client: Notification (new value)
  Client: Updates local cache
  Client: Reconnects to new service instance
```

**Consul Watch:**

```
Client: WATCH /services/user-service
Consul: Monitors service for changes

When service changes:
  Consul â†’ Client: Notification (service added/removed)
  Client: Updates service list
  Client: Reconnects to new instances
```

### 7. Configuration Management Patterns

#### A. Centralized Configuration

**Pattern:** Single source of truth for all configuration

```
Configuration Store (etcd/Consul):
  /app/database/host: "db.example.com"
  /app/database/port: "5432"
  /app/feature-flags/new-ui: "true"

All services read from same store
```

**Pros:**

- Single source of truth
- Easy to update (change once, all services get update)
- Version control (can track changes)

**Cons:**

- Single point of failure
- Network dependency (services must connect to store)

#### B. Configuration as Code

**Pattern:** Configuration stored in version control (Git)

```
Git Repository:
  config/
    production.yaml
    staging.yaml
    development.yaml

CI/CD Pipeline:
  1. Code change â†’ Git push
  2. CI/CD detects config change
  3. Deploys config to etcd/Consul
  4. Services reload config (watch API)
```

**Pros:**

- Version control
- Code review for config changes
- Rollback capability

**Cons:**

- Secrets shouldn't be in Git (use Vault)
- Deployment complexity

#### C. Environment-Specific Configuration

**Pattern:** Different configs for different environments

```
Development:
  /app/database/host: "localhost"
  /app/log-level: "debug"

Production:
  /app/database/host: "db-prod.example.com"
  /app/log-level: "info"
```

### 8. Service Discovery Patterns

#### A. Client-Side Discovery

**Pattern:** Client queries service registry, selects instance

```
1. Client queries Consul: "Where is user-service?"
2. Consul returns: [instance1, instance2, instance3]
3. Client selects instance (round-robin, random)
4. Client connects to instance
```

**Pros:**

- No load balancer needed
- Client has full control

**Cons:**

- Client complexity (must implement discovery logic)
- Client must handle failures

#### B. Server-Side Discovery

**Pattern:** Load balancer queries service registry

```
1. Client â†’ Load Balancer: Request
2. Load Balancer queries Consul: "Where is user-service?"
3. Consul returns: [instance1, instance2, instance3]
4. Load Balancer selects instance
5. Load Balancer â†’ Instance: Request
```

**Pros:**

- Client simplicity (just talks to load balancer)
- Centralized load balancing

**Cons:**

- Load balancer as single point of failure
- Additional infrastructure

---

## When to Use Configuration Management

### âœ… Use Configuration Management When:

1. **Microservices:** Multiple services need to discover each other
2. **Dynamic Infrastructure:** Services start/stop frequently (containers)
3. **Multi-Environment:** Different configs for dev/staging/prod
4. **Secrets Management:** Need secure storage for passwords, API keys
5. **Leader Election:** Multiple services need coordination
6. **Configuration Updates:** Need to update config without redeployment

### âŒ Don't Use Configuration Management When:

1. **Monolithic Application:** Single service (overhead not worth it)
2. **Static Configuration:** Configuration rarely changes
3. **Simple Use Case:** Few services, simple setup
4. **Low Scale:** Small deployment (overhead not justified)

---

## Real-World Examples

### Kubernetes (etcd)

**Use Case:** Cluster state, service discovery

**Architecture:**

- etcd stores all Kubernetes cluster state
- Service endpoints stored in etcd
- Kube-proxy watches etcd for changes
- Updates iptables/IPVS rules

**Scale:**

- Thousands of pods
- Millions of etcd operations per day
- Sub-second updates

### Netflix (Eureka)

**Use Case:** Service discovery for microservices

**Architecture:**

- Eureka service registry
- Services register on startup
- Services discover each other via Eureka
- Health checks remove unhealthy services

**Scale:**

- 100+ microservices
- Thousands of service instances
- High availability (multi-region)

### HashiCorp Vault (Secrets)

**Use Case:** Secrets management

**Architecture:**

- Vault stores all secrets
- Applications authenticate to Vault
- Vault returns secrets (decrypted)
- Secrets rotated automatically

**Scale:**

- Millions of secret accesses per day
- Thousands of applications
- Multi-region deployment

---

## Configuration Management vs. Other Solutions

| Solution      | Best For                             | Features                             | Complexity |
|---------------|--------------------------------------|--------------------------------------|------------|
| **etcd**      | Kubernetes, distributed coordination | Key-value, watch, transactions       | Medium     |
| **Consul**    | Service discovery, multi-datacenter  | Service discovery, health checks, KV | Medium     |
| **Vault**     | Secrets management                   | Secrets, rotation, policies          | High       |
| **ZooKeeper** | Coordination, leader election        | ZNodes, watches, consensus           | High       |
| **Redis**     | Simple key-value, caching            | Fast, simple                         | Low        |

---

## Common Anti-Patterns

### âŒ **1. Storing Secrets in Configuration Store**

**Problem:** Storing passwords in etcd/Consul (not encrypted)

**Solution:** Use Vault for secrets, etcd/Consul for non-sensitive config

```
âŒ Bad:
etcd: /app/database/password: "secret123"
â†’ Stored in plaintext, accessible to anyone with etcd access

âœ… Good:
etcd: /app/database/password-path: "secret/database"
Vault: secret/database â†’ password: "secret123" (encrypted)
â†’ Application reads path from etcd, retrieves secret from Vault
```

### âŒ **2. No Health Checks**

**Problem:** Unhealthy services still in service discovery

**Solution:** Implement health checks

```
âŒ Bad:
Service crashes â†’ Still in service registry â†’ Clients connect to dead service

âœ… Good:
Service crashes â†’ Health check fails â†’ Removed from registry â†’ Clients get healthy instances
```

### âŒ **3. Hardcoded Service Locations**

**Problem:** IP addresses hardcoded in code

**Solution:** Use service discovery

```
âŒ Bad:
Code: user_service_url = "http://10.0.1.5:8080"
â†’ Breaks when service moves or scales

âœ… Good:
Code: user_service = consul.get_service("user-service")
â†’ Automatically gets current service location
```

---

## Trade-offs Summary

| Aspect                 | What You Gain                        | What You Sacrifice                              |
|------------------------|--------------------------------------|-------------------------------------------------|
| **Centralized Config** | Single source of truth, easy updates | Network dependency, single point of failure     |
| **Service Discovery**  | Dynamic service location             | Additional infrastructure, complexity           |
| **Secrets Management** | Secure storage, rotation             | Operational overhead, authentication complexity |
| **Watch API**          | Real-time updates                    | Network overhead, connection management         |

---

## References

- **etcd Documentation:** [https://etcd.io/docs/](https://etcd.io/docs/)
- **Consul Documentation:** [https://www.consul.io/docs](https://www.consul.io/docs)
- **Vault Documentation:** [https://www.vaultproject.io/docs](https://www.vaultproject.io/docs)
- **Related Chapters:**
    - [1.2.5 Service Discovery](../../01-principles/1.2.5-service-discovery.md) - Service discovery overview
    - [2.6.1 Kubernetes and Docker Deep Dive](./2.6.1-kubernetes-docker-deep-dive.md) - etcd in Kubernetes

---

## âœï¸ Design Challenge

### Problem

You are designing a microservices platform with 50 services that must:

1. **Discover services dynamically** (services start/stop frequently)
2. **Store configuration** (database URLs, feature flags, API endpoints)
3. **Manage secrets** (database passwords, API keys, certificates)
4. **Elect leaders** (only one service processes scheduled tasks)
5. **Handle 10,000 service instances** (200 instances per service average)
6. **Update configuration** without service restarts (real-time updates)

**Constraints:**

- Services run in Kubernetes (containers)
- Services start/stop every few minutes (auto-scaling)
- Configuration changes 100 times per day
- Secrets rotated monthly
- Need high availability (99.99% uptime)

Design a configuration management strategy that:

- Handles service discovery at scale
- Manages configuration efficiently
- Secures secrets
- Supports leader election
- Provides real-time updates
- Ensures high availability

### Solution

#### ğŸ§© Scenario

- **Services:** 50 microservices
- **Instances:** 10,000 total (200 per service average)
- **Configuration Updates:** 100 per day
- **Secrets:** 500 secrets (database passwords, API keys)
- **Availability:** 99.99% (4 nines)

**Calculations:**

- **Service Discovery Queries:** 10,000 instances Ã— 10 queries/min = 100K queries/min
- **Configuration Reads:** 10,000 instances Ã— 1 read/min = 10K reads/min
- **Watch Connections:** 10,000 instances (one watch per instance)

#### âœ… Step 1: Architecture Choice

**Choice: Consul for Service Discovery + Configuration, Vault for Secrets**

**Why:**

- **Consul:** Built-in service discovery, health checks, KV store, watch API
- **Vault:** Specialized secrets management, rotation, policies
- **Integration:** Consul and Vault work well together
- **Kubernetes:** Native Kubernetes integration

**Architecture:**

```
Consul Cluster:
  - 5 Consul Servers (Raft consensus, high availability)
  - Consul Agents (one per Kubernetes node)

Vault Cluster:
  - 3 Vault Servers (active-standby)
  - Storage Backend: Consul (shared infrastructure)
```

#### âœ… Step 2: Service Discovery

**Consul Service Registration:**

```
Service Registration (Automatic):
  - Kubernetes Pod starts
  - Consul Agent (sidecar) registers service
  - Service metadata: name, IP, port, tags
  - Health check: HTTP GET /health (every 10 seconds)
```

**Service Discovery:**

```
Client Service:
  1. Queries Consul: "Where is user-service?"
  2. Consul returns: [instance1:8080, instance2:8080, ...]
  3. Client selects instance (round-robin)
  4. Client connects to instance
```

**Health Checks:**

```
Consul Health Checks:
  - HTTP: GET /health (every 10 seconds, timeout: 5 seconds)
  - Failure Threshold: 3 consecutive failures
  - Success Threshold: 1 success to recover
  
If unhealthy:
  - Service removed from discovery results
  - Clients get only healthy instances
```

**Caching:**

```
Client-Side Cache:
  - Cache service list (TTL: 30 seconds)
  - Watch API for updates (real-time)
  - Fallback to cache if Consul unavailable
  â†’ Reduces Consul load (90% cache hit rate)
```

#### âœ… Step 3: Configuration Management

**Configuration Storage (Consul KV):**

```
Configuration Structure:
  /app/services/user-service/database/host: "db.example.com"
  /app/services/user-service/database/port: "5432"
  /app/feature-flags/new-ui: "true"
  /app/feature-flags/payment-v2: "false"
```

**Configuration Access:**

```
Service Startup:
  1. Service queries Consul: GET /v1/kv/app/services/user-service
  2. Consul returns: Configuration JSON
  3. Service loads configuration
  4. Service watches for changes: WATCH /v1/kv/app/services/user-service
  5. On change: Service reloads configuration (no restart)
```

**Configuration Versioning:**

```
Consul KV Versioning:
  - Each update creates new version
  - Can retrieve previous versions
  - Supports rollback
```

**Configuration Distribution:**

```
Update Flow:
  1. Admin updates config in Consul UI/API
  2. Consul notifies all watchers (10,000 connections)
  3. Services receive notification (<1 second)
  4. Services reload configuration
  5. Result: All services updated in <5 seconds
```

#### âœ… Step 4: Secrets Management

**Vault Architecture:**

```
Vault Cluster:
  - 3 Vault Servers (active-standby)
  - Storage Backend: Consul (shared with service discovery)
  - Seal/Unseal: Auto-unseal with AWS KMS
```

**Secret Storage:**

```
Secret Structure:
  secret/databases/user-db:
    username: "app_user"
    password: "secret123"
    
  secret/apis/stripe:
    api_key: "sk_live_..."
    
  secret/certificates/tls:
    cert: "..."
    key: "..."
```

**Service Authentication:**

```
Kubernetes Service Account Authentication:
  1. Service uses Kubernetes service account
  2. Service authenticates to Vault (JWT token)
  3. Vault validates token with Kubernetes API
  4. Vault returns service-specific secrets
  5. Service uses secrets
```

**Secret Rotation:**

```
Dynamic Secrets (Database):
  - Vault generates database credentials on-demand
  - Credentials expire after 1 hour
  - Auto-rotation (no manual intervention)
  
Static Secrets (API Keys):
  - Manual rotation (monthly)
  - Vault notifies services of rotation
  - Services reload secrets (no restart)
```

**Secret Caching:**

```
Service Secret Cache:
  - Cache secrets in memory (TTL: 5 minutes)
  - Watch Vault for changes (real-time)
  - Fallback to cache if Vault unavailable
  â†’ Reduces Vault load (95% cache hit rate)
```

#### âœ… Step 5: Leader Election

**Consul Leader Election:**

```
Leader Election Pattern:
  1. Service creates session (TTL: 30 seconds)
  2. Service acquires lock: /leader/scheduled-tasks
  3. If successful â†’ Service becomes leader
  4. Leader renews session every 10 seconds (heartbeat)
  5. If leader crashes â†’ Session expires â†’ Lock released
  6. Another service acquires lock â†’ Becomes new leader
```

**Implementation:**

```
Service Code:
  session = consul.create_session(ttl=30)
  lock = consul.acquire_lock("/leader/scheduled-tasks", session)
  
  if lock.acquired:
    while True:
      # Leader work (process scheduled tasks)
      lock.renew_session()  # Heartbeat
      time.sleep(10)
```

**High Availability:**

```
Multiple Services:
  - 5 services compete for leadership
  - Only one becomes leader
  - If leader fails â†’ Another service takes over (<30 seconds)
  â†’ No single point of failure
```

#### âœ… Step 6: Real-Time Updates

**Watch API:**

```
Service Discovery Watch:
  - Service watches: WATCH /v1/health/service/user-service
  - Consul notifies on: service added, removed, health changed
  - Service updates local cache
  - Service reconnects to new instances
  
Configuration Watch:
  - Service watches: WATCH /v1/kv/app/services/user-service
  - Consul notifies on: configuration changed
  - Service reloads configuration (no restart)
  
Secrets Watch:
  - Service watches: WATCH secret/databases/user-db
  - Vault notifies on: secret rotated
  - Service reloads secret (no restart)
```

**Update Latency:**

```
Configuration Update:
  - Consul update: <100ms
  - Watch notification: <1 second
  - Service reload: <1 second
  - Total: <2 seconds (all 10,000 instances updated)
```

#### âœ… Step 7: High Availability

**Consul High Availability:**

```
Consul Cluster:
  - 5 Consul Servers (quorum: 3)
  - Raft consensus (can lose 2 servers)
  - Multi-region deployment (3 regions)
  - Automatic failover (<30 seconds)
```

**Vault High Availability:**

```
Vault Cluster:
  - 3 Vault Servers (active-standby)
  - Active server handles requests
  - Standby servers ready to take over
  - Automatic failover (<10 seconds)
```

**Client Resilience:**

```
Client Resilience:
  - Retry logic (exponential backoff)
  - Circuit breaker (fail fast if Consul/Vault down)
  - Local cache (fallback if service unavailable)
  - Multiple Consul/Vault endpoints (round-robin)
```

#### âœ… Complete Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Kubernetes Cluster (10,000 Pods)                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Pod 1        â”‚  â”‚ Pod 2        â”‚  â”‚ Pod N        â”‚  â”‚
â”‚  â”‚ - Service    â”‚  â”‚ - Service    â”‚  â”‚ - Service    â”‚  â”‚
â”‚  â”‚ - Consul     â”‚  â”‚ - Consul     â”‚  â”‚ - Consul     â”‚  â”‚
â”‚  â”‚   Agent      â”‚  â”‚   Agent      â”‚  â”‚   Agent      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                  â”‚                  â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚      Consul Server Cluster (5 nodes)   â”‚
        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
        â”‚  â”‚ - Service Discovery              â”‚  â”‚
        â”‚  â”‚ - Configuration (KV Store)       â”‚  â”‚
        â”‚  â”‚ - Health Checks                  â”‚  â”‚
        â”‚  â”‚ - Watch API                      â”‚  â”‚
        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚      Vault Cluster (3 nodes)            â”‚
        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
        â”‚  â”‚ - Secrets Storage                â”‚  â”‚
        â”‚  â”‚ - Secret Rotation                â”‚  â”‚
        â”‚  â”‚ - Access Policies                â”‚  â”‚
        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Request Flow:**

```
1. Service starts â†’ Consul Agent registers service
2. Service queries Consul: "Where is user-service?"
3. Consul returns: [instance1, instance2, ...]
4. Service connects to instance
5. Service queries Consul: GET /v1/kv/app/config
6. Consul returns: Configuration JSON
7. Service authenticates to Vault (Kubernetes JWT)
8. Vault returns: Secrets (decrypted)
9. Service watches for changes (real-time updates)
```

#### âš–ï¸ Trade-offs Summary

| Decision                   | What We Gain                       | What We Sacrifice                  |
|----------------------------|------------------------------------|------------------------------------|
| **Consul + Vault**         | Integrated solution, rich features | Higher complexity than single tool |
| **Watch API**              | Real-time updates                  | Network overhead (10K connections) |
| **Client-Side Cache**      | Reduced load, faster access        | Slight staleness risk (30s TTL)    |
| **Multi-Region**           | High availability                  | Configuration sync complexity      |
| **Kubernetes Integration** | Native support, easy deployment    | Kubernetes dependency              |

#### âœ… Final Summary

**Configuration Management Strategy:**

- **Service Discovery:** Consul (5 servers, automatic registration)
- **Configuration:** Consul KV Store (versioned, watchable)
- **Secrets:** Vault (3 servers, Kubernetes auth, rotation)
- **Leader Election:** Consul sessions and locks
- **Updates:** Watch API (real-time, <2 seconds)

**Performance:**

- **Service Discovery:** <10ms (cached), <100ms (uncached)
- **Configuration Read:** <10ms (cached), <50ms (uncached)
- **Secret Access:** <50ms (cached), <200ms (uncached)
- **Update Latency:** <2 seconds (all 10K instances)

**Availability:**

- **Consul:** 99.99% (5 servers, multi-region)
- **Vault:** 99.99% (3 servers, active-standby)
- **Overall:** 99.99% (redundancy, failover)

**Result:**

- âœ… Handles 10,000 service instances
- âœ… Real-time service discovery
- âœ… Centralized configuration management
- âœ… Secure secrets management
- âœ… Leader election support
- âœ… Real-time updates (<2 seconds)
- âœ… High availability (99.99%)

