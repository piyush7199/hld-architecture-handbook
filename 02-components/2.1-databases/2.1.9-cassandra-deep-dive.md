# 2.1.8 Cassandra Deep Dive: The Masterless Distributed Database

## Intuitive Explanation

Apache Cassandra is a **distributed NoSQL database** designed for massive scale and zero downtime. Unlike traditional
databases with a single leader, Cassandra has **no master node** â€” every node is equal. This makes it incredibly
resilient: if one server dies, the system keeps running without interruption. It's like having a team where everyone
knows the job, so losing one person doesn't break the system.

- **Masterless Architecture:** No single point of failure. All nodes can accept reads and writes.
- **Linear Scalability:** Add more servers = proportionally more throughput.
- **Always-On:** Designed for 99.99% uptime with multi-datacenter replication.
- **Use Cases:** Time-series data (IoT sensors), messaging apps (WhatsApp), high-write workloads (Netflix viewing
  history).

---

## In-Depth Analysis

### 1. Cassandra Architecture: The Ring

Cassandra organizes nodes in a **ring topology** using **consistent hashing**:

```
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  Token Range: 0 - 2^63  â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                â”‚                â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
    â”‚ Node A  â”‚     â”‚ Node B  â”‚     â”‚ Node C  â”‚
    â”‚ Token:  â”‚     â”‚ Token:  â”‚     â”‚ Token:  â”‚
    â”‚   42    â”‚     â”‚   130   â”‚     â”‚   210   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                â”‚                â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              Gossip Protocol (peer-to-peer)
```

**Key Concepts:**

| Concept                     | Explanation                                                               |
|-----------------------------|---------------------------------------------------------------------------|
| **Partition Key**           | Determines which node stores the data (via consistent hashing).           |
| **Token**                   | Each node owns a range of tokens (hash values).                           |
| **Virtual Nodes (vnodes)**  | Each physical node handles multiple token ranges (better load balancing). |
| **Gossip Protocol**         | Nodes exchange state information every second (peer-to-peer heartbeat).   |
| **Replication Factor (RF)** | Number of copies of each data row (e.g., RF=3 means 3 copies).            |
| **Consistency Level**       | Tunable trade-off between consistency and availability.                   |

**Example: Data Distribution**

```
Row Key: "user:12345"
Hash(user:12345) = Token 178

Token Ranges:
- Node A: 0 - 100
- Node B: 101 - 200  â† Token 178 falls here (Primary Replica)
- Node C: 201 - 300

With RF=3:
- Primary: Node B
- Replicas: Node C, Node A (next two nodes in the ring)
```

---

### 2. Data Model: Wide-Column Store

Cassandra is a **wide-column store**, meaning data is organized differently than SQL or document databases:

#### **2.1 Terminology Mapping**

| Cassandra          | SQL Equivalent       | Description                               |
|--------------------|----------------------|-------------------------------------------|
| **Keyspace**       | Database             | Top-level container                       |
| **Table**          | Table                | Collection of rows                        |
| **Partition Key**  | Primary Key (part 1) | Determines data distribution across nodes |
| **Clustering Key** | Primary Key (part 2) | Determines sort order within a partition  |
| **Column**         | Column               | Individual data field                     |

#### **2.2 Primary Key Structure**

```sql
PRIMARY KEY ((partition_key), clustering_key1, clustering_key2)
```

**Example: User Timeline Table**

```sql
CREATE TABLE user_timeline (
    user_id UUID,
    tweet_id TIMEUUID,
    content TEXT,
    created_at TIMESTAMP,
    PRIMARY KEY (user_id, tweet_id)
) WITH CLUSTERING ORDER BY (tweet_id DESC);
```

**Breakdown:**

- **Partition Key:** `user_id` â€” All tweets for a user are stored on the same node.
- **Clustering Key:** `tweet_id` â€” Tweets are sorted by time (TIMEUUID is time-ordered).
- **Query Pattern:** `SELECT * FROM user_timeline WHERE user_id = ? ORDER BY tweet_id DESC LIMIT 10;` (single-partition
  query, very fast).

#### **2.3 Partition Design (Critical)**

**The Golden Rule:** **Design your table around your queries.**

| Good Design                                        | Bad Design                                                 |
|----------------------------------------------------|------------------------------------------------------------|
| One partition = One user's timeline (bounded size) | One partition = All tweets (unbounded, causes hotspots)    |
| Single-partition queries (`WHERE user_id = ?`)     | Multi-partition queries (`WHERE content CONTAINS 'hello'`) |
| Partition size < 100MB                             | Partition size > 1GB (wide partition problem)              |

**Wide Partition Problem:**

If one partition grows too large (e.g., a celebrity with millions of followers), that single node becomes a **hotspot
** (all queries for that celebrity hit one node).

**Solution: Bucketing**

```sql
-- Bad: One partition per user (unbounded)
PRIMARY KEY (user_id, tweet_id)

-- Good: Multiple partitions per user (bounded by month)
PRIMARY KEY ((user_id, bucket_month), tweet_id)

-- Query
SELECT * FROM user_timeline
WHERE user_id = ? AND bucket_month = '2024-01'
ORDER BY tweet_id DESC;
```

---

### 3. Consistency Levels: Tunable CAP Trade-offs

Cassandra lets you **tune consistency per query** (unlike most databases):

#### **3.1 Consistency Levels**

| Level            | Behavior                     | Latency | Availability | Use Case                                     |
|------------------|------------------------------|---------|--------------|----------------------------------------------|
| **ONE**          | 1 replica responds           | Fastest | Highest      | Analytics, caching (eventual consistency OK) |
| **QUORUM**       | Majority (RF/2 + 1) responds | Medium  | High         | Default for most apps (balance)              |
| **ALL**          | All replicas respond         | Slowest | Lowest       | Critical reads (strong consistency)          |
| **LOCAL_QUORUM** | Quorum within local DC       | Medium  | High         | Multi-DC setup (avoids cross-DC latency)     |
| **EACH_QUORUM**  | Quorum in each DC            | High    | Medium       | Strong consistency across DCs                |

**Example: RF=3**

```
Consistency Level = QUORUM
- Writes: Wait for 2 out of 3 replicas to ACK.
- Reads: Query 2 out of 3 replicas and return most recent value.
```

**Strong Consistency Formula:**

```
R + W > RF  (R = read replicas, W = write replicas)

Example:
- RF = 3
- Write at QUORUM (W=2)
- Read at QUORUM (R=2)
- 2 + 2 > 3 âœ… (Guarantees you always read latest write)
```

#### **3.2 Eventual Consistency (ONE/TWO)**

- **How:** Writes go to one node, replicas sync in the background.
- **Risk:** Reads might return stale data temporarily.
- **Mitigation:** Use **read repair** and **anti-entropy** (background reconciliation).

**Example:**

```
Time    | Node A | Node B | Node C
--------|--------|--------|--------
T0      | v1     | v1     | v1
T1      | v2     | v1     | v1    (Write at ONE to Node A)
T2      | v2     | v2     | v1    (Background replication)
T3      | v2     | v2     | v2    (Eventually consistent)
```

---

### 4. Write Path: Fast by Design

Cassandra is **optimized for writes** (much faster than reads):

#### **4.1 Write Path (In-Memory)**

```
Write Request
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. Write to        â”‚
â”‚  CommitLog (disk)   â”‚  â† Append-only (sequential writes, very fast)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. Write to        â”‚
â”‚  Memtable (memory)  â”‚  â† In-memory sorted structure
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. Return ACK      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why So Fast?**

- âœ… **Sequential disk writes** (CommitLog is append-only).
- âœ… **No read-before-write** (no need to check if row exists).
- âœ… **No locks** (writes never block reads or other writes).

#### **4.2 Flush to SSTable (Background)**

When Memtable fills up (e.g., 128MB):

```
Memtable (memory)
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Flush to SSTable   â”‚  â† Immutable file on disk (sorted)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SSTables (disk)    â”‚
â”‚  - SSTable 1 (old)  â”‚
â”‚  - SSTable 2 (old)  â”‚
â”‚  - SSTable 3 (new)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Compaction         â”‚  â† Merge SSTables, remove tombstones
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**SSTable (Sorted String Table):**

- **Immutable:** Once written, never modified.
- **Sorted:** Rows sorted by primary key.
- **Indexed:** Includes partition index, bloom filter.

---

### 5. Read Path: More Complex

Reads are **slower than writes** because data might be scattered across multiple SSTables:

#### **5.1 Read Path (Multi-Level)**

```
Read Request
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. Check Memtable  â”‚  â† In-memory (fastest)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. Check Row Cache â”‚  â† Optional (caches frequently read rows)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. Bloom Filter    â”‚  â† Quickly check if SSTable might have the row
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼ (if bloom says "maybe")
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. Partition Index â”‚  â† Find offset in SSTable
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. Read SSTable    â”‚  â† Disk read (slowest)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  6. Merge Results   â”‚  â† Merge from multiple SSTables (timestamp-based)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Return Result      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why Reads Are Slower:**

- âŒ **Multiple SSTables:** Must check all SSTables (can be 10+).
- âŒ **Disk I/O:** SSTables are on disk (unless cached).
- âŒ **Merge overhead:** Must merge rows from multiple SSTables by timestamp.

**Optimization: Compaction**

- **Purpose:** Merge SSTables to reduce read amplification.
- **Strategy:** Size-Tiered (STCS), Leveled (LCS), Time-Window (TWCS).
- **Trade-off:** Compaction uses CPU/disk I/O (background task).

---

### 6. Replication and Multi-Datacenter Setup

#### **6.1 Replication Strategy**

**Simple Strategy (Single DC):**

```sql
CREATE KEYSPACE my_keyspace
WITH REPLICATION = {
    'class': 'SimpleStrategy',
    'replication_factor': 3
};
```

**Network Topology Strategy (Multi-DC):**

```sql
CREATE KEYSPACE my_keyspace
WITH REPLICATION = {
    'class': 'NetworkTopologyStrategy',
    'DC1': 3,  -- 3 replicas in DC1
    'DC2': 2   -- 2 replicas in DC2
};
```

**Topology:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Cassandra Ring                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  DC1 (US-East)              DC2 (EU-West)            â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â”€â”              â”‚
â”‚    â”‚Node 1â”‚                   â”‚Node 4â”‚              â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â”€â”              â”‚
â”‚    â”‚Node 2â”‚                   â”‚Node 5â”‚              â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”                                          â”‚
â”‚    â”‚Node 3â”‚                                          â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”˜                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                             â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€Async Replicationâ”€â”€â”€â”€â”˜
```

**Writes with LOCAL_QUORUM:**

1. Write arrives at coordinator in DC1.
2. Coordinator writes to 2 out of 3 local replicas (QUORUM in DC1).
3. Coordinator returns ACK to client (fast, no cross-DC latency).
4. Background async replication to DC2 (eventual consistency across DCs).

**Benefits:**

- âœ… **Low latency:** LOCAL_QUORUM avoids cross-region latency.
- âœ… **Disaster recovery:** If DC1 fails, DC2 can serve traffic.
- âœ… **Geo-distribution:** Serve users from nearest DC.

---

### 7. Failure Handling: Hinted Handoff & Repair

#### **7.1 Hinted Handoff**

**Problem:** Node C is temporarily down. Write targeted at C needs to succeed.

**Solution:**

```
Write Request (RF=3: A, B, C)
    â”‚
    â–¼
Coordinator (Node A)
    â”œâ”€> Node A (OK)
    â”œâ”€> Node B (OK)
    â””â”€> Node C (DOWN)
          â”‚
          â–¼
    Node A stores "hint" for C
    (writes to local disk: "C owes this write")
    â”‚
    â–¼
Node C comes back online
    â”‚
    â–¼
Node A replays hints to C
```

**Hinted Handoff Window:** Default 3 hours. If node is down longer, hints are dropped (must run repair).

#### **7.2 Read Repair**

**Problem:** Replicas are inconsistent (e.g., Node B missed a write).

**Solution:**

```
Read at QUORUM (RF=3)
    â”‚
    â–¼
Coordinator queries 2 replicas:
- Node A: version 5
- Node B: version 4  â† Stale

Coordinator detects mismatch:
    â”‚
    â–¼
Return version 5 to client (latest)
    â”‚
    â–¼
Background: Write version 5 to Node B (repair)
```

#### **7.3 Anti-Entropy Repair (Manual)**

**Purpose:** Full reconciliation across all replicas (run periodically).

**Command:**

```bash
nodetool repair -pr  # Repair primary range only
```

**When to Run:**

- After node failure/recovery.
- Periodically (weekly/monthly) to ensure consistency.
- After exceeding hinted handoff window.

---

### 8. Data Modeling Best Practices

#### **8.1 Query-Driven Design**

**Rule:** Model data around queries, not entities.

**Example: Social Media App**

**Queries Needed:**

1. Get user's timeline (latest 20 posts).
2. Get post details by post_id.
3. Get all comments for a post.

**Bad Design (RDBMS-style):**

```sql
-- Single posts table (requires scatter-gather queries)
CREATE TABLE posts (
    post_id UUID PRIMARY KEY,
    user_id UUID,
    content TEXT,
    created_at TIMESTAMP
);

-- Query requires full table scan (slow)
SELECT * FROM posts WHERE user_id = ? ORDER BY created_at DESC LIMIT 20;
```

**Good Design (Cassandra-style):**

```sql
-- Table 1: User timeline (optimized for Query 1)
CREATE TABLE user_timeline (
    user_id UUID,
    post_id TIMEUUID,
    content TEXT,
    created_at TIMESTAMP,
    PRIMARY KEY (user_id, post_id)
) WITH CLUSTERING ORDER BY (post_id DESC);

-- Table 2: Post details (optimized for Query 2)
CREATE TABLE posts_by_id (
    post_id UUID PRIMARY KEY,
    user_id UUID,
    content TEXT,
    created_at TIMESTAMP
);

-- Table 3: Comments (optimized for Query 3)
CREATE TABLE post_comments (
    post_id UUID,
    comment_id TIMEUUID,
    user_id UUID,
    content TEXT,
    PRIMARY KEY (post_id, comment_id)
) WITH CLUSTERING ORDER BY (comment_id DESC);
```

**Data Duplication:**

- âœ… **Accepted trade-off** in Cassandra (denormalization).
- âœ… **Faster reads** (single-partition queries).
- âŒ **More storage** (but storage is cheap).
- âŒ **Write overhead** (must update multiple tables).

#### **8.2 Avoid Anti-Patterns**

| Anti-Pattern                  | Why Bad                                     | Solution                                  |
|-------------------------------|---------------------------------------------|-------------------------------------------|
| **Secondary indexes**         | Requires scatter-gather (queries all nodes) | Use materialized views or separate tables |
| **ALLOW FILTERING**           | Full table scan (extremely slow)            | Redesign table to match query             |
| **Large partitions (>100MB)** | Hotspots, slow queries                      | Use bucketing (e.g., partition by month)  |
| **Unbounded partitions**      | Partition grows forever (celebrity problem) | Time-bucket or limit partition size       |
| **Multi-partition queries**   | Slow (must query many nodes)                | Single-partition queries only             |

**Example: Fixing Secondary Index**

```sql
-- Bad: Secondary index
CREATE TABLE users (
    user_id UUID PRIMARY KEY,
    email TEXT,
    name TEXT
);
CREATE INDEX ON users(email);  -- Queries all nodes!

-- Good: Separate table
CREATE TABLE users_by_email (
    email TEXT PRIMARY KEY,
    user_id UUID,
    name TEXT
);
```

---

### 9. Performance Tuning

#### **9.1 Compaction Strategy**

| Strategy               | Best For               | Trade-offs                                    |
|------------------------|------------------------|-----------------------------------------------|
| **Size-Tiered (STCS)** | Write-heavy workloads  | âŒ Read amplification (many SSTables)          |
| **Leveled (LCS)**      | Read-heavy workloads   | âŒ Write amplification (more compaction)       |
| **Time-Window (TWCS)** | Time-series data (TTL) | âœ… Efficient TTL expiry (drops entire SSTable) |

**Example: Time-Series Data**

```sql
CREATE TABLE sensor_data (
    sensor_id UUID,
    timestamp TIMESTAMP,
    value DOUBLE,
    PRIMARY KEY (sensor_id, timestamp)
) WITH CLUSTERING ORDER BY (timestamp DESC)
  AND compaction = {'class': 'TimeWindowCompactionStrategy', 'compaction_window_unit': 'DAYS', 'compaction_window_size': 1}
  AND default_time_to_live = 2592000;  -- 30 days
```

#### **9.2 Read Optimization**

**Row Cache:**

- Caches entire rows in memory (like Redis).
- Use for hot rows (e.g., celebrity profiles).

```sql
ALTER TABLE users WITH caching = {'keys': 'ALL', 'rows_per_partition': '100'};
```

**Key Cache:**

- Caches partition key offsets (always enabled by default).

**Partition Index:**

- Index inside each SSTable (automatically managed).

#### **9.3 Monitoring Key Metrics**

| Metric                 | What to Monitor                    | Threshold        |
|------------------------|------------------------------------|------------------|
| **Write latency**      | p99 latency                        | < 10ms           |
| **Read latency**       | p99 latency                        | < 50ms           |
| **Compaction pending** | Number of pending compaction tasks | < 10             |
| **Memtable flush**     | Flush rate                         | Should be steady |
| **Gossip state**       | Node UP/DOWN status                | All nodes UP     |
| **Replication lag**    | Hinted handoff queue size          | < 1000           |

**Tools:**

- **nodetool:** CLI for ops (`nodetool status`, `nodetool tpstats`).
- **Prometheus + Cassandra Exporter:** Metrics scraping.
- **Grafana:** Visualization.

---

### 10. When to Use Cassandra

#### **âœ… Use Cassandra When:**

1. **High write throughput** â€” Logs, time-series data, IoT sensors (millions of writes/sec).
2. **Linear scalability** â€” Need to scale horizontally (add nodes for more capacity).
3. **No single point of failure** â€” Always-on, multi-datacenter replication.
4. **Predictable query patterns** â€” Know your queries upfront (single-partition queries).
5. **Time-series data with TTL** â€” Automatic expiration (TWCS compaction).
6. **Geo-distributed users** â€” Multi-region, low-latency reads via LOCAL_QUORUM.

#### **âŒ Don't Use Cassandra When:**

1. **Complex joins and transactions** â€” Use PostgreSQL or MySQL instead.
2. **Ad-hoc queries** â€” Cassandra requires query-driven design upfront.
3. **Strong consistency required** â€” ACID guarantees easier in RDBMS.
4. **Small dataset (< 1TB)** â€” Overhead of Cassandra not worth it (use PostgreSQL).
5. **Heavy updates/deletes** â€” Tombstone overhead (eventual consistency issues).

---

### 11. Real-World Examples

| Company       | Use Case                          | Why Cassandra?                                          |
|---------------|-----------------------------------|---------------------------------------------------------|
| **Netflix**   | Viewing history, recommendations  | 2.5 trillion writes/day, multi-region, always-on        |
| **Apple**     | iCloud backend                    | Multi-DC, billions of users, high availability          |
| **Instagram** | User feeds, DMs                   | Time-series data, high write throughput                 |
| **Discord**   | Message history                   | Handles spikes (millions of messages/sec during events) |
| **Uber**      | Trip history, location tracking   | Time-series data, geo-distributed                       |
| **Spotify**   | User playlists, listening history | High write throughput, multi-region                     |

---

### 12. Cassandra vs. Other Databases

| Feature          | Cassandra                    | PostgreSQL                    | MongoDB            | DynamoDB               |
|------------------|------------------------------|-------------------------------|--------------------|------------------------|
| **Architecture** | Masterless (peer-to-peer)    | Leader-follower               | Leader-follower    | Managed (AWS)          |
| **Consistency**  | Tunable (eventual to strong) | Strong (ACID)                 | Eventual (default) | Eventual (default)     |
| **Scalability**  | Linear (horizontal)          | Vertical (limited horizontal) | Horizontal         | Fully managed          |
| **Writes**       | Very fast (O(1))             | Moderate                      | Fast               | Fast                   |
| **Reads**        | Slower (multi-SSTable)       | Fast (indexes)                | Fast               | Fast                   |
| **Joins**        | Not supported                | Supported                     | Limited            | Not supported          |
| **Use Case**     | High-write, time-series      | ACID, relational              | Flexible documents | Serverless, AWS-native |

---

### 13. Common Anti-Patterns

#### âŒ **1. Using SELECT * or ALLOW FILTERING**

**Problem:** Full table scan (queries all partitions).

```sql
-- Bad (scans all nodes)
SELECT * FROM users WHERE age > 25 ALLOW FILTERING;

-- Good (single-partition query)
SELECT * FROM users WHERE user_id = ?;
```

#### âŒ **2. Unbounded Partitions**

**Problem:** One partition grows forever (hotspot).

**Solution:** Time-bucket partitions.

```sql
-- Bad
PRIMARY KEY (user_id, event_time)

-- Good
PRIMARY KEY ((user_id, bucket_day), event_time)
```

#### âŒ **3. Using Secondary Indexes**

**Problem:** Queries all nodes (slow).

**Solution:** Materialized view or separate table.

#### âŒ **4. Not Running Repair**

**Problem:** Replicas drift over time.

**Solution:** Run `nodetool repair` weekly.

---

### 14. Summary

| What Cassandra Does Best                  | What Cassandra Struggles With              |
|-------------------------------------------|--------------------------------------------|
| âœ… Massive write throughput (millions/sec) | âŒ Complex queries (joins, aggregations)    |
| âœ… Linear scalability (just add nodes)     | âŒ Strong consistency (eventual by default) |
| âœ… No single point of failure (masterless) | âŒ Ad-hoc queries (requires upfront design) |
| âœ… Multi-datacenter replication            | âŒ Heavy updates/deletes (tombstones)       |
| âœ… Time-series data with TTL               | âŒ Small datasets (overkill)                |

---

### 15. References

- **Apache Cassandra Documentation:** [https://cassandra.apache.org/doc/](https://cassandra.apache.org/doc/)
- **DataStax Academy (Free Courses):** [https://academy.datastax.com/](https://academy.datastax.com/)
- **Cassandra: The Definitive Guide (Book):** By Jeff Carpenter & Eben Hewitt
- **Related Chapters:**
    - [2.1.2 NoSQL Deep Dive](./2.1.2-no-sql-deep-dive.md) â€” BASE principles
    - [2.2.2 Consistent Hashing](./2.2.2-consistent-hashing.md) â€” How Cassandra distributes data
    - [2.1.4 Database Scaling](./2.1.4-database-scaling.md) â€” Horizontal scaling strategies
    - [2.5.2 Consensus Algorithms](./2.5.2-consensus-algorithms.md) â€” Paxos in Cassandra

---

## âœï¸ Design Challenge

### Problem

You're designing a **time-series IoT platform** that collects sensor data from **10 million devices** worldwide.
Requirements:

1. **Write throughput:** 100,000 writes/sec (each device reports every 10 seconds)
2. **Data retention:** 90 days (auto-delete old data)
3. **Query pattern:** Fetch last 24 hours of data for a specific device
4. **High availability:** No downtime during datacenter failures
5. **Multi-region:** Devices spread across US, EU, Asia

Your team is considering **Cassandra vs. PostgreSQL with TimescaleDB**.

**Question:** Which database would you choose, what partition key design would you use, and how would you handle the
time-series nature of the data?

### Solution

#### ğŸ§© Scenario

- **System:** IoT sensor data collection
- **Scale:** 10M devices Ã— 100 bytes/reading Ã— 6 readings/min = **60GB/hour**
- **Write load:** 100,000 writes/sec
- **Query pattern:** `SELECT * FROM sensor_data WHERE device_id = ? AND timestamp >= now() - 24h`
- **Retention:** 90 days (auto-delete)
- **Geographic distribution:** Multi-region (US, EU, Asia)

#### âœ… Goal

- Handle massive write throughput without throttling
- Efficient time-range queries (last 24 hours)
- Automatic data expiration (TTL)
- High availability across regions
- Linear scalability (add nodes = more capacity)

#### âš™ï¸ Solution: Cassandra (Clear Winner for This Use Case)

**Why Cassandra?**

| Requirement             | Cassandra                                 | PostgreSQL + TimescaleDB                         |
|-------------------------|-------------------------------------------|--------------------------------------------------|
| **Write throughput**    | âœ… Excellent (100k+ writes/sec per node)   | âš ï¸ Limited (10k-20k writes/sec on single master) |
| **Linear scaling**      | âœ… Add nodes = more capacity               | âŒ Write bottleneck on master                     |
| **TTL**                 | âœ… Built-in (automatic deletion)           | âš ï¸ Manual DELETE or partition drops              |
| **Multi-region**        | âœ… Native multi-DC replication             | âš ï¸ Complex (streaming replication)               |
| **High availability**   | âœ… Masterless (no single point of failure) | âš ï¸ Master failover required                      |
| **Time-series queries** | âœ… Excellent (with proper partition key)   | âœ… Excellent (TimescaleDB hypertables)            |

**Recommended Data Model:**

```sql
CREATE TABLE sensor_data (
    device_id UUID,
    bucket_day TEXT,       -- e.g., "2024-01-15" (prevents wide partitions)
    timestamp TIMESTAMP,
    temperature FLOAT,
    humidity FLOAT,
    battery_level INT,
    PRIMARY KEY ((device_id, bucket_day), timestamp)
) WITH CLUSTERING ORDER BY (timestamp DESC)
  AND compaction = {'class': 'TimeWindowCompactionStrategy', 'compaction_window_unit': 'DAYS', 'compaction_window_size': 1}
  AND default_time_to_live = 7776000;  -- 90 days in seconds
```

**Key Design Decisions:**

1. **Partition Key: `(device_id, bucket_day)`**
    - **Why composite:** Prevents unbounded partitions (one device = 90 days of data)
    - **Bucketing by day:** Each partition contains 24 hours of data (~8,640 readings)
    - **Partition size:** 8,640 readings Ã— 100 bytes = ~864KB (well under 100MB limit)

2. **Clustering Key: `timestamp DESC`**
    - **Why DESC:** Most recent data first (common query pattern)
    - **Benefit:** `SELECT * WHERE device_id = ? AND bucket_day = '2024-01-15' LIMIT 100` is $\text{O}(1)$

3. **TTL: 90 days**
    - **Automatic deletion:** No manual cleanup jobs
    - **Efficient:** TWCS compaction drops entire SSTables (no tombstone overhead)

4. **Time-Window Compaction (TWCS)**
    - **Optimized for time-series:** Groups data by time windows
    - **Fast expiration:** Drops entire SSTable when all data expires (no compaction overhead)

**Example Queries:**

```sql
-- Query last 24 hours for device
SELECT * FROM sensor_data
WHERE device_id = 123e4567-e89b-12d3-a456-426614174000
  AND bucket_day IN ('2024-01-15', '2024-01-14')
  AND timestamp >= '2024-01-14 10:00:00'
ORDER BY timestamp DESC;

-- Query last hour (single partition)
SELECT * FROM sensor_data
WHERE device_id = 123e4567-e89b-12d3-a456-426614174000
  AND bucket_day = '2024-01-15'
  AND timestamp >= '2024-01-15 09:00:00'
ORDER BY timestamp DESC;
```

#### âš ï¸ Trade-offs and Challenges

**Challenge 1: Query Spans Multiple Days**

**Problem:** Query for "last 24 hours" might span 2 bucket_days.

**Solution:**

```python
# Application logic
today = datetime.now().strftime('%Y-%m-%d')
yesterday = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')

# Query both partitions
query = """
    SELECT * FROM sensor_data
    WHERE device_id = ?
      AND bucket_day IN (?, ?)
      AND timestamp >= ?
    ORDER BY timestamp DESC
"""
# Cassandra driver merges and sorts results
```

**Challenge 2: Hot Partitions (Celebrity Device)**

**Problem:** One device generates 10x more data (e.g., high-frequency sensor).

**Solution:** Add sub-bucketing by hour:

```sql
PRIMARY KEY ((device_id, bucket_day, bucket_hour), timestamp)
```

**Challenge 3: Consistency for Recent Data**

**Problem:** Eventual consistency might show stale data.

**Solution:** Use **LOCAL_QUORUM** for reads/writes:

```python
# Write with LOCAL_QUORUM (2 out of 3 local replicas)
session.execute(query, consistency_level=ConsistencyLevel.LOCAL_QUORUM)

# Read with LOCAL_QUORUM (guaranteed to see recent writes)
session.execute(query, consistency_level=ConsistencyLevel.LOCAL_QUORUM)
```

#### ğŸ§  Multi-Region Architecture

```
Region: US-East (Primary)
  â”œâ”€ Node 1, Node 2, Node 3 (RF=3)
  â””â”€ LOCAL_QUORUM writes/reads

Region: EU-West (Replica)
  â”œâ”€ Node 4, Node 5 (RF=2)
  â””â”€ LOCAL_QUORUM writes/reads

Region: Asia-Pacific (Replica)
  â”œâ”€ Node 6, Node 7 (RF=2)
  â””â”€ LOCAL_QUORUM writes/reads

Cross-DC Replication: Async (eventual consistency)
Benefit: Each region serves local devices with low latency
```

**Configuration:**

```sql
CREATE KEYSPACE iot_data WITH REPLICATION = {
  'class': 'NetworkTopologyStrategy',
  'us-east': 3,
  'eu-west': 2,
  'asia-pacific': 2
};
```

#### âœ… Final Answer

| Aspect                 | Decision                            | Reason                                                           |
|------------------------|-------------------------------------|------------------------------------------------------------------|
| **Database**           | **Cassandra**                       | Designed for high-write workloads, linear scaling, masterless HA |
| **Partition Key**      | `(device_id, bucket_day)`           | Bounded partitions (~864KB/day), prevents hotspots               |
| **Clustering Key**     | `timestamp DESC`                    | Recent data first, efficient range queries                       |
| **TTL**                | 90 days                             | Automatic deletion, no manual cleanup                            |
| **Compaction**         | Time-Window (TWCS)                  | Optimized for time-series, fast expiration                       |
| **Consistency**        | LOCAL_QUORUM                        | Balance between consistency and latency                          |
| **Replication**        | Multi-DC (RF=3 US, RF=2 EU/Asia)    | High availability, low latency per region                        |
| **Trade-off Accepted** | Eventual consistency across regions | Gain: High availability, no single point of failure              |

**Key Metrics:**

- **Write latency:** < 5ms (p99) per region
- **Query latency:** < 10ms (p99) for single-day queries
- **Storage:** ~5.4TB for 90 days (60GB/hour Ã— 24 Ã— 90 days)
- **Cost:** ~$5,000/month (9 nodes Ã— $500/node, reserved instances)

**Why NOT PostgreSQL:**

- Single-master write bottleneck (cannot handle 100k writes/sec)
- Complex multi-region setup
- Manual TTL management (DELETE jobs = performance hit)
- Vertical scaling limits

**When to Reconsider:**

- If write volume drops below 10k writes/sec â†’ PostgreSQL + TimescaleDB viable
- If need complex analytics (JOINs, aggregations) â†’ Consider data warehouse (Redshift, ClickHouse)

