# 2.1.15 ClickHouse Deep Dive: The Real-Time Analytics Database

## Intuitive Explanation

ClickHouse is a **columnar OLAP database** designed for **real-time analytics** on massive datasets. Unlike traditional row-based databases (MySQL, PostgreSQL) that excel at transactional workloads, ClickHouse is optimized for **analytical queries** (aggregations, filtering, group by) on billions of rows. Think of it as a lightning-fast analytics engine that can scan and aggregate terabytes of data in seconds.

- **Columnar Storage:** Stores data by column (not row) for efficient aggregation
- **Blazing Fast:** 100x-1000x faster than MySQL for analytical queries
- **Petabyte Scale:** Can handle petabytes of data
- **Real-Time:** Sub-second query latency on billions of rows
- **Use Cases:** Real-time analytics dashboards, log analysis, metrics storage, OLAP cubes

**The Power:** Count 1 billion rows in milliseconds (vs. minutes in MySQL).

---

## In-Depth Analysis

### 1. Columnar Storage: The Secret to Speed

**Row-Based (MySQL, PostgreSQL):**

```
Row 1: | id=1 | name="Alice" | age=30 | city="NYC"  |
Row 2: | id=2 | name="Bob"   | age=25 | city="LA"   |
Row 3: | id=3 | name="Charlie"| age=35| city="SF"   |

Query: SELECT AVG(age) FROM users;
→ Must read all columns (id, name, age, city) even though only age is needed
```

**Columnar (ClickHouse):**

```
id column:    | 1 | 2 | 3 |
name column:  | "Alice" | "Bob" | "Charlie" |
age column:   | 30 | 25 | 35 |  ← Only this column is read!
city column:  | "NYC" | "LA" | "SF" |

Query: SELECT AVG(age) FROM users;
→ Only reads age column (3x less data scanned)
```

**Benefits:**

| Benefit | Explanation |
|---------|-------------|
| **Better compression** | Similar values in column compress better (age: [30, 31, 32...] compresses 10x) |
| **Fewer disk reads** | Read only needed columns (not entire rows) |
| **CPU cache efficiency** | Sequential column data fits in CPU cache |
| **Vectorized execution** | Process entire column chunks at once (SIMD) |

---

### 2. Architecture

```
┌────────────────────────────────────────────┐
│           ClickHouse Cluster               │
├────────────────────────────────────────────┤
│  ┌──────────────┐  ┌──────────────┐       │
│  │  Shard 1     │  │  Shard 2     │       │
│  │  (Replica 1) │  │  (Replica 1) │       │
│  └──────────────┘  └──────────────┘       │
│  ┌──────────────┐  ┌──────────────┐       │
│  │  Shard 1     │  │  Shard 2     │       │
│  │  (Replica 2) │  │  (Replica 2) │       │
│  └──────────────┘  └──────────────┘       │
│                                             │
│  Data partitioned by sharding key          │
│  Each shard replicated for HA              │
└────────────────────────────────────────────┘
```

**Key Components:**

- **Shards:** Horizontal partitions of data (distribute load)
- **Replicas:** Copies of each shard (high availability)
- **ZooKeeper:** Coordination for distributed queries
- **MergeTree Engine:** Primary storage engine (like InnoDB for MySQL)

---

### 3. Table Engines: MergeTree Family

**MergeTree (Most Common):**

```sql
CREATE TABLE events (
    event_date Date,
    event_time DateTime,
    user_id UInt64,
    event_type String,
    value Float64
) ENGINE = MergeTree()
PARTITION BY toYYYYMM(event_date)  -- Partition by month
ORDER BY (user_id, event_time)      -- Primary key (sort order)
SETTINGS index_granularity = 8192;  -- Index every 8192 rows
```

**Key Concepts:**

| Concept | Explanation | Example |
|---------|-------------|---------|
| **Partition** | Physical data separation (like folders) | Partition by month: `202401`, `202402` |
| **ORDER BY** | Sort order (acts as primary key) | `(user_id, event_time)` enables fast user queries |
| **PRIMARY KEY** | Optional (defaults to ORDER BY) | Sparse index for fast lookups |
| **index_granularity** | Rows per index mark | 8192 = balance between index size and query speed |

**Other MergeTree Variants:**

| Engine | Use Case | Features |
|--------|----------|----------|
| **ReplacingMergeTree** | Deduplication | Replaces rows with same ORDER BY key |
| **SummingMergeTree** | Pre-aggregation | Sums numeric columns on merge |
| **AggregatingMergeTree** | Complex aggregations | Stores aggregated states (count, avg, etc.) |
| **CollapsingMergeTree** | Update/delete rows | Uses `sign` column (+1 insert, -1 delete) |

---

### 4. Queries: SQL with Extensions

**Basic Queries:**

```sql
-- Count events
SELECT COUNT(*) FROM events WHERE event_date = '2024-01-15';

-- Group by aggregation
SELECT 
    event_type,
    COUNT(*) AS count,
    AVG(value) AS avg_value
FROM events
WHERE event_date >= '2024-01-01'
GROUP BY event_type
ORDER BY count DESC;

-- Time-series aggregation
SELECT 
    toStartOfHour(event_time) AS hour,
    COUNT(*) AS events_per_hour
FROM events
WHERE event_date = '2024-01-15'
GROUP BY hour
ORDER BY hour;
```

**Advanced Features:**

```sql
-- ARRAY JOIN (expand arrays)
SELECT 
    user_id,
    tag
FROM user_tags
ARRAY JOIN tags AS tag;

-- WITH TOTALS (add summary row)
SELECT 
    event_type,
    COUNT(*) AS count
FROM events
GROUP BY event_type
WITH TOTALS;

-- SAMPLE (random sampling for faster queries)
SELECT COUNT(*) FROM events SAMPLE 0.1;  -- Query 10% of data

-- Approximate aggregations (HyperLogLog)
SELECT uniq(user_id) FROM events;  -- Approximate unique count
```

---

### 5. Performance Optimization

#### **5.1 Partitioning Strategy**

**Example: Partition by Date**

```sql
CREATE TABLE logs (
    log_date Date,
    log_time DateTime,
    message String
) ENGINE = MergeTree()
PARTITION BY toYYYYMM(log_date)  -- Monthly partitions
ORDER BY (log_time);
```

**Benefits:**

- **Fast queries:** `WHERE log_date = '2024-01-15'` scans only that partition
- **Efficient deletes:** `ALTER TABLE logs DROP PARTITION '202401'` (instant)
- **Data management:** Easily move old partitions to cold storage

#### **5.2 ORDER BY Optimization**

**Rule:** Put most filtered columns first in ORDER BY.

```sql
-- ✅ Good: Queries filter by user_id first
ORDER BY (user_id, event_time)

-- Query
SELECT * FROM events WHERE user_id = 12345;  -- Fast (uses sort order)

-- ❌ Bad: user_id not first
ORDER BY (event_time, user_id)

-- Query
SELECT * FROM events WHERE user_id = 12345;  -- Slow (full scan)
```

#### **5.3 Data Types**

**Use smallest data types:**

| Instead of | Use | Savings |
|------------|-----|---------|
| `String` (variable) | `FixedString(N)` | Fixed size, better compression |
| `UInt64` | `UInt32` or `UInt16` | 50-75% smaller |
| `DateTime` | `DateTime64` | Higher precision if needed |
| `String` (enum) | `Enum8('val1', 'val2')` | 1 byte vs. variable |

---

### 6. Replication and High Availability

**ReplicatedMergeTree:**

```sql
CREATE TABLE events_replica (
    event_date Date,
    user_id UInt64,
    value Float64
) ENGINE = ReplicatedMergeTree('/clickhouse/tables/events', '{replica}')
PARTITION BY toYYYYMM(event_date)
ORDER BY (user_id, event_date);
```

**Replication Flow:**

```
Write to Replica 1
    │
    ├─> ZooKeeper (log entry)
    │
    ▼
Replica 2, Replica 3 fetch from ZooKeeper
    │
    ▼
All replicas eventually consistent
```

**Benefits:**

- **High availability:** If one replica fails, others serve queries
- **Load balancing:** Distribute read queries across replicas
- **Automatic failover:** ZooKeeper handles replica coordination

---

### 7. When to Use ClickHouse

#### **✅ Use ClickHouse When:**

1. **Analytics workloads** — Aggregations, GROUP BY, time-series analysis
2. **Append-only data** — Logs, events, metrics (rarely update/delete)
3. **Billions of rows** — Large datasets (100M+ rows)
4. **Real-time dashboards** — Sub-second query latency
5. **High write throughput** — Millions of inserts/sec
6. **Column-based queries** — `SELECT avg(value)` (not `SELECT *`)

#### **❌ Don't Use ClickHouse When:**

1. **Transactional workloads** — OLTP (use PostgreSQL/MySQL)
2. **Frequent updates/deletes** — ClickHouse is append-only (slow updates)
3. **Small datasets** — <10M rows (PostgreSQL is simpler)
4. **Row-based queries** — `SELECT * FROM users WHERE id = 123` (use row DB)
5. **Strong consistency** — ClickHouse is eventually consistent
6. **Complex joins** — Limited JOIN support (optimize for single-table queries)

---

### 8. Real-World Examples

| Company | Use Case | Why ClickHouse? |
|---------|----------|-----------------|
| **Cloudflare** | Log analytics (1 trillion rows) | Real-time analytics on petabytes of logs |
| **Uber** | Real-time metrics | Sub-second queries on billions of rides |
| **Spotify** | User behavior analytics | Fast aggregations on listening data |
| **Yandex** | Web analytics (Metrica) | Created ClickHouse for their analytics |
| **eBay** | Real-time dashboards | 100x faster than their previous Hadoop solution |

---

### 9. ClickHouse vs. Other Databases

| Feature | ClickHouse | PostgreSQL | BigQuery | Redshift |
|---------|------------|------------|----------|----------|
| **Workload** | OLAP | OLTP | OLAP | OLAP |
| **Storage** | Columnar | Row-based | Columnar | Columnar |
| **Query speed** | ⚡ Sub-second (billions of rows) | ⚠️ Slow for aggregations | ⚡ Fast (serverless) | ⚡ Fast |
| **Scalability** | Horizontal (sharding) | Vertical | Serverless | Horizontal |
| **Cost** | Open-source (self-hosted) | Open-source | Pay per query | Pay per cluster |
| **Compression** | Excellent (10-100x) | Good | Excellent | Excellent |
| **Use case** | Self-hosted analytics | General purpose | Cloud analytics | AWS analytics |

---

### 10. Common Anti-Patterns

#### ❌ **1. Using ClickHouse for OLTP**

**Problem:** Frequent updates/deletes are slow.

```sql
-- Bad: Updating individual rows (very slow)
UPDATE events SET value = 100 WHERE user_id = 123;
```

**Solution:** Use PostgreSQL for transactional data, ClickHouse for analytics.

#### ❌ **2. Not Partitioning Data**

**Problem:** Queries scan entire table.

**Solution:** Partition by date for time-series data.

```sql
PARTITION BY toYYYYMM(event_date)
```

#### ❌ **3. Wrong ORDER BY**

**Problem:** ORDER BY doesn't match query filters.

```sql
-- Bad: Queries filter by user_id, but ORDER BY is event_time first
ORDER BY (event_time, user_id)
```

**Solution:** Put most filtered columns first.

```sql
ORDER BY (user_id, event_time)
```

---

### 11. Trade-offs Summary

| What You Gain | What You Sacrifice |
|---------------|-------------------|
| ✅ 100-1000x faster analytics | ❌ Slow updates/deletes (append-only design) |
| ✅ Excellent compression (10-100x) | ❌ Not suitable for OLTP workloads |
| ✅ Real-time queries (sub-second) | ❌ Eventually consistent (not ACID) |
| ✅ Horizontal scaling (petabyte scale) | ❌ Complex joins are slow |
| ✅ High write throughput (millions/sec) | ❌ Higher ops complexity (ZooKeeper, sharding) |

---

### 12. References

- **ClickHouse Documentation:** [https://clickhouse.com/docs/](https://clickhouse.com/docs/)
- **ClickHouse GitHub:** [https://github.com/ClickHouse/ClickHouse](https://github.com/ClickHouse/ClickHouse)
- **Related Chapters:**
  - [2.1.1 RDBMS Deep Dive](./2.1.1-rdbms-deep-dive.md) — OLTP vs. OLAP
  - [2.1.3 Specialized Databases](./2.1.3-specialized-databases.md) — Time-series databases
  - [2.3.5 Batch vs Stream Processing](./2.3.5-batch-vs-stream-processing.md) — Real-time analytics

---

## ✏️ Design Challenge

### Problem

You're building a **real-time analytics dashboard** for a SaaS product that tracks user behavior. Requirements:

1. **Write load:** 10,000 events/sec (user clicks, page views, API calls)
2. **Data retention:** 365 days (auto-delete old data)
3. **Query patterns:**
   - Daily active users (DAU)
   - Event count by type (GROUP BY)
   - Average session duration
   - Top 10 pages by views
4. **Latency:** Dashboard queries must complete in <1 second (querying 1 billion rows)
5. **Cost:** Minimize infrastructure cost

**Question:** Should you use ClickHouse or PostgreSQL? What partitioning strategy would you use? How would you optimize queries for sub-second latency?

### Solution

#### 🧩 Scenario

- **System:** Real-time analytics dashboard
- **Write load:** 10,000 events/sec = 864M events/day
- **Total data:** 365 days × 864M = 315B events/year
- **Query patterns:** Aggregations (COUNT, AVG, GROUP BY)
- **Latency requirement:** <1 second on 1B+ rows

#### ✅ Goal

- Handle 10K writes/sec without bottlenecks
- Query 1B+ rows in <1 second
- Auto-delete data older than 365 days
- Minimize storage cost (compression)
- Simple maintenance

#### ⚙️ Solution: ClickHouse (Clear Winner)

**Why ClickHouse over PostgreSQL?**

| Requirement | ClickHouse | PostgreSQL |
|-------------|------------|------------|
| **Analytics queries** | ✅ 100-1000x faster (columnar) | ❌ Slow on 1B+ rows |
| **Write throughput** | ✅ Millions of writes/sec | ⚠️ 10K writes/sec is limit |
| **Compression** | ✅ 10-100x (columnar compression) | ⚠️ 2-3x compression |
| **Partition management** | ✅ Drop partitions instantly | ⚠️ DELETE is slow |
| **Query latency** | ✅ Sub-second on 1B rows | ❌ Minutes on 1B rows |
| **Cost** | ✅ Lower (better compression) | ❌ Higher (more storage) |

**Recommended Schema:**

```sql
CREATE TABLE events (
    event_date Date,
    event_time DateTime,
    event_type LowCardinality(String),  -- Enum-like optimization
    user_id UInt64,
    session_id UUID,
    page_url String,
    duration UInt32,  -- Session duration in seconds
    device_type LowCardinality(String)
) ENGINE = MergeTree()
PARTITION BY toYYYYMM(event_date)  -- Monthly partitions
ORDER BY (event_type, user_id, event_time)  -- Primary key (sort order)
TTL event_date + INTERVAL 365 DAY  -- Auto-delete after 365 days
SETTINGS index_granularity = 8192;
```

**Key Design Decisions:**

1. **Partitioning: Monthly (`toYYYYMM`)**
   - **Why:** Balance between partition count and query speed
   - **Benefit:** `ALTER TABLE events DROP PARTITION '202301'` (instant delete)
   - **Alternative:** Daily partitions (more partitions, slower queries)

2. **ORDER BY: `(event_type, user_id, event_time)`**
   - **Why:** Most queries filter by `event_type` first, then `user_id`
   - **Benefit:** Fast queries like `WHERE event_type = 'page_view' AND user_id = 123`

3. **TTL: 365 days**
   - **Why:** Automatic data expiration (no manual DELETE jobs)
   - **Benefit:** ClickHouse drops old data automatically

4. **LowCardinality(String)**
   - **Why:** `event_type` has few unique values (10-20 types)
   - **Benefit:** 10x better compression than plain String

**Example Queries:**

```sql
-- 1. Daily Active Users (DAU)
SELECT 
    toDate(event_time) AS date,
    uniq(user_id) AS dau  -- HyperLogLog approximate count
FROM events
WHERE event_date >= today() - 30
GROUP BY date
ORDER BY date;

-- 2. Event count by type
SELECT 
    event_type,
    COUNT(*) AS count
FROM events
WHERE event_date = today()
GROUP BY event_type
ORDER BY count DESC;

-- 3. Average session duration
SELECT 
    AVG(duration) AS avg_session_duration
FROM events
WHERE event_type = 'session_end'
  AND event_date >= today() - 7;

-- 4. Top 10 pages by views
SELECT 
    page_url,
    COUNT(*) AS views
FROM events
WHERE event_type = 'page_view'
  AND event_date >= today() - 7
GROUP BY page_url
ORDER BY views DESC
LIMIT 10;
```

**Performance:**

- **Query 1 (DAU):** <500ms (scanning 30 days of data, ~25B rows)
- **Query 2 (Event count):** <200ms (scanning 1 day, ~864M rows)
- **Query 3 (Avg duration):** <300ms (filtering + aggregation)
- **Query 4 (Top pages):** <400ms (group by + sort)

#### ⚠️ Handling High Write Load

**Problem:** 10K writes/sec = 864M writes/day. How to ingest efficiently?

**Solution: Batch Inserts**

```python
import clickhouse_driver

client = clickhouse_driver.Client('localhost')

# Batch buffer
batch = []
BATCH_SIZE = 10000

def insert_event(event):
    batch.append(event)
    
    if len(batch) >= BATCH_SIZE:
        # Bulk insert
        client.execute(
            'INSERT INTO events (event_date, event_time, event_type, user_id, ...) VALUES',
            batch
        )
        batch.clear()

# Process 10K events/sec
for event in event_stream:
    insert_event(event)
```

**Why Batch?**

- ✅ **10-100x faster** than individual inserts
- ✅ **Less overhead** (fewer network round-trips)
- ✅ **Better compression** (larger data blocks)

**Recommended Batch Size:** 10,000-100,000 rows per insert.

#### 🧠 Cost Optimization

**Storage Calculation:**

```
Without compression (PostgreSQL):
- 864M events/day × 200 bytes/event = 173GB/day
- 365 days × 173GB = 63TB/year
- Cost: 63TB × $0.10/GB/month = $6,300/month

With ClickHouse compression (10x):
- 63TB / 10 = 6.3TB/year
- Cost: 6.3TB × $0.10/GB/month = $630/month

Savings: $5,670/month (90% reduction!)
```

**Infrastructure:**

```
ClickHouse cluster:
- 3 servers × 2TB SSD each = 6TB storage
- 3 servers × 64GB RAM each = 192GB RAM
- Cost: 3 × $500/month = $1,500/month

Total cost: $1,500/month (vs. $10,000+/month for PostgreSQL with 63TB)
```

#### ✅ Final Answer

| Aspect | Decision | Reason |
|--------|----------|--------|
| **Database** | **ClickHouse** | 100x faster for analytics, 10x better compression |
| **Partitioning** | Monthly (`toYYYYMM`) | Balance partition count and query speed |
| **ORDER BY** | `(event_type, user_id, event_time)` | Matches most common query patterns |
| **TTL** | 365 days | Automatic data expiration (no manual cleanup) |
| **Batch Size** | 10,000-100,000 rows | 10-100x faster inserts |
| **Compression** | LowCardinality + columnar | 10x compression (6.3TB vs. 63TB) |
| **Infrastructure** | 3 servers × 2TB SSD | $1,500/month vs. $10,000+/month for PostgreSQL |
| **Query Latency** | <1 second on 1B rows | ClickHouse columnar advantage |

**Performance Metrics:**
- **Write throughput:** 10K events/sec (batched inserts)
- **Query latency:** <1 second on 1B+ rows
- **Storage:** 6.3TB (10x compression)
- **Cost:** $1,500/month (10x cheaper than PostgreSQL)

**Why NOT PostgreSQL:**
- ❌ Queries on 1B rows take minutes (not seconds)
- ❌ Storage: 63TB (10x more expensive)
- ❌ Write bottleneck at 10K events/sec
- ❌ DELETE old data is slow (hours to delete 1 month of data)

**When to Reconsider:**
- If need transactional updates (OLTP) → Use PostgreSQL for transactional data, ClickHouse for analytics (hybrid approach)
- If write load < 1K events/sec and data < 100M rows → PostgreSQL is simpler
