# 2.1.18 Vector Databases Deep Dive: Semantic Search and AI Embeddings

## Intuitive Explanation

Imagine you're searching for "a photo of a sunset over the ocean." Traditional databases would look for exact matches of those words. But what if the photo is tagged "beach sunset" or "ocean horizon"? You'd miss it.

**Vector Databases** understand meaning, not just keywords. They convert text, images, or other data into **vectors** (arrays of numbers) that capture semantic meaning. Similar meanings have similar vectors. So "sunset over ocean" and "beach sunset" would have similar vectors and be found together.

**Key Insight:** Vector databases use **similarity search** (finding nearest neighbors) instead of exact matches. This enables:
- **Semantic Search:** Find documents by meaning, not keywords
- **Recommendations:** Find similar products/users/content
- **Image Search:** Find similar images (reverse image search)
- **AI/ML Applications:** Store and search embeddings from LLMs

---

## In-Depth Analysis

### 1. What are Vector Embeddings?

**Vector Embedding** is a numerical representation of data (text, image, audio) in a high-dimensional space where similar items are close together.

**Example:**
```
Text: "I love machine learning"
Embedding: [0.23, -0.45, 0.67, ..., 0.12] (768 dimensions)

Text: "I enjoy artificial intelligence"
Embedding: [0.25, -0.43, 0.65, ..., 0.11] (768 dimensions)

â†’ These vectors are similar (close in vector space)
â†’ Because the meanings are similar
```

**How Embeddings Work:**
- **Text:** Word2Vec, BERT, GPT embeddings convert words/sentences to vectors
- **Images:** CNN (Convolutional Neural Networks) extract feature vectors
- **Audio:** Audio processing models create audio embeddings

### 2. What is a Vector Database?

A **Vector Database** is a specialized database optimized for storing and querying high-dimensional vectors using similarity search.

**Key Operations:**
- **Insert:** Store vectors with metadata
- **Search:** Find k nearest neighbors (k-NN search)
- **Update:** Update vectors and metadata
- **Delete:** Remove vectors

**Traditional Database vs Vector Database:**
```
Traditional Database:
  Query: "SELECT * FROM products WHERE name LIKE '%laptop%'"
  â†’ Exact match or pattern matching
  â†’ Misses "notebook computer" or "portable PC"

Vector Database:
  Query: Find products similar to "laptop" (vector search)
  â†’ Returns: "laptop", "notebook computer", "portable PC"
  â†’ Understands semantic similarity
```

### 3. Similarity Search Algorithms

#### A. Exact k-NN Search

**Brute Force:**
```
For each query vector:
  Calculate distance to all vectors in database
  Sort by distance
  Return top k nearest
```

**Time Complexity:** O(n Ã— d) where n = number of vectors, d = dimensions
**Use Case:** Small datasets (<100K vectors)

#### B. Approximate Nearest Neighbor (ANN)

**Problem:** Exact search too slow for large datasets (millions of vectors)

**Solution:** Approximate algorithms (trade accuracy for speed)

**Popular Algorithms:**

**1. Locality-Sensitive Hashing (LSH):**
- Hashes similar vectors to same buckets
- Search only in relevant buckets
- **Speed:** 100-1000Ã— faster than brute force
- **Accuracy:** 90-95% (may miss some results)

**2. Hierarchical Navigable Small World (HNSW):**
- Builds graph structure (nodes = vectors, edges = similarity)
- Navigates graph to find nearest neighbors
- **Speed:** 100-1000Ã— faster
- **Accuracy:** 95-99%

**3. Product Quantization (PQ):**
- Compresses vectors (reduces memory)
- Fast approximate search on compressed vectors
- **Speed:** 10-100Ã— faster
- **Accuracy:** 85-95%

**4. Inverted File Index (IVF):**
- Clusters vectors into groups (centroids)
- Searches only relevant clusters
- **Speed:** 10-100Ã— faster
- **Accuracy:** 90-95%

### 4. Distance Metrics

**Euclidean Distance (L2):**
```
distance = sqrt(Î£(ai - bi)Â²)
â†’ Measures straight-line distance
â†’ Good for: General similarity
```

**Cosine Similarity:**
```
similarity = (A Â· B) / (||A|| Ã— ||B||)
â†’ Measures angle between vectors
â†’ Good for: Text embeddings (magnitude doesn't matter)
```

**Dot Product:**
```
similarity = A Â· B
â†’ Measures alignment
â†’ Good for: Normalized vectors
```

**Manhattan Distance (L1):**
```
distance = Î£|ai - bi|
â†’ Measures city-block distance
â†’ Good for: Sparse vectors
```

### 5. Major Vector Database Solutions

#### Pinecone

**Architecture:**
- Fully managed service
- Serverless (pay per query)
- Auto-scaling

**Features:**
- HNSW indexing
- Metadata filtering
- Real-time updates
- Multi-region support

**Use Cases:**
- Semantic search
- Recommendation systems
- AI applications

**Pricing:** Pay per query, storage

#### Weaviate

**Architecture:**
- Open source + managed
- GraphQL API
- Vector + keyword search hybrid

**Features:**
- Vector search + BM25 (keyword search)
- GraphQL queries
- Auto-schema generation
- Multi-tenancy

**Use Cases:**
- Hybrid search (semantic + keyword)
- Knowledge graphs
- Multi-tenant applications

**Deployment:** Self-hosted or cloud

#### Milvus

**Architecture:**
- Open source
- Distributed (scales horizontally)
- Cloud-native

**Features:**
- Multiple index types (HNSW, IVF, PQ)
- Real-time search
- Batch processing
- High performance

**Use Cases:**
- Large-scale vector search (billions of vectors)
- Real-time recommendations
- Image search

**Deployment:** Kubernetes, Docker

#### Qdrant

**Architecture:**
- Open source
- Written in Rust (high performance)
- REST and gRPC APIs

**Features:**
- HNSW indexing
- Payload filtering (metadata queries)
- Point-in-time recovery
- Horizontal scaling

**Use Cases:**
- High-performance search
- Production deployments
- Custom embeddings

**Deployment:** Self-hosted or cloud

#### FAISS (Facebook AI Similarity Search)

**Architecture:**
- Library (not a database)
- C++ with Python bindings
- In-memory or on-disk

**Features:**
- Multiple index types
- GPU acceleration
- Very fast (optimized C++)
- No persistence (application manages)

**Use Cases:**
- Research and development
- High-performance applications
- Custom implementations

**Deployment:** Library (integrated into applications)

### 6. Vector Database Architecture

#### Storage Architecture

**Vector Storage:**
```
Vectors: Stored in memory (for fast access) or on disk (for large datasets)
Metadata: Stored in separate index (PostgreSQL, MongoDB)
```

**Index Structure:**
```
HNSW Graph:
  - Nodes: Vectors
  - Edges: Similarity connections
  - Layers: Hierarchical structure (fast navigation)
```

#### Query Flow

```
1. Query Vector: [0.23, -0.45, ..., 0.12]
2. Index Search: Navigate HNSW graph
3. Distance Calculation: Calculate distance to candidate vectors
4. Top-k Selection: Return k nearest neighbors
5. Metadata Filtering: Filter by metadata (optional)
6. Results: Return vectors + metadata
```

### 7. Common Use Cases

#### A. Semantic Search

**Problem:** Keyword search misses semantically similar content

**Solution:** Vector search finds content by meaning

**Example:**
```
Query: "How to fix a broken computer?"
Vector Search Returns:
  - "Troubleshooting PC issues" (similar meaning)
  - "Computer repair guide" (similar meaning)
  - "Hardware diagnostics" (similar meaning)
```

#### B. Recommendation Systems

**Problem:** Find similar items/users

**Solution:** Vector similarity

**Example:**
```
User Vector: [0.1, 0.2, ..., 0.3] (user preferences)
Product Vectors: [0.12, 0.18, ..., 0.28] (product features)
â†’ Find products with similar vectors
â†’ Recommend to user
```

#### C. Image Search

**Problem:** Find similar images

**Solution:** Image embeddings

**Example:**
```
Query Image: Photo of sunset
â†’ Extract embedding (CNN features)
â†’ Search for similar embeddings
â†’ Return: Other sunset photos
```

#### D. Chatbots and RAG (Retrieval-Augmented Generation)

**Problem:** LLMs need context from knowledge base

**Solution:** Vector search retrieves relevant context

**Example:**
```
User Question: "What is the return policy?"
â†’ Convert question to embedding
â†’ Search knowledge base for similar content
â†’ Retrieve: "Return Policy: 30 days..."
â†’ LLM uses retrieved context to answer
```

### 8. Performance Optimization

#### A. Index Selection

**HNSW:**
- Best accuracy (95-99%)
- Higher memory usage
- Good for: Production systems

**IVF:**
- Good balance (90-95% accuracy)
- Lower memory usage
- Good for: Large datasets

**PQ:**
- Highest compression
- Lower accuracy (85-95%)
- Good for: Memory-constrained systems

#### B. Dimensionality Reduction

**Problem:** High-dimensional vectors (768, 1536 dimensions) are slow to search

**Solution:** Reduce dimensions (PCA, UMAP)

**Trade-off:**
- **Lower dimensions:** Faster search, less memory
- **Higher dimensions:** Better accuracy, more memory

#### C. Batch Processing

**Problem:** Inserting millions of vectors one-by-one is slow

**Solution:** Batch inserts

```
Single Insert: 1000 vectors/second
Batch Insert (1000 vectors): 100,000 vectors/second
â†’ 100Ã— faster
```

---

## When to Use Vector Databases

### âœ… Use Vector Databases When:

1. **Semantic Search:** Need to find content by meaning, not keywords
2. **Recommendations:** Find similar items/users/content
3. **Image/Audio Search:** Find similar media files
4. **AI/ML Applications:** Store and search embeddings from LLMs
5. **RAG Systems:** Retrieve relevant context for LLMs
6. **Anomaly Detection:** Find outliers in high-dimensional data

### âŒ Don't Use Vector Databases When:

1. **Exact Matches:** Need exact keyword matching (use traditional database)
2. **Structured Queries:** Complex SQL queries (use SQL database)
3. **Small Dataset:** <10K vectors (overhead not worth it)
4. **Low-Dimensional Data:** <50 dimensions (traditional indexes work)
5. **No Similarity Need:** Don't need similarity search

---

## Real-World Examples

### OpenAI (Embeddings API)

**Use Case:** Semantic search, RAG systems

**Architecture:**
- OpenAI generates embeddings (text-embedding-ada-002)
- Applications store embeddings in vector database
- Search for similar content

**Scale:**
- Millions of embeddings
- Real-time search
- Used by ChatGPT, GitHub Copilot

### Netflix (Recommendations)

**Use Case:** Content recommendations

**Architecture:**
- User embeddings (preferences)
- Content embeddings (features)
- Vector similarity search

**Scale:**
- 200M+ users
- Billions of content vectors
- Real-time recommendations

### Pinterest (Visual Search)

**Use Case:** Find similar images

**Architecture:**
- Image embeddings (CNN features)
- Vector database for similarity search
- Real-time image search

**Scale:**
- Billions of images
- Sub-second search latency
- Mobile app integration

---

## Vector Databases vs. Other Solutions

| Solution | Best For | Latency | Scale | Accuracy |
|----------|----------|---------|-------|----------|
| **Vector Database** | Semantic search, recommendations | <10ms | Billions | 95-99% |
| **Elasticsearch** | Keyword search, full-text | <50ms | Billions | 100% (exact) |
| **PostgreSQL (pgvector)** | Small-scale vector search | <100ms | Millions | 90-95% |
| **FAISS (Library)** | High-performance, custom | <5ms | Billions | 95-99% |

---

## Common Anti-Patterns

### âŒ **1. Using Vector Database for Exact Matches**

**Problem:** Using vector search for exact keyword matching

**Solution:** Use traditional database for exact matches

```
âŒ Bad:
Query: "SELECT * FROM products WHERE name = 'laptop'"
â†’ Using vector search (overkill, slower)

âœ… Good:
Query: "SELECT * FROM products WHERE name = 'laptop'"
â†’ Using PostgreSQL (exact match, fast)
```

### âŒ **2. No Metadata Filtering**

**Problem:** Searching all vectors without filtering

**Solution:** Use metadata filtering to narrow search space

```
âŒ Bad:
Search: All 1B vectors for "laptop"
â†’ Slow (searches everything)

âœ… Good:
Search: Vectors where category = 'electronics' AND price < 1000
â†’ Fast (searches only relevant subset)
```

### âŒ **3. Wrong Distance Metric**

**Problem:** Using Euclidean distance for text embeddings

**Solution:** Use cosine similarity for text

```
âŒ Bad:
Text embeddings with Euclidean distance
â†’ Magnitude matters (wrong for text)

âœ… Good:
Text embeddings with cosine similarity
â†’ Only direction matters (correct for text)
```

---

## Trade-offs Summary

| Aspect | What You Gain | What You Sacrifice |
|--------|---------------|-------------------|
| **Semantic Search** | Find by meaning, not keywords | Higher latency than keyword search |
| **ANN Algorithms** | 100-1000Ã— faster search | 5-10% accuracy loss |
| **High Dimensions** | Better accuracy | Slower search, more memory |
| **Managed Service** | No infrastructure management | Vendor lock-in, cost at scale |
| **Open Source** | Full control, no vendor lock-in | Operational overhead |

---

## References

- **Pinecone Documentation:** [https://docs.pinecone.io/](https://docs.pinecone.io/)
- **Weaviate Documentation:** [https://weaviate.io/developers/weaviate](https://weaviate.io/developers/weaviate)
- **Milvus Documentation:** [https://milvus.io/docs](https://milvus.io/docs)
- **FAISS:** [https://github.com/facebookresearch/faiss](https://github.com/facebookresearch/faiss)
- **Related Chapters:**
  - [2.1.13 Elasticsearch Deep Dive](./2.1.13-elasticsearch-deep-dive.md) - Keyword search vs semantic search
  - [2.1.3 Specialized Databases](./2.1.3-specialized-databases.md) - Vector database overview

---

## âœï¸ Design Challenge

### Problem

You are designing a semantic search system for a knowledge base with 10 million documents. The system must:

1. **Store 10M document embeddings** (768 dimensions each)
2. **Support semantic search** ("How to fix X?" finds relevant docs)
3. **Handle 1000 queries per second** (peak traffic)
4. **Return results in <100ms** (p95 latency)
5. **Support metadata filtering** (filter by category, date, author)
6. **Update embeddings** when documents change (real-time updates)

**Constraints:**
- Each embedding: 768 dimensions (3 KB per vector)
- Total storage: 10M Ã— 3 KB = 30 GB (vectors only)
- Query pattern: 80% semantic search, 20% keyword + semantic hybrid
- Update frequency: 1000 documents updated per hour

Design a vector database strategy that:
- Handles storage scale
- Meets latency requirements
- Supports metadata filtering
- Handles real-time updates
- Optimizes for cost

### Solution

#### ðŸ§© Scenario

- **Documents:** 10 million
- **Embeddings:** 10M Ã— 768 dimensions = 30 GB
- **Queries:** 1000 QPS peak
- **Latency:** <100ms p95
- **Updates:** 1000 documents/hour

**Calculations:**
- **Storage:** 30 GB vectors + metadata = ~50 GB total
- **Memory:** Need to cache hot vectors (10% = 3 GB in memory)
- **Index Size:** HNSW index = 2-3Ã— vector size = 60-90 GB

#### âœ… Step 1: Vector Database Choice

**Choice: Milvus with HNSW Index**

**Why:**
- **Open Source:** No vendor lock-in, cost-effective
- **High Performance:** Handles 10K+ QPS per node
- **HNSW Index:** 95-99% accuracy, <10ms search latency
- **Metadata Filtering:** Built-in support
- **Horizontal Scaling:** Can scale to billions of vectors
- **Real-time Updates:** Supports incremental updates

#### âœ… Step 2: Architecture

**Cluster Configuration:**
```
Milvus Cluster:
  - 3 Query Nodes (handle search requests)
  - 3 Data Nodes (store vectors and indexes)
  - 3 Index Nodes (build and maintain indexes)
  - 1 Coordinator (orchestrates cluster)
  - 1 etcd (metadata storage)
  - 1 MinIO (object storage for vectors)
```

**Sharding Strategy:**
```
Collection: documents
  - Shards: 10 (by document_id hash)
  - Replicas: 2 (for high availability)
  - Total: 10 shards Ã— 2 replicas = 20 shard replicas
```

#### âœ… Step 3: Index Configuration

**HNSW Index:**
```
Index Type: HNSW
Parameters:
  - M: 16 (connections per node)
  - ef_construction: 200 (build quality)
  - ef_search: 100 (search quality)
  
Performance:
  - Build Time: ~2 hours for 10M vectors
  - Search Latency: <10ms (p95)
  - Accuracy: 98% (vs brute force)
  - Memory: 60 GB (index size)
```

**Why HNSW:**
- **High Accuracy:** 98% (vs 90-95% for IVF)
- **Fast Search:** <10ms (vs 50-100ms for IVF)
- **Good for:** Production systems with latency requirements

#### âœ… Step 4: Storage Strategy

**Tiered Storage:**
```
Hot Data (10% most accessed):
  - Storage: SSD (in-memory cache)
  - Latency: <5ms
  - Size: 3 GB

Warm Data (40%):
  - Storage: SSD
  - Latency: <20ms
  - Size: 12 GB

Cold Data (50%):
  - Storage: HDD (object storage)
  - Latency: <100ms
  - Size: 15 GB
```

**Object Storage (MinIO):**
```
MinIO Cluster:
  - 4 nodes (replication factor: 2)
  - Storage: 100 GB (vectors + indexes)
  - Throughput: 1 GB/s read, 500 MB/s write
```

#### âœ… Step 5: Query Optimization

**Metadata Filtering:**
```
Query: Semantic search + filter by category
  - Step 1: Filter by metadata (category = 'tech')
  - Step 2: Vector search only on filtered subset
  - Result: 10Ã— faster (searches 1M instead of 10M)
```

**Hybrid Search (80% semantic, 20% hybrid):**
```
Semantic Search (80%):
  - Pure vector search
  - Latency: <10ms

Hybrid Search (20%):
  - Vector search (70% weight)
  - Keyword search (30% weight)
  - Combine scores
  - Latency: <50ms
```

**Caching Strategy:**
```
Query Cache (Redis):
  - Cache frequent queries (TTL: 5 minutes)
  - Cache Hit Rate: 30%
  - Latency Reduction: 30% queries <1ms (cache hit)
```

#### âœ… Step 6: Real-Time Updates

**Update Strategy:**
```
Document Updated:
  1. Generate new embedding (async)
  2. Update vector in Milvus (async)
  3. Rebuild index incrementally (background)
  4. Update metadata (sync)
```

**Incremental Index Updates:**
```
Milvus HNSW:
  - Supports incremental updates
  - Rebuilds affected graph nodes
  - Background process (doesn't block queries)
  - Update Latency: <1 second (vector update)
  - Index Rebuild: 5-10 minutes (background)
```

**Update Queue:**
```
Kafka Topic: document_updates
  - Partitions: 10 (by document_id)
  - Consumers: Embedding Service (generates embeddings)
  - Consumers: Milvus Writer (updates vectors)
```

#### âœ… Step 7: Scaling Strategy

**Horizontal Scaling:**
```
Current: 10M vectors, 1000 QPS
  - 3 Query Nodes (handles 3K QPS)
  - 3 Data Nodes (stores 10M vectors)

Scale to 100M vectors, 10K QPS:
  - 10 Query Nodes (handles 10K QPS)
  - 10 Data Nodes (stores 100M vectors)
  - Add nodes incrementally (no downtime)
```

**Load Balancing:**
```
Query Load Balancer:
  - Routes queries to Query Nodes (round-robin)
  - Health checks (removes unhealthy nodes)
  - Sticky sessions (for connection pooling)
```

#### âœ… Complete Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Clients (1000 QPS peak)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Load Balancer             â”‚
        â”‚   (Routes to Query Nodes)   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Milvus Query Nodes (3)    â”‚
        â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
        â”‚   â”‚ Query Node 1         â”‚  â”‚
        â”‚   â”‚ Query Node 2         â”‚  â”‚
        â”‚   â”‚ Query Node 3         â”‚  â”‚
        â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Milvus Data Nodes (3)    â”‚
        â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
        â”‚   â”‚ Data Node 1          â”‚  â”‚
        â”‚   â”‚ - Vectors (10 shards) â”‚  â”‚
        â”‚   â”‚ - HNSW Index         â”‚  â”‚
        â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Object Storage (MinIO)    â”‚
        â”‚   - Vector files            â”‚
        â”‚   - Index files             â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Metadata Store (etcd)     â”‚
        â”‚   - Collection metadata     â”‚
        â”‚   - Shard locations         â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Query Flow:**
```
1. Client â†’ Load Balancer: Semantic search query
2. Load Balancer â†’ Query Node: Routes to available node
3. Query Node: Checks metadata filter (category = 'tech')
4. Query Node â†’ Data Node: Vector search on filtered subset
5. Data Node: Searches HNSW index (<10ms)
6. Data Node â†’ Query Node: Returns top-k results
7. Query Node â†’ Client: Returns results + metadata
```

#### âš–ï¸ Trade-offs Summary

| Decision | What We Gain | What We Sacrifice |
|----------|--------------|-------------------|
| **HNSW Index** | 98% accuracy, <10ms latency | Higher memory (60 GB vs 30 GB) |
| **Tiered Storage** | Cost optimization | Slightly higher latency for cold data |
| **Metadata Filtering** | 10Ã— faster queries | Requires metadata index |
| **Incremental Updates** | Real-time updates | Background index rebuild (5-10 min) |
| **Horizontal Scaling** | Handles 100M+ vectors | More complex architecture |

#### âœ… Final Summary

**Vector Database Strategy:**
- **Database:** Milvus with HNSW index
- **Storage:** 50 GB (30 GB vectors + 20 GB index)
- **Architecture:** 3 Query Nodes, 3 Data Nodes, distributed
- **Index:** HNSW (M=16, ef_search=100)
- **Updates:** Incremental (async, <1s vector update)
- **Scaling:** Horizontal (add nodes to scale)

**Performance:**
- **Query Latency:** <10ms (p95) for semantic search
- **Throughput:** 1000 QPS (3 Query Nodes Ã— 333 QPS each)
- **Accuracy:** 98% (vs brute force)
- **Update Latency:** <1 second (vector update), 5-10 min (index rebuild)

**Result:**
- âœ… Handles 10M document embeddings
- âœ… <100ms query latency (p95)
- âœ… Supports metadata filtering
- âœ… Real-time updates (<1s)
- âœ… Cost-optimized (tiered storage, open source)

