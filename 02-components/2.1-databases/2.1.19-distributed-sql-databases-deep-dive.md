# 2.1.19 Distributed SQL Databases Deep Dive: CockroachDB, TiDB, Spanner, and YugabyteDB

## Intuitive Explanation

Imagine a bank with branches worldwide. Each branch needs to access the same account data, and all transactions must be consistentâ€”if you deposit money in New York, it should immediately appear in London. But the bank also needs to handle millions of transactions per second.

**Traditional databases struggle:**
- **PostgreSQL/MySQL:** Strong consistency (ACID) but can't scale horizontally
- **Cassandra:** Scales horizontally but eventual consistency (not ACID)

**Distributed SQL databases** solve this by combining:
- **Strong Consistency:** ACID transactions across all nodes
- **Horizontal Scalability:** Add nodes to increase capacity
- **Multi-Region:** Data replicated across geographic regions
- **SQL Interface:** Standard SQL (compatible with PostgreSQL/MySQL)

**Key Insight:** Distributed SQL uses **consensus algorithms (Raft)** to ensure all nodes agree on data, enabling both consistency and scalability.

---

## In-Depth Analysis

### 1. What are Distributed SQL Databases?

**Distributed SQL Databases** are databases that provide ACID transactions and SQL interface while scaling horizontally across multiple nodes and regions.

**Key Features:**
- **ACID Transactions:** Strong consistency guarantees
- **Horizontal Scaling:** Add nodes to increase capacity
- **Automatic Sharding:** Data distributed automatically
- **Multi-Region Replication:** Data replicated across regions
- **SQL Compatibility:** Standard SQL (PostgreSQL/MySQL compatible)

**Architecture:**
```
Distributed SQL Database:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ SQL Gateway     â”‚ (Routes queries)
  â”‚ Metadata Store  â”‚ (Shard locations)
  â”‚ Raft Groups     â”‚ (Consensus per shard)
  â”‚ Storage Layer   â”‚ (Distributed storage)
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. Core Concepts

#### A. Automatic Sharding

**How It Works:**
```
Data automatically partitioned by key ranges:

Shard 1: [A-F]
  - Node 1 (Leader)
  - Node 2 (Follower)
  - Node 3 (Follower)

Shard 2: [G-M]
  - Node 4 (Leader)
  - Node 5 (Follower)
  - Node 6 (Follower)

Shard 3: [N-Z]
  - Node 7 (Leader)
  - Node 8 (Follower)
  - Node 9 (Follower)
```

**Shard Splitting:**
```
When shard grows too large:
  1. Detect hot shard (high traffic)
  2. Split shard into two ranges
  3. Redistribute data
  4. Update metadata
  5. No downtime
```

#### B. Raft Consensus

**How It Works:**
```
Raft Group (3 nodes):
  - Leader: Handles writes
  - Followers: Replicate writes
  
Write Flow:
  1. Client â†’ Leader: Write request
  2. Leader â†’ Followers: Replicate write
  3. Followers â†’ Leader: Acknowledge
  4. Leader: Commit (majority = 2/3)
  5. Leader â†’ Client: Success
```

**Consensus Guarantees:**
- **Strong Consistency:** All nodes see same data
- **Durability:** Data persisted on majority of nodes
- **Availability:** Tolerates (N/2 - 1) node failures

#### C. Multi-Region Replication

**How It Works:**
```
Region 1 (US):
  - Primary replicas (writes)
  - Follower replicas (reads)

Region 2 (EU):
  - Follower replicas (reads)
  - Async replication from US

Region 3 (APAC):
  - Follower replicas (reads)
  - Async replication from US
```

**Read/Write Patterns:**
```
Writes:
  - Always go to primary region (strong consistency)
  - Replicated to other regions (async)

Reads:
  - Can read from local region (low latency)
  - May see slightly stale data (eventual consistency for reads)
```

### 3. Major Distributed SQL Databases

#### A. CockroachDB

**Architecture:**
- **SQL Compatibility:** PostgreSQL-compatible
- **Consensus:** Raft
- **Sharding:** Automatic range-based sharding
- **Multi-Region:** Built-in multi-region support

**Key Features:**
- **Global Tables:** Replicated to all regions
- **Regional Tables:** Stored in specific region
- **Follower Reads:** Read from nearest replica
- **Survival Goals:** Zone, Region, Region survival

**Use Cases:**
- Multi-region SaaS applications
- Financial systems
- E-commerce platforms

**Deployment:**
- Self-hosted (Kubernetes, Docker)
- CockroachDB Cloud (managed)

#### B. Google Spanner

**Architecture:**
- **SQL Compatibility:** Standard SQL
- **Consensus:** TrueTime (synchronized clocks)
- **Sharding:** Automatic
- **Multi-Region:** Global distribution

**Key Features:**
- **TrueTime:** Synchronized clocks enable global consistency
- **External Consistency:** Stronger than serializability
- **99.999% Availability:** Five nines SLA
- **Automatic Failover:** <10 seconds

**Use Cases:**
- Google's internal services (Ads, Gmail)
- Global financial systems
- High-availability applications

**Deployment:**
- Google Cloud Spanner (fully managed)

#### C. TiDB

**Architecture:**
- **SQL Compatibility:** MySQL-compatible
- **Consensus:** Raft (per region)
- **Sharding:** Automatic
- **Multi-Region:** Cross-region replication

**Key Features:**
- **HTAP:** Hybrid Transactional/Analytical Processing
- **TiKV:** Distributed key-value store
- **TiFlash:** Columnar storage for analytics
- **TiDB Operator:** Kubernetes operator

**Use Cases:**
- MySQL replacement at scale
- HTAP workloads
- Real-time analytics

**Deployment:**
- Self-hosted (Kubernetes)
- TiDB Cloud (managed)

#### D. YugabyteDB

**Architecture:**
- **SQL Compatibility:** PostgreSQL-compatible
- **Consensus:** Raft
- **Sharding:** Automatic
- **Multi-Region:** Built-in replication

**Key Features:**
- **YSQL:** PostgreSQL-compatible API
- **YCQL:** Cassandra-compatible API
- **DocDB:** Distributed document store
- **Multi-Cloud:** Deploy across clouds

**Use Cases:**
- PostgreSQL replacement at scale
- Multi-cloud deployments
- Microservices

**Deployment:**
- Self-hosted (Kubernetes, Docker)
- YugabyteDB Cloud (managed)

### 4. Distributed SQL vs. Traditional Databases

| Aspect | Traditional SQL | Distributed SQL | NoSQL |
|--------|----------------|-----------------|-------|
| **Consistency** | Strong (ACID) | Strong (ACID) | Eventual (BASE) |
| **Scalability** | Vertical only | Horizontal | Horizontal |
| **Transactions** | Single node | Multi-node | Limited |
| **SQL** | Full SQL | Full SQL | Limited/No SQL |
| **Latency** | Low (<10ms) | Higher (50-200ms) | Low (<10ms) |
| **Complexity** | Low | High | Medium |

### 5. Transaction Model

#### A. Distributed Transactions

**Two-Phase Commit (2PC):**
```
Transaction spanning multiple shards:

1. Prepare Phase:
   - Coordinator â†’ All shards: "Prepare transaction"
   - Shards â†’ Coordinator: "Prepared" or "Abort"

2. Commit Phase:
   - If all prepared â†’ Coordinator: "Commit"
   - If any abort â†’ Coordinator: "Abort"
   - Shards: Commit or rollback
```

**Challenges:**
- **Blocking:** Coordinator failure blocks transaction
- **Latency:** Multiple round-trips
- **Complexity:** Coordination overhead

#### B. Serializable Isolation

**How It Works:**
```
Serializable Isolation:
  - All transactions appear to execute serially
  - No anomalies (dirty reads, phantom reads)
  - Uses timestamp ordering or locking

Example:
  Transaction 1: Read balance (100)
  Transaction 2: Read balance (100)
  Transaction 1: Write balance (150)
  Transaction 2: Write balance (120)
  
  Result: One transaction aborts (conflict)
```

### 6. Performance Characteristics

#### A. Write Latency

**Single Region:**
```
Write Latency:
  - Local write: <10ms
  - Raft consensus: +20ms (2 followers)
  - Total: <50ms
```

**Multi-Region:**
```
Write Latency:
  - Cross-region replication: +100-200ms
  - Raft consensus: +20ms
  - Total: <200ms
```

#### B. Read Latency

**Local Reads:**
```
Read from local replica:
  - Latency: <10ms
  - Consistency: Strong (from leader) or eventual (from follower)
```

**Follower Reads:**
```
Read from follower (nearest region):
  - Latency: <10ms (local)
  - Consistency: Eventual (may be slightly stale)
  - Trade-off: Lower latency vs. consistency
```

#### C. Throughput

**Scaling:**
```
Throughput scales linearly with nodes:
  - 10 nodes: 100K writes/sec
  - 20 nodes: 200K writes/sec
  - 30 nodes: 300K writes/sec
```

**Bottlenecks:**
- **Consensus Overhead:** Every write requires majority acknowledgment
- **Network:** Replication traffic (3Ã— for 3 replicas)
- **Hot Shards:** Uneven distribution

### 7. Multi-Region Strategies

#### A. Global Tables

**Strategy:** Replicate to all regions

**Benefits:**
- Low read latency (read from local region)
- High availability (survives region failure)

**Trade-offs:**
- Higher write latency (replicate to all regions)
- Higher storage cost (full replication)

#### B. Regional Tables

**Strategy:** Store in specific region

**Benefits:**
- Low write latency (write to local region)
- Lower storage cost (no full replication)

**Trade-offs:**
- Higher read latency (cross-region reads)
- Lower availability (region failure = data unavailable)

#### C. Hybrid Approach

**Strategy:** Mix of global and regional tables

**Example:**
```
Global Tables:
  - User profiles (read-heavy, low write latency OK)
  - Product catalog (read-heavy)

Regional Tables:
  - Orders (write-heavy, low write latency needed)
  - Inventory (region-specific)
```

### 8. Common Use Cases

#### A. Financial Systems

**Requirements:**
- Strong consistency (ACID)
- Multi-region (global operations)
- High availability (99.99%+)

**Solution:**
- Distributed SQL (CockroachDB, Spanner)
- Multi-region replication
- Raft consensus

#### B. E-Commerce Platforms

**Requirements:**
- Strong consistency (inventory, orders)
- Horizontal scaling (millions of products)
- Multi-region (global customers)

**Solution:**
- Distributed SQL
- Regional tables (orders per region)
- Global tables (product catalog)

#### C. SaaS Platforms

**Requirements:**
- Multi-tenant (data isolation)
- Horizontal scaling (millions of tenants)
- Strong consistency (billing, subscriptions)

**Solution:**
- Distributed SQL
- Tenant-based sharding
- Multi-region deployment

---

## When to Use Distributed SQL Databases

### âœ… Use Distributed SQL When:

1. **Strong Consistency Required:** Need ACID transactions
2. **Horizontal Scaling:** Need to scale beyond single node
3. **Multi-Region:** Need global data distribution
4. **SQL Compatibility:** Existing SQL applications
5. **Complex Queries:** Need joins, aggregations, transactions
6. **Financial/Transactional:** Critical data integrity

### âŒ Don't Use Distributed SQL When:

1. **Simple Use Case:** Single region, <1TB data (use PostgreSQL/MySQL)
2. **Eventual Consistency OK:** Can tolerate stale reads (use Cassandra)
3. **Ultra-Low Latency:** Need <5ms writes (use single-node database)
4. **Read-Only Analytics:** Need columnar storage (use ClickHouse)
5. **Cost Sensitive:** Distributed SQL is expensive (use simpler solution)

---

## Real-World Examples

### Google Spanner

**Use Case:** Google's internal services

**Scale:**
- Powers Google Ads, Gmail, Google Cloud
- Petabytes of data
- Millions of QPS
- 99.999% availability

**Features:**
- TrueTime (synchronized clocks)
- External consistency
- Global distribution

### CockroachDB

**Use Case:** Multi-region SaaS platforms

**Scale:**
- Thousands of customers
- Multi-region deployments
- Strong consistency
- PostgreSQL compatibility

### TiDB

**Use Case:** MySQL replacement at scale

**Scale:**
- Replaces MySQL clusters
- HTAP workloads
- Real-time analytics
- MySQL compatibility

---

## Distributed SQL vs. Other Solutions

| Solution | Best For | Consistency | Scalability | Complexity |
|----------|----------|-------------|-------------|------------|
| **Distributed SQL** | ACID + horizontal scale | Strong | Excellent | High |
| **PostgreSQL/MySQL** | Single region, <1TB | Strong | Vertical only | Low |
| **Cassandra** | Eventual consistency OK | Eventual | Excellent | Medium |
| **DynamoDB** | Serverless, AWS-native | Eventual | Excellent | Low |
| **MongoDB** | Document model | Strong (single doc) | Good | Medium |

---

## Common Anti-Patterns

### âŒ **1. Using Distributed SQL for Simple Use Case**

**Problem:** Over-engineering for small scale

**Solution:** Use traditional database for simple cases

```
âŒ Bad:
Single region, 100GB data â†’ CockroachDB
â†’ Unnecessary complexity, higher cost

âœ… Good:
Single region, 100GB data â†’ PostgreSQL
â†’ Simple, cost-effective
```

### âŒ **2. Ignoring Write Amplification**

**Problem:** Not accounting for replication overhead

**Solution:** Plan for 3Ã— network traffic

```
âŒ Bad:
100K writes/sec â†’ 100K network traffic
â†’ Actually needs 300K (3 replicas)

âœ… Good:
100K writes/sec â†’ 300K network traffic planned
â†’ Sufficient bandwidth provisioned
```

### âŒ **3. Hot Shards**

**Problem:** Uneven data distribution

**Solution:** Monitor and split hot shards

```
âŒ Bad:
All writes to one shard â†’ Bottleneck

âœ… Good:
Monitor shard load â†’ Auto-split hot shards â†’ Even distribution
```

---

## Trade-offs Summary

| Aspect | What You Gain | What You Sacrifice |
|--------|---------------|-------------------|
| **Strong Consistency** | ACID guarantees | Higher latency (consensus) |
| **Horizontal Scaling** | Add nodes to scale | Write amplification (replication) |
| **Multi-Region** | Global distribution | Cross-region latency (100-200ms) |
| **SQL Compatibility** | Existing applications work | Complexity (distributed transactions) |
| **Automatic Sharding** | No manual partitioning | Less control over data placement |

---

## References

- **CockroachDB Documentation:** [https://www.cockroachlabs.com/docs/](https://www.cockroachlabs.com/docs/)
- **Google Spanner:** [https://cloud.google.com/spanner/docs](https://cloud.google.com/spanner/docs)
- **TiDB Documentation:** [https://docs.pingcap.com/tidb/](https://docs.pingcap.com/tidb/)
- **YugabyteDB Documentation:** [https://docs.yugabyte.com/](https://docs.yugabyte.com/)
- **Related Chapters:**
  - [2.1.1 RDBMS Deep Dive](./2.1.1-rdbms-deep-dive.md) - SQL databases overview
  - [2.1.9 Cassandra Deep Dive](./2.1.9-cassandra-deep-dive.md) - NoSQL alternative
  - [3.3.4 Distributed Database Challenge](../../../03-challenges/3.3.4-distributed-database/README.md) - Design challenge

---

## âœï¸ Design Challenge

### Problem

You are designing a multi-region financial platform that must:

1. **Handle 1M transactions per day** (financial transactions)
2. **Strong consistency** (ACID transactions across regions)
3. **Multi-region** (US, EU, APAC - 3 regions)
4. **99.99% availability** (survive region failures)
5. **Low latency** (<100ms for reads, <200ms for writes)
6. **SQL compatibility** (existing PostgreSQL applications)

**Constraints:**
- Financial data (must be ACID)
- Global users (need multi-region)
- Existing PostgreSQL codebase
- Regulatory compliance (data residency)

Design a distributed SQL database strategy that:
- Handles transaction volume
- Ensures strong consistency
- Supports multi-region
- Provides high availability
- Meets latency requirements
- Maintains SQL compatibility

### Solution

#### ğŸ§© Scenario

- **Transactions:** 1M per day = 11.6 transactions/second average, 100/second peak
- **Regions:** 3 (US, EU, APAC)
- **Availability:** 99.99% (4 nines)
- **Latency:** <100ms reads, <200ms writes

**Calculations:**
- **Peak QPS:** 100 transactions/second
- **Data per Transaction:** ~1 KB
- **Daily Data:** 1M Ã— 1 KB = 1 GB/day
- **Annual Data:** ~365 GB/year
- **Replication:** 3Ã— (3 regions) = ~1.1 TB total

#### âœ… Step 1: Database Choice

**Choice: CockroachDB**

**Why:**
- **PostgreSQL Compatible:** Existing code works without changes
- **Multi-Region:** Built-in multi-region support
- **Strong Consistency:** ACID transactions
- **Automatic Sharding:** No manual partitioning
- **Survival Goals:** Region survival (survive region failure)

#### âœ… Step 2: Multi-Region Architecture

**Region Configuration:**
```
US Region (Primary):
  - Primary replicas (writes)
  - Follower replicas (reads)
  - 3 nodes (Raft group)

EU Region:
  - Follower replicas (reads)
  - Async replication from US
  - 3 nodes

APAC Region:
  - Follower replicas (reads)
  - Async replication from US
  - 3 nodes
```

**Survival Goal: Region Survival**
```
Configuration:
  - Survive failure of 1 region
  - Data replicated to all 3 regions
  - Automatic failover if primary region fails
```

#### âœ… Step 3: Table Placement

**Global Tables (Replicated to All Regions):**
```
User Accounts:
  - Replicated to all 3 regions
  - Read from local region (low latency)
  - Write to primary region (US)

Product Catalog:
  - Replicated to all 3 regions
  - Read-heavy (low read latency)
  - Write latency OK (infrequent updates)
```

**Regional Tables (Stored in Specific Region):**
```
Transactions:
  - Stored in user's region
  - Low write latency (local writes)
  - Read from local region

Regulatory Data:
  - Stored in compliance region
  - Data residency requirements
```

#### âœ… Step 4: Transaction Handling

**Distributed Transactions:**
```
Transaction spanning multiple shards:

1. Begin Transaction
2. Write to Shard 1 (US)
3. Write to Shard 2 (EU)
4. Commit (2PC across shards)
5. Replicate to all regions
```

**Consistency Level:**
```
Serializable Isolation:
  - All transactions appear serial
  - No anomalies
  - Uses timestamp ordering
```

#### âœ… Step 5: Read/Write Patterns

**Write Pattern:**
```
Writes:
  - Always go to primary region (US)
  - Raft consensus (3 nodes)
  - Replicate to other regions (async)
  - Latency: <200ms (meets requirement)
```

**Read Pattern:**
```
Reads:
  - Follower reads (local region)
  - Low latency: <100ms (meets requirement)
  - May see slightly stale data (acceptable for reads)
  
Critical Reads:
  - Read from leader (primary region)
  - Strong consistency
  - Higher latency: <200ms
```

#### âœ… Step 6: High Availability

**Region Failure Handling:**
```
If US Region Fails:
  1. Detect failure (<30 seconds)
  2. Promote EU or APAC to primary
  3. Update routing (DNS/load balancer)
  4. Continue operations
  5. Failover time: <2 minutes
```

**Node Failure Handling:**
```
If Node Fails:
  1. Raft group continues (2/3 majority)
  2. Replace failed node
  3. Replicate data to new node
  4. No downtime
```

#### âœ… Step 7: Performance Optimization

**Connection Pooling:**
```
Per-Region Connection Pools:
  - US: 100 connections
  - EU: 100 connections
  - APAC: 100 connections
  
Benefits:
  - Reuse connections
  - Lower latency
  - Better throughput
```

**Query Optimization:**
```
Indexes:
  - Primary key indexes (automatic)
  - Secondary indexes (user-defined)
  - Covering indexes (reduce lookups)

Query Planning:
  - Distributed query planner
  - Parallel execution across shards
  - Result aggregation
```

#### âœ… Complete Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Applications (PostgreSQL Clients)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   CockroachDB Gateway        â”‚
        â”‚   (SQL Router)               â”‚
        â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
        â”‚   â”‚ Query Planner        â”‚  â”‚
        â”‚   â”‚ Metadata Store       â”‚  â”‚
        â”‚   â”‚ Shard Router         â”‚  â”‚
        â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                  â”‚                  â”‚
    â–¼                  â–¼                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ US      â”‚      â”‚ EU      â”‚      â”‚ APAC    â”‚
â”‚ Region  â”‚      â”‚ Region  â”‚      â”‚ Region  â”‚
â”‚         â”‚      â”‚         â”‚      â”‚         â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â” â”‚      â”‚ â”Œâ”€â”€â”€â”€â”€â” â”‚      â”‚ â”Œâ”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚Node1â”‚ â”‚      â”‚ â”‚Node1â”‚ â”‚      â”‚ â”‚Node1â”‚ â”‚
â”‚ â”‚Node2â”‚ â”‚      â”‚ â”‚Node2â”‚ â”‚      â”‚ â”‚Node2â”‚ â”‚
â”‚ â”‚Node3â”‚ â”‚      â”‚ â”‚Node3â”‚ â”‚      â”‚ â”‚Node3â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”˜ â”‚      â”‚ â””â”€â”€â”€â”€â”€â”˜ â”‚      â”‚ â””â”€â”€â”€â”€â”€â”˜ â”‚
â”‚         â”‚      â”‚         â”‚      â”‚         â”‚
â”‚ Raft    â”‚      â”‚ Raft    â”‚      â”‚ Raft    â”‚
â”‚ Groups  â”‚      â”‚ Groups  â”‚      â”‚ Groups  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Transaction Flow:**
```
1. Client â†’ Gateway: BEGIN TRANSACTION
2. Gateway: Routes to appropriate shards
3. Shard Leaders: Execute writes (Raft consensus)
4. Shard Leaders â†’ Followers: Replicate writes
5. Followers â†’ Leaders: Acknowledge
6. Leaders: Commit (majority = 2/3)
7. Leaders â†’ Gateway: Success
8. Gateway â†’ Client: COMMIT success
9. Async: Replicate to other regions
```

#### âš–ï¸ Trade-offs Summary

| Decision | What We Gain | What We Sacrifice |
|----------|--------------|-------------------|
| **CockroachDB** | PostgreSQL compatibility, multi-region | Higher cost than single-node DB |
| **Multi-Region Replication** | High availability, low read latency | Higher write latency (cross-region) |
| **Global Tables** | Low read latency | Higher write latency, storage cost |
| **Regional Tables** | Low write latency | Higher read latency (cross-region) |
| **Follower Reads** | Low read latency | Eventual consistency (slightly stale) |

#### âœ… Final Summary

**Distributed SQL Strategy:**
- **Database:** CockroachDB (PostgreSQL-compatible)
- **Regions:** 3 (US primary, EU/APAC followers)
- **Replication:** 3Ã— (all regions)
- **Tables:** Global (accounts, catalog) + Regional (transactions)
- **Consistency:** Serializable isolation (ACID)
- **Availability:** 99.99% (region survival)

**Performance:**
- **Write Latency:** <200ms (cross-region replication)
- **Read Latency:** <100ms (follower reads from local region)
- **Throughput:** 100 transactions/second (handles peak)
- **Availability:** 99.99% (survives region failure)

**Result:**
- âœ… Handles 1M transactions/day
- âœ… Strong consistency (ACID)
- âœ… Multi-region (3 regions)
- âœ… High availability (99.99%)
- âœ… Low latency (<100ms reads, <200ms writes)
- âœ… SQL compatibility (PostgreSQL)

