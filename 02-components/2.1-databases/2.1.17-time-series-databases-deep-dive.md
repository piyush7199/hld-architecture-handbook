# 2.1.17 Time Series Databases Deep Dive: Optimized for Temporal Data

## Intuitive Explanation

Imagine you're monitoring 1 million servers, each sending CPU, memory, and disk metrics every second. That's 3 million
data points per second. A traditional database would struggle with this write load and be slow for queries like "What
was the average CPU usage across all servers between 2 PM and 4 PM yesterday?"

**Time Series Databases (TSDB)** are specialized databases designed for exactly this: storing and querying data points
that change over time. They're optimized for:

- **High-Volume Writes:** Millions of data points per second (append-only)
- **Time-Range Queries:** "Give me all CPU metrics from last hour" (extremely fast)
- **Compression:** Store years of data efficiently (10:1 compression ratio)
- **Aggregations:** Calculate averages, percentiles, max/min over time ranges

**Key Insight:** Time series data is append-only (rarely updated) and queried by time ranges, not by individual records.
TSDBs exploit this pattern for massive performance gains.

---

## In-Depth Analysis

### 1. What is Time Series Data?

**Time Series Data** is a sequence of data points collected over time intervals.

**Characteristics:**

- **Timestamp:** Every data point has a timestamp
- **Metrics/Tags:** Data point has a value and optional tags (server_id, region)
- **Append-Only:** New data points added, old ones rarely updated
- **High Volume:** Millions of data points per second

**Example:**

```
Timestamp: 2024-01-15 14:30:00
Metric: cpu_usage
Tags: {server: web-01, region: us-east}
Value: 75.5
```

### 2. Why Not Use Traditional Databases?

**Problem with RDBMS:**

```
Table: metrics
Columns: timestamp, server_id, cpu_usage, memory_usage

Query: "Average CPU usage last hour"
â†’ Scans millions of rows
â†’ Slow (seconds to minutes)
â†’ High I/O (random disk access)
```

**Problem with NoSQL:**

```
Key: server-01:2024-01-15:14:30:00
Value: {cpu: 75.5, memory: 60.2}

Query: "All metrics last hour"
â†’ Must query millions of keys
â†’ Slow (key lookups are O(log N))
â†’ No time-range optimization
```

**TSDB Solution:**

- **Sequential Storage:** Data stored by time (sequential disk access)
- **Compression:** Similar values compressed (10:1 ratio)
- **Time Indexing:** Optimized for time-range queries (O(log N))
- **Aggregations:** Built-in functions (avg, sum, max, min)

### 3. TSDB Architecture

#### Storage Model

**Traditional Database:**

```
Row-based storage:
Row 1: timestamp=14:30, server=web-01, cpu=75.5
Row 2: timestamp=14:30, server=web-02, cpu=80.2
Row 3: timestamp=14:31, server=web-01, cpu=76.1
â†’ Random disk access (slow)
```

**Time Series Database:**

```
Column-based storage (by time):
Time:     [14:30, 14:31, 14:32, ...]
Server-01: [75.5,  76.1,  77.2,  ...]
Server-02: [80.2,  81.5,  82.1,  ...]
â†’ Sequential disk access (fast)
â†’ Compression (similar values)
```

#### Compression Algorithms

**A. Delta Encoding:**

```
Original: [100, 101, 102, 103, 104]
Delta:    [100, +1,  +1,  +1,  +1]
â†’ Smaller values â†’ Better compression
```

**B. Gorilla Compression (Facebook):**

- Compresses timestamps (delta of deltas)
- Compresses values (XOR with previous value)
- 10:1 compression ratio typical

**C. Run-Length Encoding:**

```
Original: [50, 50, 50, 50, 51, 51, 51]
Compressed: [50Ã—4, 51Ã—3]
â†’ 7 values â†’ 2 values
```

### 4. Major Time Series Databases

#### InfluxDB

**Architecture:**

- **Time Series Engine:** Optimized for time-series data
- **Tags and Fields:** Tags (indexed), Fields (not indexed)
- **Retention Policies:** Automatic data expiration
- **Continuous Queries:** Pre-compute aggregations

**Data Model:**

```
Measurement: cpu_usage
Tags: server=web-01, region=us-east (indexed)
Fields: usage=75.5, temperature=60.2 (not indexed)
Timestamp: 2024-01-15T14:30:00Z
```

**Query:**

```sql
SELECT mean(usage) 
FROM cpu_usage 
WHERE time >= now() - 1h 
  AND server = 'web-01'
GROUP BY time(5m)
```

**Use Cases:**

- IoT sensor data
- Application metrics
- Infrastructure monitoring

#### TimescaleDB

**Architecture:**

- **PostgreSQL Extension:** Built on PostgreSQL
- **Hypertables:** Automatically partitions by time
- **SQL Interface:** Standard SQL queries
- **Compression:** Automatic compression of old data

**Data Model:**

```sql
CREATE TABLE metrics (
  time TIMESTAMPTZ NOT NULL,
  server_id TEXT,
  cpu_usage DOUBLE PRECISION
);

-- Convert to hypertable
SELECT create_hypertable('metrics', 'time');
```

**Query:**

```sql
SELECT time_bucket('5 minutes', time) AS bucket,
       avg(cpu_usage) AS avg_cpu
FROM metrics
WHERE time >= now() - INTERVAL '1 hour'
GROUP BY bucket;
```

**Use Cases:**

- When you need SQL compatibility
- Complex queries (joins with other tables)
- PostgreSQL ecosystem integration

#### Prometheus

**Architecture:**

- **Pull Model:** Scrapes metrics from endpoints
- **Time Series Model:** Metric name + labels = time series
- **PromQL:** Specialized query language
- **Local Storage:** Stores data locally (not distributed by default)

**Data Model:**

```
Metric: http_requests_total
Labels: {method="GET", status="200", endpoint="/api/users"}
Value: 12345
Timestamp: 1699123456
```

**Query (PromQL):**

```
rate(http_requests_total[5m])
â†’ Calculate requests per second over 5 minutes
```

**Use Cases:**

- Kubernetes monitoring
- Application metrics
- Alerting (Prometheus + Alertmanager)

### 5. Key Features

#### A. Retention Policies

**Problem:** Time series data grows indefinitely

**Solution:** Automatic data expiration

**InfluxDB:**

```
CREATE RETENTION POLICY "1year" 
ON "metrics" 
DURATION 365d 
REPLICATION 1;
```

**TimescaleDB:**

```sql
-- Automatic data retention
SELECT add_retention_policy('metrics', INTERVAL '1 year');
```

**Prometheus:**

```yaml
# prometheus.yml
storage:
  retention: 15d  # Keep 15 days
```

#### B. Downsampling

**Problem:** Storing 1-second data for years is expensive

**Solution:** Downsample old data (keep averages)

**Strategy:**

```
Raw data (1 second): Keep for 7 days
5-minute averages: Keep for 30 days
1-hour averages: Keep for 1 year
1-day averages: Keep forever
```

**Implementation:**

```
Continuous Query (InfluxDB):
CREATE CONTINUOUS QUERY "downsample_5m"
ON "metrics"
BEGIN
  SELECT mean(value) INTO "metrics_5m"
  FROM "metrics"
  GROUP BY time(5m)
END
```

#### C. Aggregations

**Built-in Functions:**

- **mean():** Average value
- **sum():** Sum of values
- **max():** Maximum value
- **min():** Minimum value
- **percentile():** 95th percentile
- **rate():** Rate of change per second

**Example:**

```sql
-- Average CPU usage per server, last hour
SELECT mean(cpu_usage) 
FROM metrics 
WHERE time >= now() - 1h 
GROUP BY server_id;
```

### 6. Write Optimization

#### Append-Only Writes

**Traditional Database:**

```
UPDATE metrics SET cpu_usage = 76.5 WHERE id = 123;
â†’ Random disk write (slow)
â†’ May cause fragmentation
```

**Time Series Database:**

```
INSERT INTO metrics VALUES (now(), 'web-01', 76.5);
â†’ Append to end of file (fast)
â†’ Sequential write (optimal)
â†’ No fragmentation
```

#### Batching

**Strategy:**

- Buffer writes in memory
- Flush to disk in batches (1 MB chunks)
- Reduces I/O operations

**Example:**

```
1000 writes â†’ Buffer in memory â†’ 1 disk write
â†’ 1000Ã— fewer I/O operations
```

#### LSM Trees

**How It Works:**

- Writes go to in-memory buffer (memtable)
- When full, flush to disk (SSTable)
- Multiple SSTables merged periodically (compaction)

**Benefits:**

- Fast writes (in-memory)
- Sequential disk writes (SSTables)
- Automatic compaction (maintains performance)

### 7. Query Optimization

#### Time-Range Indexing

**B-Tree Index (Traditional):**

```
Index on timestamp:
â†’ O(log N) lookup per timestamp
â†’ Range query: O(N) (must scan all rows)
```

**Time Series Index:**

```
Time-partitioned storage:
â†’ O(1) to find partition
â†’ O(log M) to find data in partition (M << N)
â†’ Range query: O(log N) (only scan relevant partitions)
```

#### Columnar Storage

**Row Storage (Traditional):**

```
Row: [timestamp, server_id, cpu, memory, disk]
â†’ Must read all columns even if querying one
```

**Column Storage (Time Series):**

```
Column 1: [timestamp1, timestamp2, ...]
Column 2: [cpu1, cpu2, ...]
Column 3: [memory1, memory2, ...]
â†’ Only read columns needed for query
â†’ Better compression (similar values in column)
```

---

## When to Use Time Series Databases

### âœ… Use Time Series Databases When:

1. **High-Volume Metrics:** Millions of data points per second (IoT, monitoring)
2. **Time-Range Queries:** "Show me data from last hour/day/week"
3. **Aggregations:** Calculate averages, percentiles over time
4. **Long Retention:** Need to store years of historical data
5. **Append-Only Data:** Data rarely updated (mostly new data points)

### âŒ Don't Use Time Series Databases When:

1. **Frequent Updates:** Data updated many times (use traditional database)
2. **Complex Queries:** Joins, transactions (use SQL database)
3. **Low Volume:** Small amount of data (overhead not worth it)
4. **Non-Temporal Data:** Data not time-based (use appropriate database)

---

## Real-World Examples

### Prometheus (Kubernetes Monitoring)

**Use Case:** Container and application metrics

**Architecture:**

- Prometheus scrapes metrics from pods
- Stores metrics locally
- Grafana visualizes metrics
- Alertmanager sends alerts

**Scale:**

- Millions of time series
- Thousands of metrics per second
- 15-day retention (configurable)

### InfluxDB (IoT Platform)

**Use Case:** Sensor data from millions of devices

**Architecture:**

- Devices send metrics to InfluxDB
- High write throughput (millions/sec)
- Long retention (years)
- Downsampling for cost optimization

**Scale:**

- 10 million devices
- 100 million data points/second
- Petabytes of storage

### TimescaleDB (Financial Data)

**Use Case:** Stock market tick data

**Architecture:**

- PostgreSQL with TimescaleDB extension
- SQL queries for analytics
- Compression for historical data
- Real-time queries for current data

**Scale:**

- Billions of ticks per day
- Years of historical data
- Sub-second query latency

---

## Time Series DB vs. Other Databases

| Aspect                 | Time Series DB       | RDBMS                | NoSQL                           |
|------------------------|----------------------|----------------------|---------------------------------|
| **Write Throughput**   | â­â­â­â­â­ (millions/sec) | â­â­ (thousands/sec)   | â­â­â­ (hundreds of thousands/sec) |
| **Time-Range Queries** | â­â­â­â­â­ (optimized)    | â­â­ (slow, full scan) | â­â­ (slow, key lookups)          |
| **Compression**        | â­â­â­â­â­ (10:1 typical) | â­â­ (limited)         | â­â­ (limited)                    |
| **Aggregations**       | â­â­â­â­â­ (built-in)     | â­â­â­ (SQL functions)  | â­â­ (limited)                    |
| **Complex Queries**    | â­â­ (time-focused)    | â­â­â­â­â­ (SQL, joins)   | â­â­â­ (limited)                   |
| **Updates**            | â­ (append-only)      | â­â­â­â­â­ (full CRUD)    | â­â­â­â­â­ (full CRUD)               |

---

## Common Anti-Patterns

### âŒ **1. Using RDBMS for Time Series Data**

**Problem:** Storing metrics in PostgreSQL/MySQL

**Why It's Wrong:**

- Slow writes (random disk access)
- Slow time-range queries (full table scans)
- Poor compression (row-based storage)
- High storage cost

**Solution:** Use TimescaleDB (PostgreSQL extension) or InfluxDB

### âŒ **2. Storing All Data at Full Resolution**

**Problem:** Keeping 1-second data for years

**Why It's Wrong:**

- Expensive storage (petabytes)
- Slow queries (too much data)
- Unnecessary detail (1-second data not needed after 7 days)

**Solution:** Downsample old data (keep averages)

```
âŒ Bad:
1-second data: 1 year = 31.5M data points/server
Cost: 1M servers Ã— 31.5M Ã— 8 bytes = 252 PB

âœ… Good:
1-second data: 7 days
5-minute data: 30 days
1-hour data: 1 year
Cost: 1M servers Ã— (604K + 8.7K + 8.7K) Ã— 8 bytes = 5 PB
Savings: 98% reduction
```

### âŒ **3. No Retention Policy**

**Problem:** Data grows indefinitely

**Solution:** Set retention policies

```
âŒ Bad:
Data grows forever â†’ Storage costs explode
â†’ Queries slow down (too much data)

âœ… Good:
Retention: 1 year
â†’ Automatic deletion of old data
â†’ Predictable costs
â†’ Fast queries
```

---

## Trade-offs Summary

| Aspect                    | What You Gain             | What You Sacrifice                    |
|---------------------------|---------------------------|---------------------------------------|
| **High Write Throughput** | Millions of writes/second | Limited update capabilities           |
| **Time-Range Queries**    | Sub-second queries        | Poor performance for non-time queries |
| **Compression**           | 10:1 storage reduction    | CPU overhead for compression          |
| **Aggregations**          | Built-in functions        | Limited to time-based aggregations    |
| **Append-Only**           | Fast writes               | No updates (must insert new value)    |

---

## References

- **InfluxDB Documentation:** [https://docs.influxdata.com/](https://docs.influxdata.com/)
- **TimescaleDB Documentation:** [https://docs.timescale.com/](https://docs.timescale.com/)
- **Prometheus Documentation:** [https://prometheus.io/docs/](https://prometheus.io/docs/)
- **Related Chapters:**
    - [2.1.3 Specialized Databases](./2.1.3-specialized-databases.md) - Time series overview
    - [2.4.2 Observability](../2.4-security-observability/2.4.2-observability.md) - Monitoring with time series

---

## âœï¸ Design Challenge

### Problem

You are designing an IoT platform that collects sensor data from 10 million devices. Each device sends 10 metrics every
minute (temperature, humidity, pressure, etc.). The platform must:

1. **Ingest 100 million data points per minute** (10M devices Ã— 10 metrics)
2. **Store 2 years of historical data** (for trend analysis)
3. **Support time-range queries** ("Average temperature last 24 hours")
4. **Minimize storage costs** (primary concern)
5. **Query latency <1 second** (for dashboards)

**Constraints:**

- Each data point: 8 bytes (timestamp + value)
- Storage cost: $0.023/GB/month
- Query pattern: 80% time-range queries, 20% aggregations
- Devices send data every minute (no batching)

Design a time series database strategy that:

- Handles write throughput
- Minimizes storage costs
- Meets query latency requirements
- Supports 2-year retention

### Solution

#### ðŸ§© Scenario

- **Devices:** 10 million
- **Metrics per device:** 10
- **Write Rate:** 10M Ã— 10 = 100M data points/minute = 1.67M/second
- **Retention:** 2 years
- **Storage Cost:** Primary concern

**Calculations:**

- **Raw Data (2 years):** 10M devices Ã— 10 metrics Ã— 525,600 minutes/year Ã— 2 years Ã— 8 bytes = 840 PB
- **Cost:** 840 PB Ã— $0.023/GB/month = $19.3B/month (unacceptable!)

#### âœ… Step 1: Database Choice

**Choice: InfluxDB with Compression**

**Why:**

- **High Write Throughput:** Handles millions of writes/second
- **Compression:** 10:1 compression ratio (Gorilla algorithm)
- **Retention Policies:** Automatic data expiration
- **Downsampling:** Built-in continuous queries

#### âœ… Step 2: Compression Strategy

**Raw Data:**

```
10M devices Ã— 10 metrics Ã— 8 bytes = 800 MB/minute
= 1.15 TB/day
= 840 TB/year (raw)
```

**With Compression (10:1):**

```
Compressed: 840 TB Ã· 10 = 84 TB/year
2 years: 168 TB
Cost: 168 TB Ã— $0.023/GB/month = $3.86M/month
```

**Savings:** 95% reduction ($19.3B â†’ $3.86M)

#### âœ… Step 3: Downsampling Strategy

**Problem:** 1-minute data not needed after 7 days

**Solution: Multi-Tier Retention**

```
Tier 1: Raw Data (1-minute resolution)
  - Retention: 7 days
  - Storage: 7 days Ã— 1.15 TB/day = 8.05 TB
  - Use: Real-time dashboards, alerts

Tier 2: 5-Minute Averages
  - Retention: 30 days
  - Storage: 30 days Ã— 0.23 TB/day = 6.9 TB
  - Use: Hourly/daily trends

Tier 3: 1-Hour Averages
  - Retention: 1 year
  - Storage: 365 days Ã— 0.019 TB/day = 6.9 TB
  - Use: Monthly/yearly trends

Tier 4: 1-Day Averages
  - Retention: 1 year (2nd year)
  - Storage: 365 days Ã— 0.0008 TB/day = 0.29 TB
  - Use: Long-term trends
```

**Total Storage:**

```
Raw (7 days): 8.05 TB
5-min (30 days): 6.9 TB
1-hour (1 year): 6.9 TB
1-day (1 year): 0.29 TB
Total: 22.14 TB (compressed)
Cost: 22.14 TB Ã— $0.023/GB/month = $509K/month
```

**Additional Savings:** 87% reduction ($3.86M â†’ $509K)

#### âœ… Step 4: Write Optimization

**Batching Strategy:**

```
Devices â†’ Message Queue (Kafka) â†’ InfluxDB

Flow:
1. Devices send metrics â†’ Kafka (buffers)
2. Kafka batches: 10,000 data points
3. InfluxDB writes batch (single write operation)
4. Result: 1.67M writes/sec â†’ 167 writes/sec (10,000Ã— reduction)
```

**Partitioning:**

```
Kafka Topic: sensor-metrics
Partitions: 100 (by device_id hash)
â†’ Distributes load across InfluxDB instances
```

#### âœ… Step 5: Query Optimization

**Indexing Strategy:**

```
Tags (indexed):
  - device_id
  - metric_name
  - region

Fields (not indexed):
  - value
  - timestamp (time index)
```

**Query Pattern:**

```
80% queries: Time-range (last 24 hours)
  â†’ Use time index (fast, O(log N))
  
20% queries: Aggregations (average, max, min)
  â†’ Use continuous queries (pre-computed)
```

**Continuous Queries:**

```
Pre-compute aggregations:
  - 5-minute averages (every 5 minutes)
  - 1-hour averages (every hour)
  - 1-day averages (every day)

Result: Queries hit pre-computed data (fast)
```

#### âœ… Step 6: Retention Policies

**InfluxDB Configuration:**

```
Database: iot_metrics

Retention Policy 1: raw_data
  Duration: 7 days
  Replication: 3

Retention Policy 2: downsampled_5m
  Duration: 30 days
  Replication: 3

Retention Policy 3: downsampled_1h
  Duration: 365 days
  Replication: 3

Retention Policy 4: downsampled_1d
  Duration: 365 days
  Replication: 3
```

**Automatic Expiration:**

```
InfluxDB automatically deletes data older than retention
â†’ No manual cleanup needed
â†’ Predictable storage costs
```

#### âœ… Complete Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          10 Million IoT Devices                          â”‚
â”‚          (10 metrics each, every minute)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Kafka (Message Queue)                      â”‚
â”‚          Partitions: 100 (by device_id)                  â”‚
â”‚          Batching: 10,000 data points                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚              â”‚              â”‚
        â–¼              â–¼              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ InfluxDB     â”‚ â”‚ InfluxDB â”‚ â”‚ InfluxDB     â”‚
â”‚ Instance 1   â”‚ â”‚ Instance 2â”‚ â”‚ Instance 3   â”‚
â”‚              â”‚ â”‚          â”‚ â”‚              â”‚
â”‚ Write:       â”‚ â”‚ Write:   â”‚ â”‚ Write:       â”‚
â”‚ 33M/min      â”‚ â”‚ 33M/min  â”‚ â”‚ 34M/min      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚               â”‚              â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚              â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚      Storage Tiers                 â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
    â”‚  â”‚ Raw (1-min): 7 days          â”‚  â”‚
    â”‚  â”‚ 5-min avg: 30 days           â”‚  â”‚
    â”‚  â”‚ 1-hour avg: 1 year           â”‚  â”‚
    â”‚  â”‚ 1-day avg: 1 year            â”‚  â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Query Flow:**

```
1. User queries: "Average temperature last 24 hours"
2. Query Service â†’ InfluxDB
3. InfluxDB checks: Raw data (1-minute) available for 24h
4. InfluxDB aggregates: Calculates average
5. Response: <1 second (time-indexed query)
```

#### âš–ï¸ Trade-offs Summary

| Decision               | What We Gain            | What We Sacrifice                    |
|------------------------|-------------------------|--------------------------------------|
| **Downsampling**       | 87% storage reduction   | Lose 1-minute detail after 7 days    |
| **Compression**        | 95% storage reduction   | CPU overhead for compression         |
| **Batching**           | 10,000Ã— write reduction | Slight latency (seconds)             |
| **Multi-Tier Storage** | Cost optimization       | Query complexity (which tier to use) |

#### âœ… Final Summary

**Time Series Database Strategy:**

- **Database:** InfluxDB (high write throughput, compression)
- **Storage:** Multi-tier (raw â†’ 5-min â†’ 1-hour â†’ 1-day)
- **Compression:** 10:1 ratio (Gorilla algorithm)
- **Retention:** 2 years (with downsampling)
- **Cost:** $509K/month (vs $19.3B/month raw, 99.997% reduction)

**Performance:**

- **Write Throughput:** 1.67M data points/second (handled by Kafka + InfluxDB)
- **Query Latency:** <1 second (time-indexed queries, pre-computed aggregations)
- **Storage:** 22.14 TB (compressed, downsampled)

**Result:**

- âœ… Handles 100M data points/minute
- âœ… 2-year retention (with downsampling)
- âœ… <1 second query latency
- âœ… 99.997% storage cost reduction
- âœ… Supports time-range queries and aggregations

