# 2.1.10 MongoDB Deep Dive: The Document Database

## Intuitive Explanation

MongoDB is a **document-oriented NoSQL database** that stores data in flexible, JSON-like documents instead of rigid tables. Think of it as a filing cabinet where each folder (document) can have different contents — one folder might have 5 papers, another might have 10 completely different papers. This flexibility makes MongoDB perfect for rapidly evolving applications where the data structure changes frequently.

- **Document Model:** Stores data as JSON-like BSON documents (Binary JSON).
- **Schema Flexibility:** No rigid schema — documents in the same collection can have different fields.
- **Horizontal Scaling:** Built-in sharding for distributing data across multiple servers.
- **Use Cases:** Content management, mobile apps, real-time analytics, product catalogs.

---

## In-Depth Analysis

### 1. Document Model: BSON

MongoDB stores data as **BSON** (Binary JSON) documents:

**Example Document:**

```json
{
  "_id": ObjectId("507f1f77bcf86cd799439011"),
  "name": "John Doe",
  "email": "john@example.com",
  "age": 30,
  "address": {
    "street": "123 Main St",
    "city": "New York",
    "zipcode": "10001"
  },
  "tags": ["developer", "mongodb", "nodejs"],
  "created_at": ISODate("2024-01-15T10:30:00Z")
}
```

**BSON vs. JSON:**

| Feature | JSON | BSON |
|---------|------|------|
| **Format** | Text-based | Binary |
| **Size** | Larger (human-readable) | Smaller (compressed) |
| **Parsing** | Slower | Faster |
| **Data Types** | Limited (string, number, boolean, null) | Extended (Date, ObjectId, Binary, Decimal128) |
| **Use Case** | APIs, config files | MongoDB internal storage |

**ObjectId Structure:**

```
ObjectId: 12-byte unique identifier
┌─────────────┬───────────┬─────────┬───────────┐
│  Timestamp  │ Machine ID│ Process │  Counter  │
│   4 bytes   │  3 bytes  │ 2 bytes │  3 bytes  │
└─────────────┴───────────┴─────────┴───────────┘

Benefits:
- Globally unique (no central ID generator needed)
- Sortable by creation time
- Includes machine/process info (distributed-friendly)
```

---

### 2. Data Modeling: Embedded vs. Referenced

MongoDB supports two main modeling patterns:

#### **2.1 Embedded Documents (Denormalization)**

**Pattern:** Store related data in a single document.

```json
{
  "_id": ObjectId("..."),
  "user": "john@example.com",
  "address": {
    "street": "123 Main St",
    "city": "New York"
  },
  "orders": [
    { "order_id": 1, "total": 99.99, "items": ["laptop"] },
    { "order_id": 2, "total": 29.99, "items": ["mouse"] }
  ]
}
```

**Benefits:**

- ✅ **Single query:** Fetch user + address + orders in one operation.
- ✅ **Atomicity:** Updates to embedded documents are atomic.
- ✅ **Performance:** No joins (everything in one document).

**Trade-offs:**

- ❌ **Document size limit:** 16MB max per document.
- ❌ **Data duplication:** If address changes, must update all documents.
- ❌ **Unbounded arrays:** If user has 10,000 orders, document becomes huge.

**When to Use:**

- One-to-few relationships (user → addresses).
- Data that is always queried together.
- Read-heavy workloads.

#### **2.2 Referenced Documents (Normalization)**

**Pattern:** Store related data in separate collections (like SQL foreign keys).

```json
// Users collection
{
  "_id": ObjectId("user123"),
  "name": "John Doe",
  "email": "john@example.com"
}

// Orders collection
{
  "_id": ObjectId("order456"),
  "user_id": ObjectId("user123"),  // Reference
  "total": 99.99,
  "items": ["laptop"]
}
```

**Benefits:**

- ✅ **No duplication:** User data stored once.
- ✅ **Unbounded relationships:** User can have millions of orders.
- ✅ **Smaller documents:** Each document is focused.

**Trade-offs:**

- ❌ **Multiple queries:** Need to fetch user, then orders (or use `$lookup`).
- ❌ **No ACID across collections:** Updates to user + orders not atomic (until MongoDB 4.0+ multi-document transactions).

**When to Use:**

- One-to-many or many-to-many relationships.
- Data that is rarely queried together.
- Write-heavy workloads (avoid duplication).

**Modeling Decision Matrix:**

| Relationship | Recommended Pattern | Rationale |
|--------------|---------------------|-----------|
| **One-to-One** (User → Profile) | Embed | Always queried together |
| **One-to-Few** (User → 3 Addresses) | Embed | Small, bounded array |
| **One-to-Many** (User → 1000 Orders) | Reference | Unbounded, grows over time |
| **Many-to-Many** (Students ↔ Courses) | Reference | Complex relationships |

---

### 3. Indexing Strategies

MongoDB supports multiple index types:

| Index Type | Use Case | Performance | Example |
|------------|----------|-------------|---------|
| **Single Field** | Query on one field | $\text{O}(\log n)$ | `db.users.createIndex({ email: 1 })` |
| **Compound** | Query on multiple fields | $\text{O}(\log n)$ | `db.orders.createIndex({ user_id: 1, created_at: -1 })` |
| **Multikey** | Arrays (tags, categories) | $\text{O}(\log n)$ per element | `db.products.createIndex({ tags: 1 })` |
| **Text** | Full-text search | Varies | `db.articles.createIndex({ body: "text" })` |
| **Geospatial** | Location queries | $\text{O}(\log n)$ | `db.stores.createIndex({ location: "2dsphere" })` |
| **Hashed** | Sharding key (even distribution) | $\text{O}(1)$ | `db.users.createIndex({ user_id: "hashed" })` |
| **TTL** | Auto-expire documents | N/A | `db.sessions.createIndex({ created_at: 1 }, { expireAfterSeconds: 3600 })` |

**Index Best Practices:**

```javascript
// ✅ Good: Compound index covers query
db.orders.createIndex({ user_id: 1, status: 1, created_at: -1 })

// Query uses index efficiently
db.orders.find({ user_id: "user123", status: "shipped" })
  .sort({ created_at: -1 })

// ❌ Bad: Index doesn't match query order
db.orders.createIndex({ status: 1, user_id: 1 })

// Query can't use index efficiently (user_id not first)
db.orders.find({ user_id: "user123", status: "shipped" })
```

**ESR Rule (Equality, Sort, Range):**

```javascript
// Query: Find user's orders in last 30 days, sorted by total
db.orders.find({
  user_id: "user123",          // Equality
  created_at: { $gte: date30DaysAgo }  // Range
}).sort({ total: -1 })          // Sort

// Optimal index: Equality → Sort → Range
db.orders.createIndex({ user_id: 1, total: -1, created_at: 1 })
```

**Explain Plan:**

```javascript
// Check if query uses index
db.orders.find({ user_id: "user123" }).explain("executionStats")

// Look for:
// - "IXSCAN" (index scan) ✅
// - "COLLSCAN" (collection scan) ❌
// - nReturned vs totalDocsExamined (should be close)
```

---

### 4. Aggregation Framework

MongoDB's aggregation pipeline is like Unix pipes for data:

**Example: E-commerce Analytics**

```javascript
db.orders.aggregate([
  // Stage 1: Filter (last 30 days)
  {
    $match: {
      created_at: { $gte: new Date("2024-01-01") },
      status: "completed"
    }
  },
  
  // Stage 2: Group by product, sum revenue
  {
    $group: {
      _id: "$product_id",
      total_revenue: { $sum: "$total" },
      order_count: { $sum: 1 },
      avg_order_value: { $avg: "$total" }
    }
  },
  
  // Stage 3: Sort by revenue (descending)
  {
    $sort: { total_revenue: -1 }
  },
  
  // Stage 4: Limit to top 10
  {
    $limit: 10
  },
  
  // Stage 5: Lookup product details
  {
    $lookup: {
      from: "products",
      localField: "_id",
      foreignField: "_id",
      as: "product_info"
    }
  },
  
  // Stage 6: Project final shape
  {
    $project: {
      product_name: { $arrayElemAt: ["$product_info.name", 0] },
      total_revenue: 1,
      order_count: 1,
      avg_order_value: { $round: ["$avg_order_value", 2] }
    }
  }
])
```

**Common Aggregation Stages:**

| Stage | Purpose | SQL Equivalent |
|-------|---------|----------------|
| `$match` | Filter documents | `WHERE` |
| `$group` | Aggregate by field | `GROUP BY` |
| `$sort` | Sort results | `ORDER BY` |
| `$limit` | Limit results | `LIMIT` |
| `$skip` | Skip results | `OFFSET` |
| `$lookup` | Join collections | `JOIN` |
| `$unwind` | Flatten arrays | N/A |
| `$project` | Select fields | `SELECT` |
| `$addFields` | Add computed fields | Computed columns |

**Performance Tips:**

- ✅ **Put `$match` early:** Filter data ASAP to reduce pipeline size.
- ✅ **Use indexes:** `$match` and `$sort` can use indexes if early in pipeline.
- ❌ **Avoid `$lookup` on large collections:** Joins are expensive (consider embedding).
- ✅ **Use `$facet` for multiple aggregations:** Run multiple pipelines in parallel.

---

### 5. Sharding: Horizontal Scaling

MongoDB shards data across multiple servers using **range-based** or **hash-based** sharding:

```
┌─────────────────────────────────────────────────────┐
│              MongoDB Sharded Cluster                │
├─────────────────────────────────────────────────────┤
│                                                      │
│  ┌───────────────┐                                  │
│  │  mongos       │  (Router - routes queries)       │
│  │  (Query       │                                  │
│  │   Router)     │                                  │
│  └───────┬───────┘                                  │
│          │                                           │
│  ┌───────▼───────────────────────────────┐         │
│  │  Config Servers (3 replicas)          │         │
│  │  Store metadata: shard → chunk map    │         │
│  └────────────────────────────────────────┘         │
│          │                                           │
│          ▼                                           │
│  ┌─────────────┬─────────────┬─────────────┐       │
│  │  Shard 1    │  Shard 2    │  Shard 3    │       │
│  │  Chunks:    │  Chunks:    │  Chunks:    │       │
│  │  0-1000     │  1001-2000  │  2001-3000  │       │
│  └─────────────┴─────────────┴─────────────┘       │
└─────────────────────────────────────────────────────┘
```

**Shard Key Selection (Critical):**

| Criterion | Good Shard Key | Bad Shard Key |
|-----------|----------------|---------------|
| **Cardinality** | High (user_id) | Low (status: ["active", "inactive"]) |
| **Distribution** | Even (hashed user_id) | Skewed (monotonic _id) |
| **Query Pattern** | Matches queries (user_id) | Not in queries (random UUID) |

**Example: E-commerce**

```javascript
// ❌ Bad: Monotonic _id (all writes go to last shard)
sh.shardCollection("ecommerce.orders", { _id: 1 })

// ✅ Good: Hashed user_id (even distribution)
sh.shardCollection("ecommerce.orders", { user_id: "hashed" })

// ✅ Good: Compound key (user_id + created_at for range queries)
sh.shardCollection("ecommerce.orders", { user_id: 1, created_at: 1 })
```

**Chunk Balancing:**

- **Chunks:** MongoDB splits data into 64MB chunks.
- **Balancer:** Background process moves chunks between shards to keep load balanced.
- **Jumbo Chunks:** If a chunk grows >64MB and can't be split (single shard key value), it becomes a hotspot.

**Avoiding Hotspots:**

```javascript
// Problem: Celebrity user with 10,000 orders (all in one chunk)
sh.shardCollection("orders", { user_id: 1 })

// Solution: Add timestamp to shard key (distributes single user across shards)
sh.shardCollection("orders", { user_id: 1, created_at: 1 })
```

---

### 6. Replication: High Availability

MongoDB uses **replica sets** for HA:

```
┌─────────────────────────────────────────────┐
│           Replica Set (3 nodes)             │
├─────────────────────────────────────────────┤
│                                              │
│  ┌──────────────┐                           │
│  │   Primary    │  (Read/Write)             │
│  │   Node 1     │                           │
│  └──────┬───────┘                           │
│         │ Oplog replication                 │
│         ▼                                    │
│  ┌──────────────┐     ┌──────────────┐     │
│  │  Secondary   │     │  Secondary   │     │
│  │   Node 2     │     │   Node 3     │     │
│  └──────────────┘     └──────────────┘     │
│   (Read-only)          (Read-only)          │
│                                              │
│  Election on Primary failure: 2/3 nodes     │
│  elect new Primary (Raft-like consensus)    │
└─────────────────────────────────────────────┘
```

**Write Concern (Durability):**

| Level | Behavior | Use Case |
|-------|----------|----------|
| `w: 0` | Fire-and-forget (no acknowledgment) | Logging (data loss OK) |
| `w: 1` | Primary acknowledges | Default |
| `w: majority` | Majority acknowledges (2 out of 3) | Critical data (no data loss) |
| `w: 3` | All 3 nodes acknowledge | Maximum durability (slowest) |

**Read Preference:**

| Mode | Reads From | Use Case |
|------|------------|----------|
| `primary` | Primary only | Strong consistency |
| `primaryPreferred` | Primary if available, else secondary | High availability |
| `secondary` | Secondary only | Offload reads from primary |
| `secondaryPreferred` | Secondary if available, else primary | Analytics queries |
| `nearest` | Lowest latency node | Geo-distributed |

**Read Concern:**

| Level | Guarantee | Latency |
|-------|-----------|---------|
| `local` | Latest data (may be rolled back) | Fastest |
| `majority` | Majority-committed data | Slower |
| `linearizable` | Latest majority-committed + causally consistent | Slowest |

**Causally Consistent Sessions:**

```javascript
// Ensures "read your writes" consistency
const session = client.startSession({ causalConsistency: true })

// Write
await db.collection('users').updateOne(
  { _id: userId },
  { $set: { status: 'active' } },
  { session }
)

// Read (guaranteed to see the write)
const user = await db.collection('users').findOne(
  { _id: userId },
  { session }
)
```

---

### 7. Transactions (MongoDB 4.0+)

MongoDB supports **multi-document ACID transactions**:

**Example: Bank Transfer**

```javascript
const session = client.startSession()

try {
  await session.withTransaction(async () => {
    // Deduct from sender
    await db.collection('accounts').updateOne(
      { _id: senderAccountId },
      { $inc: { balance: -amount } },
      { session }
    )
    
    // Add to receiver
    await db.collection('accounts').updateOne(
      { _id: receiverAccountId },
      { $inc: { balance: amount } },
      { session }
    )
    
    // Log transaction
    await db.collection('transactions').insertOne(
      { sender: senderAccountId, receiver: receiverAccountId, amount },
      { session }
    )
  })
  
  console.log('Transaction committed')
} catch (error) {
  console.log('Transaction aborted:', error)
} finally {
  await session.endSession()
}
```

**Performance Considerations:**

- ✅ **Use sparingly:** Transactions have overhead (locks, oplog entries).
- ✅ **Keep transactions short:** <60 seconds (default timeout).
- ❌ **Avoid on sharded collections:** Cross-shard transactions are slower.
- ✅ **Prefer embedded documents:** Atomic updates without transactions.

---

### 8. Change Streams: Real-Time Data

**Change Streams** let you listen to database changes in real-time:

```javascript
// Watch for new orders
const changeStream = db.collection('orders').watch()

changeStream.on('change', (change) => {
  console.log('Change detected:', change)
  
  if (change.operationType === 'insert') {
    const newOrder = change.fullDocument
    console.log('New order:', newOrder)
    
    // Trigger actions (send email, update inventory)
    sendOrderConfirmation(newOrder)
  }
})
```

**Use Cases:**

- ✅ **Real-time dashboards:** Update UI when data changes.
- ✅ **Microservices:** Propagate changes to other services (event-driven).
- ✅ **Audit logs:** Track all database changes.
- ✅ **Cache invalidation:** Invalidate cache when data updates.

**Performance:**

- Uses **oplog** (operation log) internally.
- Minimal overhead (tailing oplog).
- Can filter changes with aggregation pipeline.

---

### 9. Performance Tuning

#### **9.1 Profiler**

```javascript
// Enable profiler (log slow queries > 100ms)
db.setProfilingLevel(1, { slowms: 100 })

// View slow queries
db.system.profile.find().sort({ ts: -1 }).limit(10)

// Disable profiler
db.setProfilingLevel(0)
```

#### **9.2 Connection Pooling**

```javascript
// Connection pool configuration
const client = new MongoClient(uri, {
  maxPoolSize: 100,       // Max connections
  minPoolSize: 10,        // Min connections (warm pool)
  maxIdleTimeMS: 30000,   // Close idle connections after 30s
})
```

#### **9.3 Schema Design**

| Anti-Pattern | Fix |
|--------------|-----|
| **Unbounded arrays** (user → 10,000 orders) | Use references or bucketing |
| **Large documents** (>1MB) | Split into multiple docs |
| **Deep nesting** (5+ levels) | Flatten or reference |
| **No indexes** on query fields | Add indexes (ESR rule) |

#### **9.4 Query Optimization**

```javascript
// ❌ Bad: Fetch all fields
db.users.find({ email: "user@example.com" })

// ✅ Good: Project only needed fields
db.users.find(
  { email: "user@example.com" },
  { name: 1, email: 1, _id: 0 }
)

// ❌ Bad: Filter in application
const allOrders = await db.orders.find({}).toArray()
const userOrders = allOrders.filter(o => o.user_id === userId)

// ✅ Good: Filter in database
const userOrders = await db.orders.find({ user_id: userId }).toArray()
```

---

### 10. When to Use MongoDB

#### **✅ Use MongoDB When:**

1. **Flexible schema** — Data structure changes frequently (prototyping, agile dev).
2. **Document-centric** — Data is naturally hierarchical (user profiles, product catalogs).
3. **Horizontal scaling** — Need to scale out to many servers.
4. **High write throughput** — Millions of writes/sec (logs, events, IoT).
5. **Real-time analytics** — Aggregation framework is powerful.
6. **Rapid development** — No schema migrations (schema-on-read).

#### **❌ Don't Use MongoDB When:**

1. **Complex transactions** — Multi-table joins and ACID guarantees (use PostgreSQL).
2. **Strong consistency** — Need strict consistency (use PostgreSQL/MySQL).
3. **Ad-hoc queries** — Frequent schema changes break indexes (use Elasticsearch for search).
4. **Small dataset** — <1GB (PostgreSQL is simpler).
5. **Heavy relational data** — Many foreign keys and joins (use RDBMS).

---

### 11. Real-World Examples

| Company | Use Case | Why MongoDB? |
|---------|----------|--------------|
| **eBay** | Product catalog | Flexible schema for varied product attributes |
| **Adobe** | User profiles | Document model for nested user data |
| **Uber** | Trip data | High write throughput, horizontal scaling |
| **The Guardian** | Content management | Flexible schema for articles, videos, etc. |
| **Bosch** | IoT sensor data | Time-series data, high write volume |

---

### 12. MongoDB vs. Other Databases

| Feature | MongoDB | PostgreSQL | Cassandra | Elasticsearch |
|---------|---------|------------|-----------|---------------|
| **Data Model** | Document (JSON) | Relational (tables) | Wide-column | Document (JSON) |
| **Schema** | Flexible | Rigid | Flexible | Flexible |
| **Transactions** | ACID (4.0+) | ACID | Eventually consistent | No |
| **Scaling** | Horizontal (sharding) | Vertical (limited horizontal) | Horizontal | Horizontal |
| **Query Language** | MQL (MongoDB Query Language) | SQL | CQL (Cassandra Query Language) | Query DSL |
| **Joins** | `$lookup` (slow) | Fast (indexed) | Not supported | Not supported |
| **Use Case** | Flexible schemas, documents | ACID, relational | High writes, time-series | Full-text search |

---

### 13. Common Anti-Patterns

#### ❌ **1. Using MongoDB Like SQL**

**Problem:**

```javascript
// Treating MongoDB like SQL with many small documents
db.users.insertOne({ id: 1, name: "John" })
db.addresses.insertOne({ user_id: 1, street: "123 Main" })
db.orders.insertOne({ user_id: 1, total: 99.99 })

// Requires 3 queries to fetch user data
```

**Solution:**

```javascript
// Embed related data
db.users.insertOne({
  id: 1,
  name: "John",
  address: { street: "123 Main" },
  orders: [{ total: 99.99 }]
})
```

#### ❌ **2. Not Using Indexes**

**Problem:** Collection scan on 1M documents.

**Solution:** Add index on query fields.

```javascript
db.users.createIndex({ email: 1 })
```

#### ❌ **3. Unbounded Arrays**

**Problem:** User has 10,000 orders embedded (document size limit).

**Solution:** Use references or bucketing.

```javascript
// Bucket orders by month
{
  user_id: "user123",
  month: "2024-01",
  orders: [/* orders in Jan 2024 */]
}
```

---

### 14. Trade-offs Summary

| What You Gain | What You Sacrifice |
|---------------|-------------------|
| ✅ Schema flexibility | ❌ Less query optimization (no query planner like PostgreSQL) |
| ✅ Horizontal scaling | ❌ Eventual consistency (in sharded clusters) |
| ✅ Fast development (no migrations) | ❌ Data duplication (denormalization) |
| ✅ High write throughput | ❌ Slower joins (`$lookup` not as fast as SQL) |
| ✅ Native JSON support | ❌ 16MB document size limit |

---

### 15. References

- **MongoDB Documentation:** [https://docs.mongodb.com/](https://docs.mongodb.com/)
- **MongoDB University (Free Courses):** [https://university.mongodb.com/](https://university.mongodb.com/)
- **MongoDB Schema Design Patterns:** [https://www.mongodb.com/blog/post/building-with-patterns-a-summary](https://www.mongodb.com/blog/post/building-with-patterns-a-summary)
- **Related Chapters:**
  - [2.1.2 NoSQL Deep Dive](./2.1.2-no-sql-deep-dive.md) — BASE principles
  - [2.1.6 Data Modeling for Scale](./2.1.6-data-modeling-for-scale.md) — Denormalization patterns
  - [2.1.4 Database Scaling](./2.1.4-database-scaling.md) — Sharding strategies
  - [2.1.5 Indexing and Query Optimization](./2.1.5-indexing-and-query-optimization.md) — Index strategies


---

## ✏️ Design Challenge

### Problem

You're designing a **social media** platform's user profile system with the following requirements:

1. **Flexible schema:** Users can have varying profile fields (some have portfolios, some don't)
2. **Embedded data:** Each user has 5-10 addresses, 20-50 posts (preview), 100-500 followers
3. **Query patterns:**
   - Fetch user with all embedded data (profile page)
   - Query users by location (geospatial)
   - Search users by skills/interests
4. **Scale:** 100M users, 10K writes/sec, 50K reads/sec

**Question:** Should you use embedded documents or references for addresses, posts, and followers? What indexes would you create? How would you handle the "celebrity problem" (user with 10M followers)?

### Solution

#### 🧩 Scenario

- **System:** Social media user profiles
- **Data:** 100M users
- **Relationships:**
  - User → Addresses (5-10 per user)
  - User → Posts preview (20-50 recent posts)
  - User → Followers (100-500 average, but celebrities have 10M+)
- **Query patterns:** Full profile fetch, location search, skill search

#### ✅ Goal

- Minimize database queries (fetch profile in one query)
- Handle varying data shapes (flexible schema)
- Prevent document size explosion (16MB limit)
- Efficient geospatial and text search
- Scale to 100M users

#### ⚙️ Solution: Hybrid Approach (Embed Small, Reference Large)

**Data Model:**

```javascript
// Users collection (embedded data for small, bounded fields)
{
  "_id": ObjectId("..."),
  "username": "john_doe",
  "email": "john@example.com",
  "profile": {
    "name": "John Doe",
    "bio": "Software Engineer",
    "avatar_url": "https://...",
    "location": {
      "type": "Point",
      "coordinates": [-122.4194, 37.7749]  // [longitude, latitude]
    }
  },
  "addresses": [
    { "type": "home", "street": "123 Main St", "city": "SF" },
    { "type": "work", "street": "456 Market St", "city": "SF" }
  ],
  "skills": ["JavaScript", "Python", "MongoDB"],
  "recent_posts": [
    { "post_id": ObjectId("..."), "title": "Hello World", "created_at": ISODate("...") }
  ],
  "stats": {
    "followers_count": 150,
    "following_count": 200,
    "posts_count": 45
  },
  "created_at": ISODate("2024-01-15T10:30:00Z")
}

// Followers collection (referenced, unbounded)
{
  "_id": ObjectId("..."),
  "user_id": ObjectId("..."),
  "follower_id": ObjectId("..."),
  "followed_at": ISODate("...")
}

// Posts collection (referenced, unbounded)
{
  "_id": ObjectId("..."),
  "user_id": ObjectId("..."),
  "title": "My first post",
  "body": "...",
  "likes_count": 42,
  "created_at": ISODate("...")
}
```

**Rationale:**

| Data | Embedded or Referenced? | Why? |
|------|------------------------|------|
| **Addresses** | ✅ Embedded | Small (5-10 items), always queried with profile, bounded |
| **Skills** | ✅ Embedded | Small array, frequently updated, bounded |
| **Recent posts preview** | ✅ Embedded (limited to 20) | Fast profile page load, bounded size |
| **Followers** | ❌ Referenced | Unbounded (celebrities have millions), rarely queried with profile |
| **All posts** | ❌ Referenced | Unbounded (users have thousands of posts over time) |

**Indexes:**

```javascript
// 1. Username lookup (unique)
db.users.createIndex({ username: 1 }, { unique: true })

// 2. Email lookup (unique, for login)
db.users.createIndex({ email: 1 }, { unique: true })

// 3. Geospatial search (find users near location)
db.users.createIndex({ "profile.location": "2dsphere" })

// 4. Skill search (multikey index on array)
db.users.createIndex({ skills: 1 })

// 5. Followers lookup
db.followers.createIndex({ user_id: 1, follower_id: 1 }, { unique: true })
db.followers.createIndex({ follower_id: 1 })  // For "following" queries

// 6. Posts by user
db.posts.createIndex({ user_id: 1, created_at: -1 })
```

#### ⚠️ Handling the Celebrity Problem

**Problem:** User with 10M followers causes:
- Follower count update is slow
- Fetching all followers times out
- Shard hotspot (all followers in one shard key)

**Solution 1: Denormalize Follower Count**

```javascript
// DON'T increment count by querying followers collection
// ❌ Bad
user.stats.followers_count = db.followers.count({ user_id: user._id })

// ✅ Good: Increment atomically in user document
db.users.updateOne(
  { _id: user_id },
  { $inc: { "stats.followers_count": 1 } }
)
```

**Solution 2: Paginate Followers**

```javascript
// ❌ Bad: Fetch all 10M followers
db.followers.find({ user_id: celebrity_id })

// ✅ Good: Paginate with limit + skip
db.followers.find({ user_id: celebrity_id })
  .sort({ followed_at: -1 })
  .limit(20)
  .skip(page * 20)
```

**Solution 3: Shard Followers Collection**

```javascript
// Shard key: user_id + follower_id (prevents hotspot)
sh.shardCollection("social.followers", { user_id: 1, follower_id: 1 })
```

#### 🧠 Query Examples

**Query 1: Full Profile (One Query)**

```javascript
const user = db.users.findOne({ username: "john_doe" })
// Returns user with embedded addresses, skills, recent posts
// No additional queries needed!
```

**Query 2: Find Users Near Location**

```javascript
db.users.find({
  "profile.location": {
    $near: {
      $geometry: {
        type: "Point",
        coordinates: [-122.4194, 37.7749]
      },
      $maxDistance: 5000  // 5km radius
    }
  }
})
```

**Query 3: Search Users by Skill**

```javascript
db.users.find({ skills: "MongoDB" })
```

**Query 4: Get User's Followers (Paginated)**

```javascript
db.followers.find({ user_id: user_id })
  .sort({ followed_at: -1 })
  .limit(20)
```

#### ✅ Final Answer

| Aspect | Decision | Reason |
|--------|----------|--------|
| **Data Model** | **Hybrid (embed small, reference large)** | Balance between query performance and document size |
| **Addresses** | Embedded | Small, bounded, always needed |
| **Recent posts** | Embedded (limit 20) | Fast profile load, bounded |
| **Followers** | Referenced | Unbounded (celebrity problem) |
| **Skills** | Embedded array | Small, frequently filtered |
| **Geospatial** | 2dsphere index on location | Efficient radius queries |
| **Sharding** | Shard followers by (user_id, follower_id) | Prevents celebrity hotspots |
| **Trade-off** | Extra query for all posts/followers | Gain: No 16MB document limit issues |

**Key Metrics:**
- **Profile fetch:** <10ms (single query with embedded data)
- **Geospatial query:** <50ms (indexed)
- **Follower pagination:** <20ms (indexed + limit)
- **Document size:** <1MB per user (well under 16MB limit)

**Anti-Pattern to Avoid:**
❌ Embedding 10M followers in user document → Document size explosion
✅ Reference followers, paginate queries, denormalize count

