# 2.1.20 CQRS Deep Dive: Command-Query Responsibility Segregation

## Intuitive Explanation

Imagine a library with two entrances: one for returning books (writes) and one for browsing (reads). The return desk is
optimized for processing returns quickly (write-optimized), while the reading area is optimized for finding books
easily (read-optimized). They serve different purposes and are designed differently.

**CQRS (Command-Query Responsibility Segregation)** is like having separate systems for writes and reads:

- **Command Side (Write):** Optimized for data integrity, ACID transactions
- **Query Side (Read):** Optimized for query performance, denormalized data
- **Goal:** Scale reads and writes independently
- **Benefit:** Better performance, independent scaling, optimized for each use case

---

## In-Depth Analysis

### 1. What is CQRS?

**CQRS (Command-Query Responsibility Segregation)** is an architectural pattern that separates the model used to update
information (Command) from the model used to read information (Query).

**Key Concepts:**

- **Command:** Operation that changes state (write)
- **Query:** Operation that reads state (read)
- **Command Model:** Write-optimized (normalized, ACID)
- **Query Model:** Read-optimized (denormalized, indexed)
- **Synchronization:** Command model updates query model asynchronously

**Traditional Approach:**

```
Single Model:
  - Same database for reads and writes
  - Optimized for both (compromise)
  - Can't optimize for either perfectly
```

**CQRS Approach:**

```
Command Model (Write):
  - Optimized for writes (ACID, normalized)
  - Fast writes, data integrity

Query Model (Read):
  - Optimized for reads (denormalized, indexed)
  - Fast queries, flexible schema

Synchronization:
  - Command â†’ Event â†’ Query Model Update
  - Eventually consistent
```

### 2. CQRS Architecture

**Basic Flow:**

```
1. Command â†’ Command Handler â†’ Command Model (Write DB)
2. Command Model â†’ Event â†’ Message Queue
3. Message Queue â†’ Query Handler â†’ Query Model (Read DB)
4. Query â†’ Query Model (Read DB) â†’ Response
```

**Components:**

```
Command Side:
  - Command Handler (validates, processes)
  - Command Model (write-optimized database)
  - Event Publisher (publishes changes)

Query Side:
  - Query Handler (processes queries)
  - Query Model (read-optimized database)
  - Event Consumer (updates query model)
```

### 3. Command Model (Write Side)

**Characteristics:**

- **Normalized:** Follows database normalization
- **ACID:** Strong consistency guarantees
- **Optimized for Writes:** Fast inserts, updates
- **Data Integrity:** Constraints, validations

**Example Schema:**

```sql
-- Normalized (Command Model)
CREATE TABLE users (
    user_id BIGINT PRIMARY KEY,
    email VARCHAR(255) UNIQUE NOT NULL,
    created_at TIMESTAMP
);

CREATE TABLE orders (
    order_id BIGINT PRIMARY KEY,
    user_id BIGINT REFERENCES users(user_id),
    total_amount DECIMAL(10,2),
    status VARCHAR(50),
    created_at TIMESTAMP
);

CREATE TABLE order_items (
    item_id BIGINT PRIMARY KEY,
    order_id BIGINT REFERENCES orders(order_id),
    product_id BIGINT,
    quantity INT,
    price DECIMAL(10,2)
);
```

**Benefits:**

- **Data Integrity:** Foreign keys, constraints
- **ACID Transactions:** Strong consistency
- **Normalized:** No data duplication
- **Fast Writes:** Optimized for inserts/updates

### 4. Query Model (Read Side)

**Characteristics:**

- **Denormalized:** Data duplicated for performance
- **Optimized for Reads:** Indexes, materialized views
- **Flexible Schema:** Can have multiple read models
- **Eventually Consistent:** May lag behind command model

**Example Schema:**

```sql
-- Denormalized (Query Model)
CREATE TABLE user_orders_view (
    order_id BIGINT PRIMARY KEY,
    user_id BIGINT,
    user_email VARCHAR(255),  -- Denormalized
    total_amount DECIMAL(10,2),
    status VARCHAR(50),
    item_count INT,  -- Pre-computed
    created_at TIMESTAMP,
    INDEX idx_user_created (user_id, created_at),
    INDEX idx_status (status)
);

-- Materialized View for Analytics
CREATE MATERIALIZED VIEW order_stats_daily AS
SELECT
    DATE(created_at) as date,
    COUNT(*) as order_count,
    SUM(total_amount) as total_revenue,
    AVG(total_amount) as avg_order_value
FROM user_orders_view
GROUP BY DATE(created_at);
```

**Benefits:**

- **Fast Queries:** No joins needed
- **Flexible:** Multiple read models for different queries
- **Optimized:** Indexes for specific query patterns
- **Scalable:** Can scale independently

### 5. Synchronization Strategies

#### A. Event-Driven Synchronization

**How It Works:**

```
1. Command Handler: Processes command, updates command model
2. Command Model: Publishes event (OrderCreated, OrderPaid, etc.)
3. Message Queue: Buffers events (Kafka, RabbitMQ)
4. Query Handler: Consumes events, updates query model
```

**Benefits:**

- **Decoupled:** Command and query sides independent
- **Scalable:** Can have multiple query models
- **Resilient:** Events buffered, can replay

**Example:**

```
Command: Create Order
  â†’ Command Model: INSERT INTO orders (...)
  â†’ Event: OrderCreated { order_id, user_id, items, total }
  â†’ Kafka: Publishes event
  â†’ Query Handler: Consumes event
  â†’ Query Model: INSERT INTO user_orders_view (denormalized)
```

#### B. Change Data Capture (CDC)

**How It Works:**

```
1. Command Model: Database changes (INSERT, UPDATE, DELETE)
2. CDC Tool: Captures database changes (Debezium, AWS DMS)
3. Message Queue: Publishes change events
4. Query Handler: Consumes changes, updates query model
```

**Benefits:**

- **Automatic:** No code changes needed
- **Complete:** Captures all changes
- **Real-Time:** Low latency

**Example:**

```
PostgreSQL â†’ Debezium â†’ Kafka:
  - Captures all INSERT/UPDATE/DELETE
  - Publishes as events
  - Query handlers consume and update read models
```

#### C. Dual Write (Not Recommended)

**How It Works:**

```
1. Command Handler: Updates both command and query models
2. Transaction: Both updates in same transaction
```

**Problems:**

- **Tight Coupling:** Command and query sides coupled
- **Performance:** Slower (two writes)
- **Scaling:** Can't scale independently

### 6. Eventual Consistency

**Challenge:** Query model lags behind command model

**Consistency Levels:**

```
Strong Consistency:
  - Query model updated in same transaction
  - No lag, but slower and coupled

Eventual Consistency:
  - Query model updated asynchronously
  - 1-2 second lag typical
  - Faster and decoupled
```

**Handling Consistency:**

```
1. Version Numbers:
   - Command model: version = 10
   - Query model: version = 8 (lagging)
   - Client can check version

2. Read-After-Write:
   - After write, read from command model
   - Or wait for query model update

3. Stale Reads Acceptable:
   - Most queries can tolerate 1-2s lag
   - Critical queries read from command model
```

### 7. Multiple Read Models

**Pattern:** One command model, multiple query models

**Use Cases:**

```
Command Model:
  - Single source of truth
  - Normalized, ACID

Query Model 1 (User Orders):
  - Denormalized user + orders
  - Optimized for: "Get user's orders"

Query Model 2 (Order Analytics):
  - Aggregated statistics
  - Optimized for: "Get daily revenue"

Query Model 3 (Search):
  - Full-text search index (Elasticsearch)
  - Optimized for: "Search orders by product"
```

**Benefits:**

- **Optimized:** Each read model optimized for specific queries
- **Flexible:** Can add new read models without changing command model
- **Scalable:** Scale each read model independently

### 8. CQRS vs. Traditional Approach

| Aspect           | Traditional                 | CQRS                              |
|------------------|-----------------------------|-----------------------------------|
| **Model**        | Single model (read + write) | Separate models (command + query) |
| **Optimization** | Compromise (both)           | Optimized for each                |
| **Scaling**      | Scale together              | Scale independently               |
| **Consistency**  | Strong (immediate)          | Eventual (1-2s lag)               |
| **Complexity**   | Low                         | High                              |
| **Flexibility**  | Limited                     | High (multiple read models)       |
| **Performance**  | Good (compromise)           | Excellent (optimized)             |

### 9. When to Use CQRS

**Indicators:**

- **Read/Write Ratio:** 100:1 or higher (read-heavy)
- **Different Patterns:** Reads and writes have different patterns
- **Scaling Needs:** Need to scale reads and writes independently
- **Performance Critical:** Need maximum performance for both

**Example Scenarios:**

```
âœ… Good for CQRS:
  - Social media feed (1000 reads per write)
  - Analytics dashboard (complex queries, infrequent writes)
  - Search system (full-text search, simple writes)

âŒ Not Good for CQRS:
  - Simple CRUD (read/write ratio ~1:1)
  - Low traffic (complexity not worth it)
  - Strong consistency required (can't tolerate lag)
```

### 10. Common Patterns

#### A. CQRS + Event Sourcing

**Pattern:**

```
Command Side:
  - Commands â†’ Events â†’ Event Store
  - State derived from events

Query Side:
  - Events â†’ Projections â†’ Read Models
  - Multiple read models from same events
```

**Benefits:**

- **Complete History:** Event sourcing provides audit trail
- **Flexibility:** Multiple read models from events
- **Replay:** Can rebuild read models from events

#### B. CQRS + Materialized Views

**Pattern:**

```
Command Model:
  - Normalized tables (writes)

Query Model:
  - Materialized views (reads)
  - Refreshed periodically or on-demand
```

**Benefits:**

- **Simple:** Uses database features
- **Fast Queries:** Pre-computed aggregations
- **Low Complexity:** No custom code needed

#### C. CQRS + Search Index

**Pattern:**

```
Command Model:
  - Primary database (writes)

Query Model:
  - Search index (Elasticsearch, Algolia)
  - Updated via events or CDC
```

**Benefits:**

- **Fast Search:** Full-text search capabilities
- **Flexible:** Can search across multiple fields
- **Scalable:** Search index scales independently

---

## When to Use CQRS

### âœ… Use CQRS When:

1. **High Read/Write Ratio:** 100:1 or higher (read-heavy)
2. **Different Patterns:** Reads and writes have different access patterns
3. **Scaling Needs:** Need to scale reads and writes independently
4. **Performance Critical:** Need maximum performance
5. **Multiple Query Patterns:** Need different read models for different queries
6. **Event-Driven:** Already using events/message queues

### âŒ Don't Use CQRS When:

1. **Simple CRUD:** Standard CRUD is sufficient
2. **Low Traffic:** Complexity not worth it
3. **Strong Consistency Required:** Can't tolerate eventual consistency
4. **Read/Write Ratio ~1:1:** No benefit from separation
5. **Simple Queries:** Standard queries work fine

---

## Real-World Examples

### Twitter (CQRS)

**Use Case:** Timeline generation

**Command Side:**

- Write tweets (normalized, ACID)
- Low write volume

**Query Side:**

- Pre-computed timelines (denormalized)
- High read volume (1000Ã— writes)
- Multiple read models (home timeline, user timeline)

### Netflix (CQRS)

**Use Case:** Recommendation system

**Command Side:**

- User interactions (watched, rated)
- Event store

**Query Side:**

- Recommendation models (pre-computed)
- Search index (Elasticsearch)
- Analytics (aggregated data)

### E-Commerce (CQRS)

**Use Case:** Product catalog

**Command Side:**

- Product management (admin writes)
- Normalized product data

**Query Side:**

- Product search (Elasticsearch)
- Product listings (denormalized)
- Recommendations (ML models)

---

## CQRS vs. Other Solutions

| Solution               | Best For              | Complexity | Performance | Consistency |
|------------------------|-----------------------|------------|-------------|-------------|
| **CQRS**               | Read/write separation | High       | Excellent   | Eventual    |
| **Traditional**        | Simple applications   | Low        | Good        | Strong      |
| **Read Replicas**      | Read scaling          | Low        | Good        | Eventual    |
| **Materialized Views** | Aggregations          | Medium     | Good        | Periodic    |

---

## Common Anti-Patterns

### âŒ **1. Using CQRS for Simple CRUD**

**Problem:** Over-engineering for simple use case

**Solution:** Use traditional approach for simple cases

```
âŒ Bad:
Simple blog with 100 posts/day â†’ CQRS
â†’ Unnecessary complexity, no benefit

âœ… Good:
Simple blog â†’ Traditional CRUD
â†’ Simple, sufficient
```

### âŒ **2. Synchronous Query Model Updates**

**Problem:** Updating query model in same transaction

**Solution:** Use asynchronous updates

```
âŒ Bad:
BEGIN TRANSACTION
  UPDATE command_model
  UPDATE query_model  -- Blocks!
COMMIT
â†’ Slow, coupled, can't scale

âœ… Good:
UPDATE command_model
PUBLISH event
â†’ Query model updates asynchronously
â†’ Fast, decoupled, scalable
```

### âŒ **3. No Version Tracking**

**Problem:** Can't detect query model lag

**Solution:** Track versions

```
âŒ Bad:
Command model updated, query model not updated
â†’ Client reads stale data, no way to know

âœ… Good:
Command model: version = 10
Query model: version = 8
â†’ Client can check version, handle stale reads
```

---

## Trade-offs Summary

| Aspect           | What You Gain                  | What You Sacrifice       |
|------------------|--------------------------------|--------------------------|
| **Performance**  | Optimized for reads and writes | Complexity (two models)  |
| **Scalability**  | Independent scaling            | Eventual consistency     |
| **Flexibility**  | Multiple read models           | Synchronization overhead |
| **Optimization** | Each model optimized           | More infrastructure      |
| **Consistency**  | Eventual (1-2s lag)            | Not immediate            |

---

## References

- **CQRS Pattern:** [https://martinfowler.com/bliki/CQRS.html](https://martinfowler.com/bliki/CQRS.html)
- **Related Chapters:**
    - [2.1.6 Data Modeling for Scale](./2.1.6-data-modeling-for-scale.md) - CQRS overview
    - [2.3.9 Event Sourcing Deep Dive](../2.3-messaging-streaming/2.3.9-event-sourcing-deep-dive.md) - CQRS + Event
      Sourcing
    - [2.3.2 Kafka Deep Dive](../2.3-messaging-streaming/2.3.2-kafka-deep-dive.md) - Event synchronization
    - [3.2.1 Twitter Timeline Challenge](../../../03-challenges/3.2.1-twitter-timeline/README.md) - CQRS for feeds

---

## âœï¸ Design Challenge

### Problem

You are designing a social media feed system that must:

1. **Handle 1M posts per day** (write volume)
2. **Serve 100M feed requests per day** (read volume, 100:1 ratio)
3. **Generate personalized feeds** (complex queries, user-specific)
4. **Support multiple feed types** (home feed, user feed, trending)
5. **Fast feed loading** (<200ms for feed generation)
6. **Handle celebrity posts** (millions of followers, fanout)

**Constraints:**

- Reads are 100Ã— more frequent than writes
- Feed generation requires complex queries (joins, aggregations)
- Need to support different feed algorithms (chronological, ranked, personalized)
- Must handle traffic spikes (viral posts)

Design a CQRS strategy that:

- Handles high write volume
- Handles high read volume
- Enables fast feed generation
- Supports multiple feed types
- Optimizes for read/write patterns

### Solution

#### ğŸ§© Scenario

- **Posts:** 1M posts/day = 11.6 posts/second average, 100/second peak
- **Feed Requests:** 100M requests/day = 1,157 requests/second average, 10K/second peak
- **Read/Write Ratio:** 100:1 (perfect for CQRS)

**Calculations:**

- **Writes:** 100 posts/second peak
- **Reads:** 10K feed requests/second peak
- **Feed Generation:** Complex (joins, ranking, personalization)

#### âœ… Step 1: CQRS Architecture

**Command Side (Write):**

```
Post Service:
  - Receives post creation
  - Validates post
  - Writes to command model (normalized)
  - Publishes PostCreated event
```

**Query Side (Read):**

```
Feed Service:
  - Receives feed request
  - Queries read model (denormalized)
  - Generates feed (fast, pre-computed)
  - Returns feed
```

#### âœ… Step 2: Command Model (Write)

**Schema (PostgreSQL):**

```sql
-- Normalized (Command Model)
CREATE TABLE posts (
    post_id BIGINT PRIMARY KEY,
    user_id BIGINT NOT NULL,
    content TEXT NOT NULL,
    created_at TIMESTAMP DEFAULT NOW(),
    INDEX idx_user_created (user_id, created_at)
);

CREATE TABLE users (
    user_id BIGINT PRIMARY KEY,
    username VARCHAR(255) NOT NULL,
    avatar_url VARCHAR(255)
);
```

**Benefits:**

- **ACID:** Strong consistency
- **Normalized:** No duplication
- **Fast Writes:** Optimized for inserts
- **Data Integrity:** Foreign keys, constraints

#### âœ… Step 3: Query Model (Read)

**Schema (Cassandra/Redis):**

```sql
-- Denormalized (Query Model)
CREATE TABLE user_feeds (
    user_id BIGINT,
    post_id BIGINT,
    author_id BIGINT,
    author_username VARCHAR(255),  -- Denormalized
    author_avatar VARCHAR(255),   -- Denormalized
    content TEXT,
    created_at TIMESTAMP,
    score DECIMAL,  -- Pre-computed ranking score
    PRIMARY KEY (user_id, created_at, post_id)
) WITH CLUSTERING ORDER BY (created_at DESC);

-- Materialized View for Trending
CREATE MATERIALIZED VIEW trending_posts AS
SELECT post_id, author_id, content, created_at, score
FROM user_feeds
WHERE created_at > now() - 24 hours
PRIMARY KEY (score, created_at, post_id)
WITH CLUSTERING ORDER BY (score DESC, created_at DESC);
```

**Benefits:**

- **Fast Queries:** No joins needed
- **Pre-computed:** Ranking scores calculated
- **Denormalized:** All data in one place
- **Indexed:** Optimized for feed queries

#### âœ… Step 4: Event Synchronization

**Event Flow:**

```
1. Post Service: Creates post
2. Command Model: INSERT INTO posts (...)
3. Event: PostCreated { post_id, user_id, content, ... }
4. Kafka: Publishes event
5. Feed Service: Consumes event
6. Feed Service: Updates read models (fanout)
```

**Fanout (Write-Heavy):**

```
PostCreated Event:
  - Get all followers of author
  - For each follower:
    - INSERT INTO user_feeds (denormalized)
    - Pre-compute ranking score
    - Update trending feed if needed
```

#### âœ… Step 5: Multiple Read Models

**Read Model 1: User Feeds (Cassandra)**

```
Optimized for: "Get user's home feed"
  - Pre-computed feeds per user
  - Denormalized (author info included)
  - Sorted by created_at (chronological)
  - Sorted by score (ranked)
```

**Read Model 2: Trending Feed (Redis Sorted Set)**

```
Optimized for: "Get trending posts"
  - Sorted by score (engagement metrics)
  - Time-windowed (last 24 hours)
  - Fast top-K queries
```

**Read Model 3: User Posts (Cassandra)**

```
Optimized for: "Get user's posts"
  - All posts by specific user
  - Sorted by created_at
  - Fast user profile queries
```

#### âœ… Step 6: Feed Generation

**Home Feed (Pre-computed):**

```
Query:
  SELECT * FROM user_feeds
  WHERE user_id = ?
  ORDER BY created_at DESC
  LIMIT 50

Performance:
  - Single query (no joins)
  - Indexed (user_id, created_at)
  - <50ms (meets <200ms requirement)
```

**Trending Feed (Pre-computed):**

```
Query:
  SELECT * FROM trending_posts
  ORDER BY score DESC, created_at DESC
  LIMIT 50

Performance:
  - Single query (materialized view)
  - Sorted by score
  - <50ms
```

#### âœ… Step 7: Performance Optimization

**Caching:**

```
Redis Cache:
  - Cache hot feeds (top 10% of users)
  - TTL: 60 seconds
  - Reduces database load

Cache Strategy:
  - Cache-aside pattern
  - Cache miss â†’ Query database â†’ Update cache
```

**Fanout Optimization:**

```
Celebrity Handling:
  - Users with >10K followers: Don't fanout
  - Instead: Pull model (query on read)
  - Hybrid approach (push for normal, pull for celebrities)
```

#### âœ… Complete Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Post Service (Command Side)                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Create Post â†’ Command Handler                      â”‚ â”‚
â”‚  â”‚ Validates, writes to PostgreSQL                   â”‚ â”‚
â”‚  â”‚ Publishes PostCreated event                       â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   PostgreSQL (Command Model)      â”‚
        â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
        â”‚   â”‚ posts table (normalized)    â”‚ â”‚
        â”‚   â”‚ users table                 â”‚ â”‚
        â”‚   â”‚ ACID transactions           â”‚ â”‚
        â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Kafka (Event Bus)               â”‚
        â”‚   Topic: post-events              â”‚
        â”‚   - PostCreated                   â”‚
        â”‚   - PostLiked                     â”‚
        â”‚   - PostCommented                 â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Feed Service (Query Side)       â”‚
        â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
        â”‚   â”‚ Consumes events             â”‚ â”‚
        â”‚   â”‚ Updates read models         â”‚ â”‚
        â”‚   â”‚ Fanout to followers         â”‚ â”‚
        â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                 â”‚                 â”‚
        â–¼                 â–¼                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Cassandra    â”‚  â”‚ Redis        â”‚  â”‚ Redis Cache  â”‚
â”‚ (User Feeds) â”‚  â”‚ (Trending)   â”‚  â”‚ (Hot Feeds)  â”‚
â”‚              â”‚  â”‚              â”‚  â”‚              â”‚
â”‚ - Pre-       â”‚  â”‚ - Sorted Set â”‚  â”‚ - Cache      â”‚
â”‚   computed   â”‚  â”‚ - Top-K      â”‚  â”‚   hot feeds  â”‚
â”‚   feeds      â”‚  â”‚   queries    â”‚  â”‚ - TTL: 60s   â”‚
â”‚ - Denorm-    â”‚  â”‚              â”‚  â”‚              â”‚
â”‚   alized     â”‚  â”‚              â”‚  â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Request Flow:**

```
Write Path:
1. User â†’ Post Service: Create post
2. Post Service: Validates post
3. Post Service â†’ PostgreSQL: INSERT INTO posts
4. Post Service â†’ Kafka: Publishes PostCreated event
5. Feed Service â†’ Kafka: Consumes event
6. Feed Service: Gets author's followers
7. Feed Service â†’ Cassandra: Fanout to followers (INSERT into user_feeds)
8. Feed Service â†’ Redis: Updates trending feed if needed

Read Path:
1. User â†’ Feed Service: Get home feed
2. Feed Service â†’ Redis Cache: Check cache (cache hit â†’ return)
3. Feed Service â†’ Cassandra: Query user_feeds (cache miss)
4. Feed Service â†’ Redis Cache: Update cache
5. Feed Service â†’ User: Returns feed
```

#### âš–ï¸ Trade-offs Summary

| Decision                    | What We Gain                  | What We Sacrifice                     |
|-----------------------------|-------------------------------|---------------------------------------|
| **CQRS Separation**         | Optimized for reads/writes    | Complexity (two models)               |
| **Denormalized Read Model** | Fast queries (no joins)       | Storage cost, eventual consistency    |
| **Pre-computed Feeds**      | Fast feed generation          | Fanout overhead (write amplification) |
| **Multiple Read Models**    | Optimized for each query type | More infrastructure, sync complexity  |
| **Eventual Consistency**    | Independent scaling           | 1-2s lag (acceptable for feeds)       |

#### âœ… Final Summary

**CQRS Strategy:**

- **Command Model:** PostgreSQL (normalized, ACID, fast writes)
- **Query Models:** Cassandra (user feeds), Redis (trending, cache)
- **Synchronization:** Kafka events (asynchronous, decoupled)
- **Fanout:** Write-heavy (pre-compute feeds)
- **Caching:** Redis (hot feeds, 60s TTL)

**Performance:**

- **Write Throughput:** 100 posts/second (handles peak)
- **Read Throughput:** 10K feed requests/second (handles peak)
- **Feed Generation:** <50ms (pre-computed, meets <200ms requirement)
- **Consistency:** Eventual (1-2s lag, acceptable)

**Result:**

- âœ… Handles 1M posts/day (command model)
- âœ… Handles 100M feed requests/day (query models)
- âœ… Fast feed generation (<50ms, pre-computed)
- âœ… Multiple feed types (home, trending, user)
- âœ… Optimized for read/write patterns (CQRS)
- âœ… Handles celebrity posts (hybrid fanout)

