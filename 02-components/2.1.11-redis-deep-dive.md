# 2.1.11 Redis Deep Dive: The In-Memory Data Structure Store

## Intuitive Explanation

Redis (REmote DIctionary Server) is much more than a cache â€” it's an **in-memory data structure store** that can function as a database, cache, message broker, and queue. Think of it as a super-fast, in-memory Swiss Army knife for data that supports complex data structures (lists, sets, sorted sets, hashes) with atomic operations. Redis is single-threaded but incredibly fast ($\text{O}(1)$ operations, < 1ms latency).

- **In-Memory:** All data lives in RAM (optionally persisted to disk).
- **Data Structures:** Strings, lists, sets, sorted sets, hashes, bitmaps, HyperLogLog, streams, geospatial indexes.
- **Atomic Operations:** All operations are atomic (thread-safe without explicit locking).
- **Use Cases:** Caching, session storage, real-time analytics, leaderboards, pub/sub messaging, rate limiting.

---

## In-Depth Analysis

### 1. Architecture: Single-Threaded Event Loop

Redis uses a **single-threaded event loop** (like Node.js):

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Redis Server                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚  Single-Threaded Event Loop  â”‚           â”‚
â”‚  â”‚  (epoll/kqueue for I/O)      â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚           â”‚                                  â”‚
â”‚           â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚  In-Memory Data Structures   â”‚           â”‚
â”‚  â”‚  - Strings, Lists, Sets, etc.â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚           â”‚                                  â”‚
â”‚           â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚  Persistence (Optional)      â”‚           â”‚
â”‚  â”‚  - RDB (snapshots)           â”‚           â”‚
â”‚  â”‚  - AOF (append-only log)     â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why Single-Threaded?**

- âœ… **No locks:** No race conditions, no deadlocks, no context switching.
- âœ… **Simple:** Easier to reason about and debug.
- âœ… **Fast:** Modern CPUs can handle 100k+ ops/sec on a single core.
- âŒ **Limited by single CPU core:** Can't utilize multiple cores for one instance (use Redis Cluster instead).

**Throughput:**

- **Typical:** 100,000 - 1,000,000 ops/sec (single instance).
- **Latency:** < 1ms for most operations (submillisecond for simple gets/sets).

---

### 2. Data Structures

Redis supports 10+ data structures:

#### **2.1 String (Most Common)**

**Use Cases:** Caching, counters, rate limiting, session storage.

```bash
# Set/Get
SET user:1000:name "John Doe"
GET user:1000:name  # "John Doe"

# Increment (atomic)
SET page:views 0
INCR page:views  # 1
INCR page:views  # 2

# Expiration (TTL)
SETEX session:abc123 3600 "user_data"  # Expires in 1 hour
TTL session:abc123  # Remaining seconds
```

**Use Case: Rate Limiting**

```bash
# Allow 10 requests per minute
SET rate:user:1000 0 EX 60 NX  # NX = only if not exists
INCR rate:user:1000
GET rate:user:1000  # If > 10, reject request
```

#### **2.2 Hash (Object Storage)**

**Use Cases:** Storing objects (user profiles, product details).

```bash
# Store user object
HSET user:1000 name "John" email "john@example.com" age 30

# Get field
HGET user:1000 name  # "John"

# Get all fields
HGETALL user:1000  # { name: "John", email: "john@example.com", age: 30 }

# Increment field
HINCRBY user:1000 login_count 1
```

**Benefits:**

- âœ… **Memory efficient:** Uses less memory than storing full JSON strings.
- âœ… **Atomic updates:** Update individual fields without reading entire object.

#### **2.3 List (Queue, Stack)**

**Use Cases:** Task queues, activity feeds, chat messages.

```bash
# Push to list (queue)
LPUSH queue:tasks "task1"
LPUSH queue:tasks "task2"

# Pop from list
RPOP queue:tasks  # "task1" (FIFO)

# Blocking pop (waits for item)
BRPOP queue:tasks 30  # Block for 30 seconds

# List length
LLEN queue:tasks
```

**Use Case: Activity Feed**

```bash
# Add activity (recent first)
LPUSH feed:user:1000 "User liked post 123"
LPUSH feed:user:1000 "User followed user 456"

# Get recent 10 activities
LRANGE feed:user:1000 0 9

# Trim to keep only 100 recent
LTRIM feed:user:1000 0 99
```

#### **2.4 Set (Unique Collection)**

**Use Cases:** Tags, unique visitors, followers, online users.

```bash
# Add to set
SADD tags:post:123 "redis" "database" "nosql"

# Check membership
SISMEMBER tags:post:123 "redis"  # 1 (true)

# Get all members
SMEMBERS tags:post:123  # ["redis", "database", "nosql"]

# Set operations
SADD following:1000 "user2" "user3"
SADD followers:1000 "user2" "user4"
SINTER following:1000 followers:1000  # Mutual friends: ["user2"]
```

**Use Case: Online Users**

```bash
# User comes online
SADD online:users "user1000"

# User goes offline
SREM online:users "user1000"

# Count online users
SCARD online:users
```

#### **2.5 Sorted Set (Leaderboard)**

**Use Cases:** Leaderboards, priority queues, time-series data.

```bash
# Add with score
ZADD leaderboard 100 "player1"
ZADD leaderboard 200 "player2"
ZADD leaderboard 150 "player3"

# Get top 10 (highest scores)
ZREVRANGE leaderboard 0 9 WITHSCORES
# 1. "player2" 200
# 2. "player3" 150
# 3. "player1" 100

# Get rank
ZREVRANK leaderboard "player1"  # 2 (0-indexed)

# Increment score
ZINCRBY leaderboard 50 "player1"  # Now 150
```

**Use Case: Trending Posts (Time-Decay)**

```bash
# Score = timestamp + engagement_score
ZADD trending 1706181600 "post123"  # Unix timestamp
ZADD trending 1706181700 "post456"

# Get posts from last hour
ZRANGEBYSCORE trending 1706178000 1706181600
```

#### **2.6 Bitmap**

**Use Cases:** Daily active users, feature flags, user permissions.

```bash
# Mark user as active on day 1
SETBIT active:2024-01-15 1000 1  # User 1000 active

# Check if user was active
GETBIT active:2024-01-15 1000  # 1 (true)

# Count active users
BITCOUNT active:2024-01-15
```

#### **2.7 HyperLogLog (Approximate Count)**

**Use Cases:** Unique visitor counting (memory-efficient).

```bash
# Add unique visitors
PFADD visitors:2024-01-15 "user1" "user2" "user3"
PFADD visitors:2024-01-15 "user1"  # Duplicate ignored

# Count unique visitors (approximate)
PFCOUNT visitors:2024-01-15  # 3 (error < 1%)
```

**Memory Efficiency:**

- **Exact count:** 1M unique users = ~16MB (set).
- **HyperLogLog:** 1M unique users = ~12KB (99%+ accuracy).

#### **2.8 Stream (Event Log)**

**Use Cases:** Event sourcing, activity logs, message queues (Kafka-like).

```bash
# Add event
XADD events:orders * user_id 1000 product_id 123 total 99.99
# Returns: "1706181600000-0" (timestamp-sequence)

# Read events
XREAD STREAMS events:orders 0  # From beginning

# Consumer groups (like Kafka)
XGROUP CREATE events:orders order-processor 0
XREADGROUP GROUP order-processor consumer1 STREAMS events:orders >
```

---

### 3. Persistence: RDB vs. AOF

Redis is in-memory, but can persist data to disk:

#### **3.1 RDB (Snapshots)**

**How:** Periodic snapshots of entire dataset.

```ini
# redis.conf
save 900 1    # Save if 1 key changed in 900 seconds
save 300 10   # Save if 10 keys changed in 300 seconds
save 60 10000 # Save if 10,000 keys changed in 60 seconds
```

**Pros:**

- âœ… **Compact:** Single file, easy to backup.
- âœ… **Fast recovery:** Loads entire snapshot at once.
- âœ… **Low overhead:** Snapshots run in background (fork).

**Cons:**

- âŒ **Data loss risk:** Lose data between snapshots (e.g., last 5 minutes).
- âŒ **Fork overhead:** Child process doubles memory usage temporarily.

#### **3.2 AOF (Append-Only File)**

**How:** Logs every write operation.

```ini
# redis.conf
appendonly yes
appendfsync everysec  # Sync to disk every second
```

**Fsync Options:**

| Mode | Behavior | Data Loss | Performance |
|------|----------|-----------|-------------|
| `no` | OS decides when to sync | Up to 30s | Fastest |
| `everysec` | Sync every second | Up to 1s | Good balance |
| `always` | Sync after every write | None | Slowest |

**Pros:**

- âœ… **Minimal data loss:** At most 1 second of data lost (with `everysec`).
- âœ… **Human-readable:** AOF file contains Redis commands.

**Cons:**

- âŒ **Larger files:** AOF files grow quickly.
- âŒ **Slower recovery:** Must replay all commands.

**AOF Rewrite:**

```bash
# Compact AOF file (background)
BGREWRITEAOF
```

**Recommendation:**

- Use **both RDB + AOF** for maximum durability.
- Use **RDB only** for caching (data loss OK).
- Use **AOF only** for critical data (session storage).

---

### 4. Replication: Master-Slave

Redis supports **asynchronous replication**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Redis Replication                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚  â”‚   Master     â”‚  (Read/Write)          â”‚
â”‚  â”‚   (Primary)  â”‚                        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚         â”‚ Async replication              â”‚
â”‚         â–¼                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Replica 1   â”‚     â”‚  Replica 2   â”‚  â”‚
â”‚  â”‚ (Read-only)  â”‚     â”‚ (Read-only)  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Configuration:**

```bash
# On replica
replicaof 192.168.1.100 6379  # Master IP and port
```

**Use Cases:**

- âœ… **Read scaling:** Offload reads to replicas.
- âœ… **High availability:** Promote replica to master on failure (manual or with Sentinel).
- âœ… **Backups:** Snapshot replicas without impacting master.

**Limitations:**

- âŒ **Eventual consistency:** Replica may lag behind master (milliseconds).
- âŒ **Manual failover:** No automatic promotion (use Redis Sentinel).

---

### 5. High Availability: Redis Sentinel

**Sentinel** provides automatic failover:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       Redis Sentinel (3 instances)         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚Sentinel 1â”‚  â”‚Sentinel 2â”‚  â”‚Sentinel 3â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â”‚
â”‚       â”‚             â”‚             â”‚        â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                     â”‚ Monitor              â”‚
â”‚                     â–¼                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚  Master â†’ Replica 1, Replica 2â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                             â”‚
â”‚  If Master fails:                           â”‚
â”‚  - Sentinels vote (quorum = 2)             â”‚
â”‚  - Promote Replica 1 to Master             â”‚
â”‚  - Notify clients of new Master            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Features:**

- âœ… **Monitoring:** Continuously checks if master is alive.
- âœ… **Automatic failover:** Promotes replica to master.
- âœ… **Notification:** Alerts on failover events.
- âœ… **Configuration provider:** Clients discover current master from Sentinel.

---

### 6. Scaling: Redis Cluster

**Redis Cluster** provides **horizontal scaling** (sharding):

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Redis Cluster (6 nodes)           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Master 1 (Slots 0-5460)                   â”‚
â”‚    â””â”€â”€ Replica 1                           â”‚
â”‚                                             â”‚
â”‚  Master 2 (Slots 5461-10922)               â”‚
â”‚    â””â”€â”€ Replica 2                           â”‚
â”‚                                             â”‚
â”‚  Master 3 (Slots 10923-16383)              â”‚
â”‚    â””â”€â”€ Replica 3                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Total: 16384 hash slots distributed across masters
```

**How Sharding Works:**

```bash
# Key: "user:1000"
Hash slot = CRC16("user:1000") mod 16384  # e.g., slot 5500
# Slot 5500 â†’ Master 2

# Client routes request to Master 2
GET user:1000
```

**Hash Tags (For Multi-Key Operations):**

```bash
# Problem: MGET keys from different slots (requires CROSSSLOT)
MGET user:1000 user:2000  # ERROR: Keys on different nodes

# Solution: Use hash tags {user} to force same slot
SET {user}:1000:name "John"
SET {user}:1000:email "john@example.com"
MGET {user}:1000:name {user}:1000:email  # OK (same slot)
```

**Cluster Limitations:**

- âŒ **No multi-key operations across slots:** `MGET`, `MSET` require keys on same slot.
- âŒ **No distributed transactions:** `MULTI/EXEC` works only on single node.
- âŒ **Resharding overhead:** Moving slots between nodes is manual and slow.

---

### 7. Pub/Sub: Message Broadcasting

Redis supports **publish-subscribe** messaging:

```bash
# Subscribe to channel
SUBSCRIBE news:tech

# Publish message (from another client)
PUBLISH news:tech "Redis 7.0 released!"

# Pattern subscription
PSUBSCRIBE news:*  # Matches news:tech, news:sports, etc.
```

**Use Cases:**

- âœ… **Real-time notifications:** Broadcast events to all subscribers.
- âœ… **Chat applications:** Send messages to all users in a channel.
- âœ… **Cache invalidation:** Notify all servers to clear cache.

**Limitations:**

- âŒ **Fire-and-forget:** Messages are lost if no subscribers.
- âŒ **No persistence:** Messages not stored (use Streams instead).
- âŒ **No acknowledgment:** Can't confirm delivery.

**Redis Streams vs. Pub/Sub:**

| Feature | Pub/Sub | Streams |
|---------|---------|---------|
| **Persistence** | No | Yes |
| **Acknowledgment** | No | Yes (consumer groups) |
| **Replay** | No | Yes (can read from any offset) |
| **Use Case** | Real-time broadcasting | Event sourcing, message queues |

---

### 8. Performance Best Practices

#### **8.1 Pipeline (Batch Commands)**

**Problem:** Network round-trip for each command.

```bash
# Without pipeline (3 round-trips)
SET user:1 "John"
SET user:2 "Jane"
SET user:3 "Bob"
```

**Solution:** Pipeline multiple commands.

```python
# Python example
pipe = redis.pipeline()
pipe.set('user:1', 'John')
pipe.set('user:2', 'Jane')
pipe.set('user:3', 'Bob')
pipe.execute()  # Single round-trip
```

**Performance:** 10x faster for bulk operations.

#### **8.2 Avoid KEYS Command**

**Problem:** `KEYS *` scans entire keyspace (blocks server).

```bash
# âŒ Bad: Blocks Redis
KEYS user:*  # Scans all keys
```

**Solution:** Use `SCAN` (cursor-based iteration).

```bash
# âœ… Good: Non-blocking
SCAN 0 MATCH user:* COUNT 100
```

#### **8.3 Use Appropriate Data Structures**

| Anti-Pattern | Fix |
|--------------|-----|
| Storing JSON strings (`SET user:1 "{...}"`) | Use Hash (`HSET user:1 name "John"`) |
| Storing list as JSON array | Use List (`LPUSH list:1 "item"`) |
| Storing set as comma-separated string | Use Set (`SADD set:1 "item"`) |

#### **8.4 Memory Optimization**

```bash
# Check memory usage
MEMORY USAGE user:1000

# Set maxmemory policy
CONFIG SET maxmemory 2gb
CONFIG SET maxmemory-policy allkeys-lru
```

**Eviction Policies:**

| Policy | Behavior |
|--------|----------|
| `noeviction` | Return errors when memory full |
| `allkeys-lru` | Evict least recently used keys |
| `volatile-lru` | Evict LRU keys with TTL |
| `allkeys-random` | Evict random keys |
| `volatile-ttl` | Evict keys with shortest TTL |

---

### 9. Monitoring

#### **Key Metrics:**

```bash
# General stats
INFO stats

# Memory usage
INFO memory

# Connected clients
INFO clients

# Replication lag
INFO replication

# Slow queries
SLOWLOG GET 10
```

**Critical Metrics:**

| Metric | Command | Threshold |
|--------|---------|-----------|
| **Hit rate** | `INFO stats` | > 80% |
| **Memory usage** | `INFO memory` | < 80% of maxmemory |
| **Connected clients** | `INFO clients` | < 10,000 |
| **Evicted keys** | `INFO stats` | Should be 0 (if cache) |
| **Blocked clients** | `INFO stats` | Should be 0 |

**Tools:**

- **redis-cli --stat:** Real-time stats.
- **RedisInsight:** GUI for monitoring.
- **Prometheus + redis_exporter:** Metrics collection.

---

### 10. When to Use Redis

#### **âœ… Use Redis When:**

1. **Caching** â€” Fast, in-memory cache layer (most common use case).
2. **Session storage** â€” Store user sessions (with TTL).
3. **Real-time analytics** â€” Counters, leaderboards, trending data.
4. **Rate limiting** â€” Token bucket, sliding window counters.
5. **Job queues** â€” Background task processing (with Lists or Streams).
6. **Pub/Sub messaging** â€” Real-time notifications, chat.
7. **Geospatial queries** â€” Location-based search (with geospatial indexes).

#### **âŒ Don't Use Redis When:**

1. **Primary database** â€” Data must survive server restarts (use PostgreSQL).
2. **Complex queries** â€” No joins, no SQL (use PostgreSQL/Elasticsearch).
3. **Large datasets (>RAM)** â€” Redis is in-memory (use disk-based DB).
4. **Strong consistency** â€” Redis replication is async (eventual consistency).
5. **Multi-key transactions across shards** â€” Not supported in Redis Cluster.

---

### 11. Real-World Examples

| Company | Use Case | Why Redis? |
|---------|----------|------------|
| **Twitter** | Timeline caching | Fast reads, millions of requests/sec |
| **GitHub** | Job queuing | Reliable task queue with Lists |
| **Stack Overflow** | Session storage | Fast, in-memory session management |
| **Uber** | Geospatial queries | Geospatial indexes for driver location |
| **Instagram** | Rate limiting | Token bucket with INCR |
| **Pinterest** | Leaderboard | Sorted sets for trending pins |

---

### 12. Redis vs. Other Databases

| Feature | Redis | Memcached | PostgreSQL | MongoDB |
|---------|-------|-----------|------------|---------|
| **Data Structures** | 10+ types | Strings only | Tables | Documents |
| **Persistence** | RDB + AOF | No | Disk | Disk |
| **Replication** | Master-slave | No | Streaming | Replica sets |
| **Transactions** | MULTI/EXEC | No | ACID | ACID (4.0+) |
| **Clustering** | Yes (16K slots) | Client-side | Limited | Sharding |
| **Use Case** | Cache, queue, analytics | Pure cache | Primary DB | Document store |

---

### 13. Trade-offs Summary

| What You Gain | What You Sacrifice |
|---------------|-------------------|
| âœ… Extremely fast (in-memory, <1ms latency) | âŒ Limited by RAM (expensive for large datasets) |
| âœ… Rich data structures (lists, sets, sorted sets) | âŒ No complex queries (no SQL, no joins) |
| âœ… Atomic operations (no locks needed) | âŒ Single-threaded (can't utilize multiple cores) |
| âœ… Simple to use and deploy | âŒ Not a primary database (eventual consistency) |
| âœ… Pub/Sub, Streams, Lua scripting | âŒ Cluster limitations (no cross-slot transactions) |

---

### 14. References

- **Redis Documentation:** [https://redis.io/documentation](https://redis.io/documentation)
- **Redis University (Free Courses):** [https://university.redis.com/](https://university.redis.com/)
- **Redis Best Practices:** [https://redis.io/docs/manual/patterns/](https://redis.io/docs/manual/patterns/)
- **Related Chapters:**
  - [2.2.1 Caching Deep Dive](./2.2.1-caching-deep-dive.md) â€” Caching strategies
  - [2.2.2 Consistent Hashing](./2.2.2-consistent-hashing.md) â€” How Redis Cluster distributes keys
  - [2.5.1 Rate Limiting Algorithms](./2.5.1-rate-limiting-algorithms.md) â€” Redis-based rate limiting
  - [2.5.3 Distributed Locking](./2.5.3-distributed-locking.md) â€” Redis locks (Redlock)


---

## âœï¸ Design Challenge

### Problem

You're building a **gaming leaderboard** for a mobile game with 10M players. Requirements:

1. **Real-time updates:** Scores updated every few seconds
2. **Top 100 global leaderboard:** Display top 100 players worldwide
3. **Player rank:** Show user's current rank (e.g., "You are rank #12,345")
4. **Nearby ranks:** Show 10 players above and below user
5. **Performance:** Leaderboard queries must be <10ms

**Question:** How would you design this using Redis? What data structure would you use, and how would you handle the massive write load (100K score updates/sec)?

### Solution

#### ðŸ§© Scenario

- **System:** Mobile game leaderboard
- **Scale:** 10M active players
- **Write load:** 100K score updates/sec
- **Query patterns:**
  - Top 100 global
  - User's rank
  - 10 players above/below user
- **Latency requirement:** <10ms

#### âœ… Goal

- Handle 100K writes/sec without performance degradation
- Real-time rank calculation
- Sub-10ms query latency
- Memory-efficient storage

#### âš™ï¸ Solution: Redis Sorted Set (Perfect Fit)

**Data Structure:**

```redis
# Sorted Set: leaderboard (score = player's points, member = player_id)
ZADD leaderboard 1500 player:123
ZADD leaderboard 2000 player:456
ZADD leaderboard 1800 player:789
```

**Why Sorted Set?**

| Requirement | Sorted Set Feature | Time Complexity |
|-------------|-------------------|-----------------|
| **Update score** | `ZADD` (upsert) | $\text{O}(\log n)$ |
| **Get top 100** | `ZREVRANGE 0 99` | $\text{O}(\log n + 100)$ |
| **Get rank** | `ZREVRANK` | $\text{O}(\log n)$ |
| **Get nearby players** | `ZREVRANGE` with rank Â± 10 | $\text{O}(\log n + 20)$ |
| **Memory** | Compressed skiplist | ~50 bytes/player |

**Implementation:**

```python
import redis

r = redis.Redis()

# 1. Update player score (100K/sec)
def update_score(player_id, points):
    r.zadd('leaderboard', {f'player:{player_id}': points})
    # O(log n) - very fast even with 10M players

# 2. Get top 100 players
def get_top_100():
    top_players = r.zrevrange('leaderboard', 0, 99, withscores=True)
    # Returns: [(b'player:456', 2000.0), (b'player:789', 1800.0), ...]
    return [
        {'player_id': p[0].decode(), 'score': int(p[1])}
        for p in top_players
    ]

# 3. Get player's rank
def get_player_rank(player_id):
    rank = r.zrevrank('leaderboard', f'player:{player_id}')
    # rank is 0-indexed, so rank 0 = #1 globally
    return rank + 1 if rank is not None else None

# 4. Get nearby players (10 above + 10 below)
def get_nearby_players(player_id):
    rank = r.zrevrank('leaderboard', f'player:{player_id}')
    if rank is None:
        return []
    
    # Get 10 above and 10 below (21 total including player)
    start = max(0, rank - 10)
    end = rank + 10
    
    nearby = r.zrevrange('leaderboard', start, end, withscores=True)
    return [
        {'player_id': p[0].decode(), 'score': int(p[1]), 'rank': start + i + 1}
        for i, p in enumerate(nearby)
    ]
```

#### âš ï¸ Scaling Challenges & Solutions

**Challenge 1: Write Throughput (100K writes/sec)**

**Problem:** Single Redis instance handles ~100K ops/sec, but leaderboard might hit limit.

**Solution: Horizontal Sharding (Multiple Leaderboards)**

```python
# Shard by region (US, EU, Asia)
def update_score_sharded(region, player_id, points):
    leaderboard_key = f'leaderboard:{region}'
    r.zadd(leaderboard_key, {f'player:{player_id}': points})

# Global leaderboard = merge top 100 from each region
def get_global_top_100():
    us_top = r.zrevrange('leaderboard:us', 0, 99, withscores=True)
    eu_top = r.zrevrange('leaderboard:eu', 0, 99, withscores=True)
    asia_top = r.zrevrange('leaderboard:asia', 0, 99, withscores=True)
    
    # Merge and sort (in application or use Redis Cluster)
    all_top = us_top + eu_top + asia_top
    all_top.sort(key=lambda x: x[1], reverse=True)
    return all_top[:100]
```

**Challenge 2: Memory Usage (10M players Ã— 50 bytes = 500MB)**

**Solution: Keep Only Active Players (Last 30 Days)**

```python
# Add timestamp to track last activity
def update_score_with_activity(player_id, points):
    pipeline = r.pipeline()
    pipeline.zadd('leaderboard', {f'player:{player_id}': points})
    pipeline.hset(f'activity:{player_id}', 'last_active', time.time())
    pipeline.execute()

# Cleanup inactive players (cron job)
def cleanup_inactive_players():
    cutoff = time.time() - (30 * 24 * 3600)  # 30 days ago
    
    for player_key in r.scan_iter('activity:*'):
        last_active = float(r.hget(player_key, 'last_active') or 0)
        if last_active < cutoff:
            player_id = player_key.decode().split(':')[1]
            r.zrem('leaderboard', f'player:{player_id}')
```

**Challenge 3: Persistence (Redis is in-memory)**

**Solution: Enable AOF (Append-Only File)**

```ini
# redis.conf
appendonly yes
appendfsync everysec  # Sync every second (balance durability vs. performance)
```

#### ðŸ§  Advanced: Real-Time Rank Updates (Push Notifications)

**Use Case:** Notify player when they enter top 100.

```python
# Redis Streams for real-time notifications
def update_score_with_notification(player_id, points):
    old_rank = r.zrevrank('leaderboard', f'player:{player_id}')
    r.zadd('leaderboard', {f'player:{player_id}': points})
    new_rank = r.zrevrank('leaderboard', f'player:{player_id}')
    
    # Entered top 100?
    if old_rank is not None and old_rank >= 100 and new_rank < 100:
        r.xadd('rank_updates', {
            'player_id': player_id,
            'event': 'entered_top_100',
            'rank': new_rank + 1
        })
```

#### âœ… Final Answer

| Aspect | Decision | Reason |
|--------|----------|--------|
| **Data Structure** | **Sorted Set (ZSET)** | O(log n) updates, O(log n) rank queries, perfect for leaderboards |
| **Write Performance** | 100K+ writes/sec (single instance) | Redis is in-memory, extremely fast |
| **Query Latency** | <1ms for rank, <5ms for top 100 | O(log n) complexity, in-memory speed |
| **Memory** | ~500MB for 10M players | 50 bytes/player (compressed skiplist) |
| **Persistence** | AOF with everysec fsync | Balance durability vs. performance |
| **Sharding** | Shard by region if >100K writes/sec | Horizontal scaling |
| **Cleanup** | Remove inactive players (>30 days) | Keep memory usage bounded |

**Performance Metrics:**
- **Update score:** <1ms (O(log n))
- **Get rank:** <1ms (O(log n))
- **Top 100:** <5ms (O(log n + 100))
- **Nearby players:** <5ms (O(log n + 20))
- **Memory:** 500MB (10M players)
- **Throughput:** 100K+ writes/sec (single instance)

**Why NOT Other Solutions:**
- âŒ PostgreSQL: Too slow for 100K writes/sec, complex rank calculation
- âŒ DynamoDB: Expensive for frequent updates, no native sorted set
- âŒ Cassandra: Overkill, no native rank queries

