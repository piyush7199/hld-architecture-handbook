# 2.1.12 MySQL Deep Dive: The World's Most Popular Open-Source Database

## Intuitive Explanation

MySQL is the **most widely deployed open-source relational database** in the world. It powers millions of websites and applications, from small blogs to enterprise giants like Facebook, YouTube, and Twitter. While PostgreSQL is often praised for advanced features, MySQL shines in **simplicity, speed, and battle-tested reliability**. It's the default choice for web applications, especially in the LAMP stack (Linux, Apache, MySQL, PHP).

- **Storage Engines:** Pluggable architecture (InnoDB for transactions, MyISAM for read-heavy).
- **Replication:** Master-slave, master-master, Group Replication.
- **Performance:** Optimized for read-heavy workloads, extremely fast on simple queries.
- **Use Cases:** Web applications, e-commerce, content management systems, analytics dashboards.

---

## In-Depth Analysis

### 1. Storage Engines: InnoDB vs. MyISAM

MySQL's unique feature is **pluggable storage engines** — you can choose how data is stored:

| Feature | InnoDB (Default) | MyISAM (Legacy) |
|---------|------------------|-----------------|
| **Transactions** | ✅ ACID support | ❌ No transactions |
| **Foreign Keys** | ✅ Supported | ❌ Not supported |
| **Row-Level Locking** | ✅ Yes (high concurrency) | ❌ Table-level locks |
| **Crash Recovery** | ✅ Auto-recovery | ❌ Manual repair |
| **Full-Text Search** | ✅ Yes (5.6+) | ✅ Yes |
| **Use Case** | Modern apps (99% use case) | Legacy read-heavy (deprecated) |

**InnoDB Architecture:**

```
┌──────────────────────────────────────────────┐
│              InnoDB Storage Engine            │
├──────────────────────────────────────────────┤
│  ┌────────────────────────────────┐          │
│  │  Buffer Pool (In-Memory)       │          │
│  │  - Data Pages                  │          │
│  │  - Index Pages                 │          │
│  │  - Change Buffer               │          │
│  └────────────────────────────────┘          │
│           │                                   │
│           ▼                                   │
│  ┌────────────────────────────────┐          │
│  │  Redo Log (Durability)         │          │
│  │  - Circular buffer             │          │
│  │  - Crash recovery              │          │
│  └────────────────────────────────┘          │
│           │                                   │
│           ▼                                   │
│  ┌────────────────────────────────┐          │
│  │  Data Files (.ibd)             │          │
│  │  - Tablespaces                 │          │
│  │  - Undo Logs                   │          │
│  └────────────────────────────────┘          │
└──────────────────────────────────────────────┘
```

**Recommendation:** Always use **InnoDB** (default since MySQL 5.5).

---

### 2. MVCC: Multi-Version Concurrency Control

Like PostgreSQL, InnoDB uses **MVCC** for high concurrency:

**How It Works:**

1. Each transaction sees a snapshot of the database.
2. When a row is updated, InnoDB keeps **old versions** in the **undo log**.
3. Readers don't block writers (and vice versa).

**Example:**

```sql
-- Transaction 1: Read
START TRANSACTION;
SELECT balance FROM accounts WHERE id = 1;  -- Sees balance = 100

-- Transaction 2: Update (creates new version)
START TRANSACTION;
UPDATE accounts SET balance = 150 WHERE id = 1;
COMMIT;

-- Transaction 1: Read again
SELECT balance FROM accounts WHERE id = 1;  -- Still sees 100 (repeatable read)
COMMIT;
```

**Undo Log:**

- Stores old row versions for MVCC.
- Purged by background threads when no transactions need old versions.
- If not purged fast enough, **undo log grows** (can cause performance issues).

---

### 3. Replication Strategies

MySQL supports multiple replication modes:

#### **3.1 Asynchronous Replication (Traditional)**

```
┌─────────────────────────────────────────┐
│        Master (Primary)                 │
│        - Writes to Binary Log           │
└────────────┬────────────────────────────┘
             │
             │ Async replication (binlog)
             ▼
┌─────────────────────────────────────────┐
│        Replica 1 (Slave)                │
│        - Reads binlog from Master       │
│        - Applies changes                │
└─────────────────────────────────────────┘
             │
             ▼
┌─────────────────────────────────────────┐
│        Replica 2 (Slave)                │
└─────────────────────────────────────────┘
```

**Binary Log Formats:**

| Format | How It Works | Pros | Cons |
|--------|-------------|------|------|
| **Statement-Based (SBR)** | Logs SQL statements | Small log size | Non-deterministic functions (NOW(), RAND()) cause inconsistency |
| **Row-Based (RBR)** | Logs actual row changes | Deterministic, accurate | Larger log size |
| **Mixed** | Switches between SBR and RBR | Balanced | Complex |

**Recommendation:** Use **Row-Based Replication (RBR)**.

**Replication Lag:**

- **Problem:** Replica may lag behind master (seconds to minutes).
- **Check Lag:**

```sql
SHOW SLAVE STATUS\G
-- Look at: Seconds_Behind_Master
```

- **Mitigation:**
  - Use faster hardware for replicas.
  - Reduce write load on master.
  - Use semi-synchronous replication.

#### **3.2 Semi-Synchronous Replication**

- **How:** Master waits for **at least one replica** to acknowledge before committing.
- **Benefit:** No data loss on failover (replica is up-to-date).
- **Trade-off:** Slower writes (network round-trip).

```sql
-- Enable on master
INSTALL PLUGIN rpl_semi_sync_master SONAME 'semisync_master.so';
SET GLOBAL rpl_semi_sync_master_enabled = 1;

-- Enable on replica
INSTALL PLUGIN rpl_semi_sync_slave SONAME 'semisync_slave.so';
SET GLOBAL rpl_semi_sync_slave_enabled = 1;
```

#### **3.3 Group Replication (MySQL 5.7+)**

- **How:** Multi-master replication with **Paxos-based consensus**.
- **Benefit:** Automatic failover, no data loss.
- **Use Case:** High availability clusters (like PostgreSQL's Patroni).

```
┌────────────────────────────────────────┐
│      Group Replication (3 nodes)      │
├────────────────────────────────────────┤
│  ┌──────────┐  ┌──────────┐  ┌──────┐ │
│  │  Node 1  │  │  Node 2  │  │ Node │ │
│  │ (Primary)│  │(Secondary│  │  3   │ │
│  └────┬─────┘  └────┬─────┘  └──┬───┘ │
│       └─────────────┴───────────┘      │
│         Paxos consensus (quorum = 2)   │
└────────────────────────────────────────┘
```

---

### 4. Indexing Strategies

#### **4.1 B+Tree Indexes (Default)**

MySQL InnoDB uses **B+Tree indexes**:

```
                  [Root]
                 /      \
            [Branch]    [Branch]
            /    \       /    \
        [Leaf] [Leaf] [Leaf] [Leaf]
         (Data) (Data) (Data) (Data)
```

**Clustered Index (Primary Key):**

- **InnoDB stores data IN the primary key index** (no separate table).
- Rows are physically sorted by primary key.

```sql
CREATE TABLE users (
    id INT PRIMARY KEY,
    email VARCHAR(255),
    name VARCHAR(100)
);

-- Clustered index (id) stores entire row
-- Secondary indexes (email) store (email, id) → lookup primary key
```

**Secondary Indexes:**

- **Structure:** Stores (indexed_column, primary_key).
- **Lookup:** Two steps: (1) Find primary key in secondary index, (2) Lookup row in clustered index.

**Example:**

```sql
CREATE INDEX idx_email ON users(email);

-- Query
SELECT * FROM users WHERE email = 'john@example.com';

-- Step 1: Lookup email in idx_email → finds id = 123
-- Step 2: Lookup id = 123 in primary key index → gets row
```

**Covering Index (Optimization):**

```sql
-- Query only needs email + name (not all columns)
SELECT name FROM users WHERE email = 'john@example.com';

-- Create covering index (includes name)
CREATE INDEX idx_email_name ON users(email, name);

-- Now query can be answered from index alone (no table lookup)
```

#### **4.2 Full-Text Indexes**

```sql
-- Create full-text index
CREATE FULLTEXT INDEX idx_body ON articles(body);

-- Full-text search
SELECT * FROM articles
WHERE MATCH(body) AGAINST('mysql performance');

-- Boolean mode (AND, OR, NOT)
SELECT * FROM articles
WHERE MATCH(body) AGAINST('+mysql -postgres' IN BOOLEAN MODE);
```

**Limitations:**

- Only on **InnoDB** (MySQL 5.6+) and **MyISAM**.
- Not as powerful as Elasticsearch (no typo tolerance, limited ranking).

#### **4.3 Composite Indexes (Best Practices)**

**Leftmost Prefix Rule:**

```sql
-- Index
CREATE INDEX idx_abc ON users(a, b, c);

-- ✅ Uses index
WHERE a = 1
WHERE a = 1 AND b = 2
WHERE a = 1 AND b = 2 AND c = 3

-- ❌ Does NOT use index
WHERE b = 2
WHERE c = 3
WHERE b = 2 AND c = 3
```

**Index Cardinality:**

```sql
-- Check index statistics
SHOW INDEX FROM users;

-- Look at 'Cardinality' column
-- High cardinality = good (many unique values)
-- Low cardinality = bad (few unique values, e.g., status column)
```

---

### 5. Query Optimization

#### **5.1 EXPLAIN Plan**

```sql
EXPLAIN SELECT * FROM orders
WHERE user_id = 123 AND status = 'shipped'
ORDER BY created_at DESC;
```

**Key Columns:**

| Column | Meaning | Good Value |
|--------|---------|------------|
| `type` | Join type | `const`, `eq_ref`, `ref` (✅), `ALL` (❌) |
| `key` | Index used | Should show index name (not `NULL`) |
| `rows` | Estimated rows scanned | Lower is better |
| `Extra` | Additional info | `Using index` (✅), `Using filesort` (⚠️) |

#### **5.2 Common Issues**

**Issue 1: Full Table Scan**

```sql
-- ❌ Bad: No index on email
EXPLAIN SELECT * FROM users WHERE email = 'john@example.com';
-- type: ALL (full scan)

-- ✅ Fix: Add index
CREATE INDEX idx_email ON users(email);
```

**Issue 2: Index Not Used (Type Mismatch)**

```sql
-- ❌ Bad: email is VARCHAR, but querying with INT
SELECT * FROM users WHERE email = 123;  -- Doesn't use index

-- ✅ Fix: Use correct type
SELECT * FROM users WHERE email = '123';
```

**Issue 3: Function on Indexed Column**

```sql
-- ❌ Bad: Function prevents index use
SELECT * FROM users WHERE LOWER(email) = 'john@example.com';

-- ✅ Fix: Store lowercase email or use generated column
ALTER TABLE users ADD email_lower VARCHAR(255) AS (LOWER(email)) STORED;
CREATE INDEX idx_email_lower ON users(email_lower);
```

#### **5.3 Pagination Optimization**

**Problem: OFFSET is slow on large datasets**

```sql
-- ❌ Bad: Scans 1,000,000 rows to skip them
SELECT * FROM orders
ORDER BY created_at DESC
LIMIT 10 OFFSET 1000000;  -- Slow!
```

**Solution: Keyset Pagination (Seek Method)**

```sql
-- ✅ Good: Uses index, no offset
SELECT * FROM orders
WHERE created_at < '2024-01-15 10:30:00'
ORDER BY created_at DESC
LIMIT 10;
```

---

### 6. Performance Tuning

#### **6.1 Configuration (my.cnf)**

**Critical Parameters:**

| Parameter | Default | Recommended | Purpose |
|-----------|---------|-------------|---------|
| `innodb_buffer_pool_size` | 128MB | 50-75% of RAM | In-memory cache for data pages |
| `innodb_log_file_size` | 48MB | 512MB-2GB | Redo log size (larger = better write performance) |
| `max_connections` | 151 | 500-1000 | Max concurrent connections |
| `innodb_flush_log_at_trx_commit` | 1 | 1 (ACID) or 2 (performance) | Durability vs. speed |
| `query_cache_type` | OFF | OFF | Deprecated in MySQL 8.0 (use external cache like Redis) |

**Flush Log Modes:**

| Mode | Behavior | Durability | Performance |
|------|----------|------------|-------------|
| `0` | Flush every second | ❌ Risk of 1s data loss | ⚡ Fastest |
| `1` | Flush on every commit | ✅ ACID | ⏱️ Slowest |
| `2` | Flush to OS buffer on commit | ⚠️ Risk on OS crash | ⚡ Fast |

#### **6.2 Monitoring**

**Key Metrics:**

```sql
-- Connections
SHOW STATUS LIKE 'Threads_connected';
SHOW STATUS LIKE 'Max_used_connections';

-- Buffer pool hit rate
SHOW STATUS LIKE 'Innodb_buffer_pool_reads';
SHOW STATUS LIKE 'Innodb_buffer_pool_read_requests';
-- Hit rate = 1 - (reads / read_requests)  -- Should be > 99%

-- Slow queries
SHOW STATUS LIKE 'Slow_queries';

-- InnoDB metrics
SHOW ENGINE INNODB STATUS\G
```

**Slow Query Log:**

```ini
# my.cnf
slow_query_log = 1
slow_query_log_file = /var/log/mysql/slow.log
long_query_time = 1  # Log queries > 1 second
```

**Tools:**

- **mysqldumpslow:** Analyze slow query log.
- **pt-query-digest (Percona Toolkit):** Advanced query analysis.
- **Prometheus + mysqld_exporter:** Metrics collection.
- **Grafana:** Dashboards.

---

### 7. High Availability: ProxySQL & Orchestrator

**ProxySQL:** Query routing and load balancing.

```
┌────────────────────────────────────────┐
│             Application                │
└────────────┬───────────────────────────┘
             │
             ▼
┌────────────────────────────────────────┐
│           ProxySQL                     │
│  - Read/Write Split                    │
│  - Query Caching                       │
│  - Connection Pooling                  │
└────────────┬───────────────────────────┘
             │
    ┌────────┼────────┐
    ▼        ▼        ▼
┌─────────┐ ┌─────────┐ ┌─────────┐
│ Master  │ │Replica 1│ │Replica 2│
└─────────┘ └─────────┘ └─────────┘
```

**Orchestrator:** Automated failover.

- Monitors topology (master + replicas).
- Promotes replica on master failure.
- Reconfigures replication chain.

---

### 8. Partitioning

**Range Partitioning:**

```sql
CREATE TABLE orders (
    id INT,
    user_id INT,
    created_at DATE,
    total DECIMAL(10,2)
)
PARTITION BY RANGE (YEAR(created_at)) (
    PARTITION p2022 VALUES LESS THAN (2023),
    PARTITION p2023 VALUES LESS THAN (2024),
    PARTITION p2024 VALUES LESS THAN (2025)
);
```

**Benefits:**

- ✅ **Query pruning:** Only scans relevant partitions.
- ✅ **Fast deletes:** Drop old partitions (e.g., `ALTER TABLE orders DROP PARTITION p2022`).

---

### 9. When to Use MySQL

#### **✅ Use MySQL When:**

1. **Web applications** — LAMP/LEMP stack (WordPress, Drupal, Magento).
2. **Read-heavy workloads** — Blogs, content sites, e-commerce.
3. **Simplicity** — Easier to learn and deploy than PostgreSQL.
4. **Wide ecosystem** — Most hosting providers support MySQL.
5. **Replication** — Built-in master-slave replication.
6. **Cost** — Free and open-source.

#### **❌ Don't Use MySQL When:**

1. **Complex queries** — PostgreSQL has better query planner.
2. **JSONB** — PostgreSQL's JSONB is faster than MySQL's JSON.
3. **Advanced features** — Window functions, CTEs, partial indexes (PostgreSQL better).
4. **Geographic data** — PostGIS is more mature than MySQL spatial.

---

### 10. Real-World Examples

| Company | Use Case | Why MySQL? |
|---------|----------|------------|
| **Facebook** | User data, messages (sharded) | Custom MySQL (sharded across thousands of servers) |
| **YouTube** | Video metadata | Simple schema, read-heavy |
| **Twitter** | Tweets, users | MySQL + Redis for timeline |
| **GitHub** | Git metadata | MySQL + Redis for caching |
| **Shopify** | E-commerce data | MySQL + ProxySQL for HA |

---

### 11. MySQL vs. PostgreSQL

| Feature | MySQL | PostgreSQL |
|---------|-------|------------|
| **Performance** | ⚡ Faster on simple queries | Faster on complex queries |
| **ACID** | ✅ InnoDB only | ✅ All tables |
| **JSON** | ✅ JSON (slower) | ✅ JSONB (faster) |
| **Full-Text Search** | ✅ Basic | ✅ Advanced (tsvector) |
| **Replication** | ✅ Built-in | ✅ Streaming replication |
| **Community** | Larger | Smaller but very active |
| **Use Case** | Web apps, read-heavy | Complex queries, analytics |

---

### 12. Trade-offs Summary

| What You Gain | What You Sacrifice |
|---------------|-------------------|
| ✅ Simple, easy to learn | ❌ Fewer advanced features (vs. PostgreSQL) |
| ✅ Fast on simple queries | ❌ Slower on complex queries |
| ✅ Wide ecosystem, hosting support | ❌ JSON support not as good |
| ✅ Battle-tested reliability | ❌ Some features require InnoDB (not all engines) |
| ✅ Master-slave replication (built-in) | ❌ Replication lag (async) |

---

### 13. References

- **MySQL Documentation:** [https://dev.mysql.com/doc/](https://dev.mysql.com/doc/)
- **Percona (MySQL Performance Experts):** [https://www.percona.com/](https://www.percona.com/)
- **ProxySQL:** [https://proxysql.com/](https://proxysql.com/)
- **Orchestrator:** [https://github.com/openark/orchestrator](https://github.com/openark/orchestrator)
- **Related Chapters:**
  - [2.1.1 RDBMS Deep Dive](./2.1.1-rdbms-deep-dive.md) — Core SQL concepts
  - [2.1.7 PostgreSQL Deep Dive](./2.1.7-postgresql-deep-dive.md) — MySQL vs. PostgreSQL
  - [2.1.4 Database Scaling](./2.1.4-database-scaling.md) — Replication and sharding
  - [2.1.5 Indexing and Query Optimization](./2.1.5-indexing-and-query-optimization.md) — Index strategies


---

## ✏️ Design Challenge

### Problem

You're designing an **e-commerce order management system** with the following requirements:

1. **ACID transactions:** Order creation must be atomic (deduct inventory, create order, charge payment)
2. **High read load:** 80% reads (order history), 20% writes (new orders)
3. **Complex queries:** JOIN orders with products, users, payments
4. **Scale:** 1M orders/day, 100K products, 10M users
5. **Replication:** Read replicas for scaling reads

**Question:** How would you design the MySQL schema? What indexes would you create? How would you handle read scaling with replication, and what consistency level would you use for reads?

### Solution

#### 🧩 Scenario

- **System:** E-commerce order management
- **Write load:** 12 orders/sec (1M/day)
- **Read load:** 48 queries/sec (4x reads vs. writes)
- **Transactions:** Order creation = deduct inventory + create order + log payment (ACID required)
- **Query patterns:**
  - Order history by user
  - Product inventory lookup
  - Daily sales reports (SUM, AVG)

#### ✅ Goal

- Strong consistency for inventory (prevent overselling)
- ACID transactions for order creation
- Scale reads with replicas
- Efficient queries (proper indexes)
- Handle 1M orders/day growth

#### ⚙️ Solution: MySQL with Read Replicas + Proper Indexing

**Schema Design:**

```sql
-- Users table
CREATE TABLE users (
    id BIGINT PRIMARY KEY AUTO_INCREMENT,
    email VARCHAR(255) UNIQUE NOT NULL,
    name VARCHAR(100) NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    INDEX idx_email (email)
) ENGINE=InnoDB;

-- Products table
CREATE TABLE products (
    id BIGINT PRIMARY KEY AUTO_INCREMENT,
    name VARCHAR(255) NOT NULL,
    price DECIMAL(10,2) NOT NULL,
    stock INT NOT NULL DEFAULT 0,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    INDEX idx_stock (stock),  -- For low-stock alerts
    INDEX idx_name (name)     -- For product search
) ENGINE=InnoDB;

-- Orders table (master data)
CREATE TABLE orders (
    id BIGINT PRIMARY KEY AUTO_INCREMENT,
    user_id BIGINT NOT NULL,
    total_amount DECIMAL(10,2) NOT NULL,
    status ENUM('pending', 'paid', 'shipped', 'delivered', 'cancelled') NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (user_id) REFERENCES users(id),
    INDEX idx_user_created (user_id, created_at),  -- For user order history
    INDEX idx_status_created (status, created_at)  -- For admin dashboard
) ENGINE=InnoDB;

-- Order items (details of what was ordered)
CREATE TABLE order_items (
    id BIGINT PRIMARY KEY AUTO_INCREMENT,
    order_id BIGINT NOT NULL,
    product_id BIGINT NOT NULL,
    quantity INT NOT NULL,
    price_at_purchase DECIMAL(10,2) NOT NULL,  -- Snapshot price
    FOREIGN KEY (order_id) REFERENCES orders(id),
    FOREIGN KEY (product_id) REFERENCES products(id),
    INDEX idx_order (order_id),
    INDEX idx_product (product_id)
) ENGINE=InnoDB;

-- Payments table
CREATE TABLE payments (
    id BIGINT PRIMARY KEY AUTO_INCREMENT,
    order_id BIGINT NOT NULL,
    amount DECIMAL(10,2) NOT NULL,
    status ENUM('pending', 'completed', 'failed', 'refunded') NOT NULL,
    payment_method VARCHAR(50) NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (order_id) REFERENCES orders(id),
    INDEX idx_order (order_id),
    INDEX idx_status (status)
) ENGINE=InnoDB;
```

**ACID Transaction: Create Order**

```sql
START TRANSACTION;

-- 1. Check inventory (lock row to prevent overselling)
SELECT stock FROM products WHERE id = 123 FOR UPDATE;

-- 2. If stock sufficient, deduct inventory
UPDATE products SET stock = stock - 2 WHERE id = 123 AND stock >= 2;

-- 3. Create order
INSERT INTO orders (user_id, total_amount, status)
VALUES (1000, 99.99, 'pending');

SET @order_id = LAST_INSERT_ID();

-- 4. Create order items
INSERT INTO order_items (order_id, product_id, quantity, price_at_purchase)
VALUES (@order_id, 123, 2, 49.99);

-- 5. Log payment attempt
INSERT INTO payments (order_id, amount, status, payment_method)
VALUES (@order_id, 99.99, 'pending', 'credit_card');

-- 6. If all succeeds, commit (otherwise rollback)
COMMIT;
```

**Rollback on Failure:**

```sql
-- If payment fails or inventory insufficient, rollback entire transaction
ROLLBACK;
```

#### ⚠️ Read Scaling: Master-Slave Replication

**Architecture:**

```
┌─────────────────────────────────────┐
│    Application Layer                │
├─────────────────────────────────────┤
│  Write Queries  →  Master (Primary) │
│  Read Queries   →  Replicas (Slaves)│
└─────────────────────────────────────┘
         │                  │
         ▼                  ▼
┌──────────────┐    ┌──────────────┐
│   Master     │───>│  Replica 1   │
│  (Read/Write)│    │  (Read-only) │
└──────────────┘    └──────────────┘
                           │
                           ▼
                    ┌──────────────┐
                    │  Replica 2   │
                    │  (Read-only) │
                    └──────────────┘
```

**Read/Write Splitting in Application:**

```python
import pymysql

# Connection pools
master_pool = pymysql.connect(host='master.db.example.com', ...)
replica_pool = pymysql.connect(host='replica.db.example.com', ...)

def create_order(user_id, items):
    # ALWAYS use master for writes
    with master_pool.cursor() as cursor:
        cursor.execute("START TRANSACTION")
        try:
            # ACID transaction
            for item in items:
                cursor.execute(
                    "UPDATE products SET stock = stock - %s WHERE id = %s AND stock >= %s",
                    (item['quantity'], item['product_id'], item['quantity'])
                )
            cursor.execute(
                "INSERT INTO orders (user_id, total_amount, status) VALUES (%s, %s, 'pending')",
                (user_id, calculate_total(items))
            )
            cursor.execute("COMMIT")
            return {"success": True}
        except Exception as e:
            cursor.execute("ROLLBACK")
            return {"success": False, "error": str(e)}

def get_order_history(user_id):
    # Use replica for reads (scale reads horizontally)
    with replica_pool.cursor() as cursor:
        cursor.execute("""
            SELECT o.id, o.total_amount, o.status, o.created_at
            FROM orders o
            WHERE o.user_id = %s
            ORDER BY o.created_at DESC
            LIMIT 20
        """, (user_id,))
        return cursor.fetchall()

def get_product_inventory(product_id):
    # Critical: Check inventory on MASTER (avoid replication lag)
    with master_pool.cursor() as cursor:
        cursor.execute("SELECT stock FROM products WHERE id = %s", (product_id,))
        return cursor.fetchone()['stock']
```

**Critical: Inventory Check Must Use Master**

**Problem:** If you check inventory on replica, replication lag might show stale stock → overselling!

```python
# ❌ BAD: Check inventory on replica (replication lag = overselling)
stock = replica.query("SELECT stock FROM products WHERE id = 123")

# ✅ GOOD: Check inventory on master (ACID consistency)
stock = master.query("SELECT stock FROM products WHERE id = 123 FOR UPDATE")
```

#### 🧠 Query Optimization

**Query 1: User Order History (Optimized)**

```sql
-- ✅ Good: Uses composite index (user_id, created_at)
SELECT o.id, o.total_amount, o.status, o.created_at
FROM orders o
WHERE o.user_id = 1000
ORDER BY o.created_at DESC
LIMIT 20;

-- Index used: idx_user_created (user_id, created_at)
```

**Query 2: Order Details with Products (JOIN)**

```sql
-- ✅ Good: Proper indexes on foreign keys
SELECT 
    o.id,
    o.total_amount,
    oi.product_id,
    p.name AS product_name,
    oi.quantity,
    oi.price_at_purchase
FROM orders o
JOIN order_items oi ON o.id = oi.order_id
JOIN products p ON oi.product_id = p.id
WHERE o.user_id = 1000
ORDER BY o.created_at DESC
LIMIT 10;

-- Indexes used:
-- - orders.idx_user_created
-- - order_items.idx_order
-- - products PRIMARY KEY
```

**Query 3: Daily Sales Report**

```sql
-- ✅ Good: Aggregation with index
SELECT 
    DATE(created_at) AS sale_date,
    COUNT(*) AS order_count,
    SUM(total_amount) AS total_revenue,
    AVG(total_amount) AS avg_order_value
FROM orders
WHERE status = 'paid'
  AND created_at >= '2024-01-01'
  AND created_at < '2024-02-01'
GROUP BY DATE(created_at)
ORDER BY sale_date DESC;

-- Index used: idx_status_created (status, created_at)
```

#### ✅ Final Answer

| Aspect | Decision | Reason |
|--------|----------|--------|
| **Database** | **MySQL (InnoDB)** | ACID transactions for order creation, strong consistency |
| **Replication** | Master + 2 read replicas | Scale reads (80% of traffic), handle 48 queries/sec easily |
| **Write Strategy** | All writes to master | ACID consistency, prevent overselling |
| **Read Strategy** | Reads from replicas (except inventory) | Horizontal read scaling |
| **Inventory Check** | **Always query master** | Prevent replication lag → overselling |
| **Transaction Isolation** | READ COMMITTED (default) | Balance consistency and performance |
| **Indexes** | Composite indexes on (user_id, created_at), foreign keys | Fast queries, support JOINs |
| **Trade-off** | Replication lag (1-2s) for non-critical reads | Gain: Read scaling, lower master load |

**Performance Metrics:**
- **Order creation:** 50-100ms (ACID transaction on master)
- **Order history query:** 10-20ms (replica, indexed)
- **Replication lag:** 1-2 seconds (acceptable for order history)
- **Throughput:** 12 writes/sec (master), 48 reads/sec (replicas)

**When to Scale Further:**
- If writes > 1,000/sec → Consider sharding by user_id
- If reads > 10,000/sec → Add more read replicas (horizontal scaling)
- If complex analytics needed → Use data warehouse (Redshift, BigQuery)

