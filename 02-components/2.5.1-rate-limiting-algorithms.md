# 2.5.1 Rate Limiting Algorithms: Protecting Your API

## Intuitive Explanation

Rate Limiting is a defense mechanism that restricts the number of $\text{API}$ requests a user or client can make in a
given time period (e.g., $10$ requests per second).

- **Why do we use it?** To prevent denial-of-service ($\text{DoS}$) attacks, block abusive clients, prevent billing
  overages, and ensure fair usage of resources among all users.

---

## In-Depth Analysis

### 1. Algorithms for Rate Limiting

Rate limits are typically enforced by checking a counter in a fast data store like $\text{Redis}$ before allowing the
request to proceed.

| Algorithm              | How it Works                                                                                                                                               | Pros                                                                                                                | Cons                                                                                                                                    |
|------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------|
| Fixed Window Counter   | A counter is reset at the start of a fixed time window (e.g., exactly on the hour, every minute).                                                          | Extremely simple to implement and understand.                                                                       | **Allows a "Double Burst**" at the edge of the window (e.g., a user can make $10$ requests at $1:59:59$ and another $10$ at $2:00:01$). |
| Sliding Log            | The system stores a timestamp for every request made by the user. Requests outside the time window are discarded.                                          | **Highly accurate;** prevents the double-burst issue.                                                               | **High Storage Cost** and $\text{CPU}$ overhead, as checking the limit requires iterating over potentially thousands of timestamps.     |
| Sliding Window Counter | An improvement over Fixed Window that approximates the Sliding Log. It combines the current window counter with a weighted count from the previous window. | Good balance between accuracy and performance; uses much less storage than the Sliding Log.                         | It is an approximation and can still be slightly inaccurate for bursty traffic.                                                         |
| Token Bucket           | Tokens are added to a "bucket" at a fixed rate. Each request consumes one token. If the bucket is empty, the request is denied.                            | Allows short bursts (if the bucket has tokens), which is useful for user experience. Simple to implement in memory. | Choosing the correct bucket size and refill rate can be complex.                                                                        |
| Leaky Bucket           | Requests are processed at a fixed rate, like water leaking out of a bucket. If the bucket overflows, new requests are dropped.                             | Smooths traffic spikes; excellent for protecting downstream services from uneven load.                              | Requests are treated as $\text{FIFO}$, and there's no way to prioritize traffic.                                                        |

### 2. Where to Enforce the Limit

The limit should be enforced as close to the client as possible to protect the inner system.

- **API Gateway/Load Balancer:** Best place for enforcement ($\text{Edge}$ $\text{Rate}$ $\text{Limiting}$). This
  prevents unauthenticated or abusive traffic from even hitting your microservices.
- **Application Service:** Used for specific, business-logic limits (e.g., "A user can only create $5$ posts per
  minute").

### 3. Distributed Rate Limiting

When requests are hitting many different $\text{API}$ gateway instances, the counter must be shared.

- **Implementation:** Use a centralized, low-latency store (like a $\text{Redis}$ cluster) to maintain the counters
  across all gateway instances. $\text{Redis}$ provides atomic $\text{INCR}$ operations, which are essential to prevent
  race conditions when two gateways check the limit simultaneously.

### Key Concepts / Tradeoffs

| Feature             | Description                                                                                                                                                                 | Trade-off / Scalability Issue                                                                            |
|---------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------|
| Atomic Counters     | Using $\text{Redis}$'s atomic `INCR` operation ensures that even if $100$ requests arrive simultaneously, the counter update is accurate, preventing overselling the limit. | Adds a network hop and reliance on a centralized $\text{Redis}$ cluster, which must be highly available. |
| Hard vs Soft Limits | Hard limits deny all requests; soft limits may slow down or degrade the service but don't outright fail the request.                                                        | Soft limits provide a better $\text{UX}$ but are harder to manage and implement.                         |

---

## ‚úèÔ∏è Design Challenge

### Problem

You are protecting a public $\text{API}$ endpoint used by external partners, limiting them to $1,000$ requests per hour.
Which rate limiting algorithm (Fixed Window or Token Bucket) would you choose if your primary goal is to allow partners
to handle short, heavy bursts of traffic occasionally without hitting the limit immediately? Justify your choice.

### Solution

#### üß© Scenario Summary

| Parameter           | Description                                            |
|---------------------|--------------------------------------------------------|
| **API Type**        | Public (external partners)                             |
| **Limit**           | 1,000 requests/hour                                    |
| **Traffic Pattern** | Occasional short bursts (e.g., batch uploads or syncs) |
| **Goal**            | Smooth burst handling without unfair overuse           |

#### ‚úÖ Step 1: Fixed Window ‚Äî How It Works

- The time is divided into fixed intervals (e.g., 1 hour = 0‚Äì59 min, 60‚Äì119 min, etc.).
- Each user can make up to 1,000 requests in the current window.
- When the window resets, the counter resets to zero.

**‚ùå Problem with Fixed Window**

If a partner makes 1,000 requests in the last minute of one hour and 1,000 more in the first minute of the next hour,

‚Üí effectively 2,000 requests in 2 minutes.

This causes ‚Äú**bursting across windows**‚Äù, which breaks fairness.
Also, Fixed Window immediately **blocks** requests once the cap is hit ‚Äî no flexibility for short bursts.

#### ‚úÖ Step 2: Token Bucket ‚Äî How It Works

- Each partner has a **bucket** with a maximum capacity of 1,000 tokens.
- Tokens are **added gradually** at a constant refill rate (‚âà 1,000 / 3600 ‚âà 0.28 tokens/sec).
- Each request **consumes one token.**
- If tokens are available ‚Üí request is allowed.
- If bucket is empty ‚Üí requests are temporarily throttled or queued until tokens refill.

**‚úÖ Why It Fits Best Here**

* Allows bursts: Partners can send a short burst (e.g., 200 requests instantly) if tokens are saved up.
* Self-regulating: If they burst, they must wait for tokens to refill before sending more.
* Smooth control: Rate enforcement happens continuously, not abruptly per window.

#### ‚úÖ Step 3: Behavior Comparison

| Feature                       | Fixed Window                           | Token Bucket                              |
|-------------------------------|----------------------------------------|-------------------------------------------|
| **Burst tolerance**           | ‚ùå None ‚Äî strict per window             | ‚úÖ Allows short bursts if tokens available |
| **Fairness**                  | ‚ùå May double-count around window edges | ‚úÖ Smooth, continuous rate control         |
| **Implementation complexity** | ‚úÖ Simple                               | ‚ö†Ô∏è Slightly more complex                  |
| **User experience**           | ‚ùå Spiky rejections                     | ‚úÖ Predictable throttling, smoother UX     |

#### ‚öñÔ∏è Trade-offs

| Aspect   | Trade-off                                                    |
|----------|--------------------------------------------------------------|
| **Pros** | Smooth handling of burst traffic, flexible rate control      |
| **Cons** | Slightly harder to implement (requires state + refill logic) |

#### ‚úÖ Final Summary

| Aspect                | Design Decision                             | Reason                                                   |
|-----------------------|---------------------------------------------|----------------------------------------------------------|
| **Algorithm**         | **Token Bucket**                            | Allows short bursts while enforcing long-term rate limit |
| **Refill Rate**       | 0.28 tokens/sec (‚âà 1,000/hour)              | Smooth token replenishment                               |
| **Bucket Size**       | 1,000 tokens                                | Enables immediate bursts up to hourly quota              |
| **Primary Advantage** | Partners can handle short spikes gracefully |                                                          |
| **Trade-off**         | Slightly higher implementation complexity   |                                                          |
