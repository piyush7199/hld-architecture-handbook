# 2.3.9 Event Sourcing Deep Dive: Immutable Event Logs and State Reconstruction

## Intuitive Explanation

Imagine a bank that only stores your current account balance. If you dispute a transaction, they can't show you the
history‚Äîthey can only say "your balance
is $1,000." But what if they kept a complete ledger of every deposit, withdrawal, and transaction? You could see exactly how you got to $
1,000, and they could reconstruct your balance at any point in time.

**Event Sourcing** is like that complete ledger. Instead of storing the current state (
balance = $1,000), you store all the events that led to that state (Deposit $500, Withdraw $200, Deposit $700). The
current state is derived by replaying events.

**In distributed systems:**

- **Event Sourcing:** Store all changes as immutable events
- **Event Store:** Append-only log of events
- **State Reconstruction:** Replay events to rebuild current state
- **Goal:** Complete audit trail, time travel, reproducibility
- **Benefit:** Never lose history, perfect debugging, compliance

---

## In-Depth Analysis

### 1. What is Event Sourcing?

**Event Sourcing** is a pattern where the state of an application is determined by a sequence of events, rather than
storing the current state directly.

**Key Concepts:**

- **Event:** Immutable record of something that happened
- **Event Store:** Append-only log of events
- **Aggregate:** Entity whose state is built from events
- **Snapshot:** Periodic saved state (optimization)
- **Replay:** Rebuilding state by processing events

**Traditional Approach:**

```
Current State Storage:
  User Account:
    balance: $1,000
    last_updated: 2024-01-15

Problem: Lost history, can't see how balance changed
```

**Event Sourcing Approach:**

```
Event Store:
  Event 1: Deposit $500 (2024-01-10)
  Event 2: Withdraw $200 (2024-01-12)
  Event 3: Deposit $700 (2024-01-15)

Current State (derived):
  balance = $500 - $200 + $700 = $1,000
```

### 2. Event Store Design

**Append-Only Log:**

```
Event Store Structure:
  - Events are immutable (never updated or deleted)
  - Events are appended sequentially
  - Events are ordered by timestamp or sequence number
  - Events are partitioned by aggregate ID
```

**Event Schema:**

```json
{
  "event_id": "evt_123",
  "aggregate_id": "user_456",
  "event_type": "AccountDeposited",
  "event_data": {
    "amount": 500,
    "currency": "USD",
    "source": "bank_transfer"
  },
  "metadata": {
    "user_id": "user_456",
    "timestamp": "2024-01-10T10:30:00Z",
    "correlation_id": "req_789"
  },
  "version": 1
}
```

**Event Store Implementation:**

```
Options:
  1. Kafka (distributed log)
  2. PostgreSQL (table with append-only writes)
  3. EventStore DB (specialized event store)
  4. DynamoDB Streams (AWS)
```

### 3. State Reconstruction

**How It Works:**

```
1. Load all events for aggregate
2. Start with initial state (empty or snapshot)
3. Apply each event in order
4. Current state = result after all events

Example:
  Initial: balance = $0
  Event 1: Deposit $500 ‚Üí balance = $500
  Event 2: Withdraw $200 ‚Üí balance = $300
  Event 3: Deposit $700 ‚Üí balance = $1,000
```

**Event Handlers:**

```
Event Handler (Apply Function):
  function apply(event, current_state):
    switch event.type:
      case "AccountDeposited":
        return current_state.balance + event.amount
      case "AccountWithdrawn":
        return current_state.balance - event.amount
      case "AccountOpened":
        return { balance: 0, status: "active" }
```

### 4. Snapshot Strategy

**Problem:** Replaying millions of events is slow

**Solution:** Periodic snapshots

**How It Works:**

```
Snapshot Policy:
  - Save snapshot every N events (e.g., every 100 events)
  - Snapshot contains: aggregate_id, version, state

Loading Aggregate:
  1. Load latest snapshot (version 1000)
  2. Load events after snapshot (version 1001-1050)
  3. Replay events from snapshot (only 50 events, not 1050)
```

**Snapshot Schema:**

```sql
CREATE TABLE snapshots (
    aggregate_id VARCHAR(255) PRIMARY KEY,
    version BIGINT NOT NULL,
    state JSONB NOT NULL,
    created_at TIMESTAMP DEFAULT NOW()
);
```

**Snapshot Benefits:**

- **Fast Loading:** Replay 50 events instead of 10,000
- **Reduced Load:** Less I/O, less CPU
- **Better Performance:** Sub-second aggregate loading

### 5. Event Sourcing vs. Traditional CRUD

| Aspect                | Traditional CRUD           | Event Sourcing                   |
|-----------------------|----------------------------|----------------------------------|
| **Storage**           | Current state (overwrites) | All events (append-only)         |
| **History**           | Lost (only current state)  | Complete (all events)            |
| **Audit Trail**       | Separate audit log needed  | Built-in (events are audit log)  |
| **Time Travel**       | Not possible               | Replay to any point              |
| **Write Performance** | UPDATE queries (locks)     | INSERT queries (no locks)        |
| **Read Performance**  | Fast (single query)        | Slow (replay events)             |
| **Storage Cost**      | Low (only current state)   | High (all events)                |
| **Complexity**        | Low (standard CRUD)        | High (event handlers, snapshots) |

### 6. Event Store Technologies

#### A. Kafka (Distributed Log)

**Why Kafka:**

- **Distributed:** Handles high throughput
- **Durability:** Replicated across brokers
- **Retention:** Configurable (days, weeks, months)
- **Partitioning:** Partition by aggregate ID

**Kafka Configuration:**

```
Topic: account-events
  - Partitions: 100 (for 100M accounts)
  - Replication: 3
  - Retention: 7 years (compliance)
  - Compression: gzip
```

**Partitioning Strategy:**

```
Partition Key: aggregate_id (account_id)
  - All events for same account in same partition
  - Maintains ordering per account
  - Enables efficient replay
```

#### B. PostgreSQL (Table-Based)

**Why PostgreSQL:**

- **ACID:** Strong consistency
- **Queries:** SQL for event queries
- **Familiar:** Standard database

**Table Schema:**

```sql
CREATE TABLE events (
    event_id BIGSERIAL PRIMARY KEY,
    aggregate_id VARCHAR(255) NOT NULL,
    event_type VARCHAR(100) NOT NULL,
    event_data JSONB NOT NULL,
    metadata JSONB,
    version BIGINT NOT NULL,
    created_at TIMESTAMP DEFAULT NOW(),
    INDEX idx_aggregate_version (aggregate_id, version)
) PARTITION BY HASH (aggregate_id);
```

**Benefits:**

- **ACID Transactions:** Strong consistency
- **SQL Queries:** Easy event queries
- **Indexes:** Fast aggregate loading

#### C. EventStore DB

**Why EventStore:**

- **Specialized:** Built for event sourcing
- **Features:** Snapshots, projections, subscriptions
- **Performance:** Optimized for append-only

**Features:**

- **Streams:** Logical grouping of events
- **Projections:** Real-time event processing
- **Subscriptions:** Real-time event notifications

### 7. Common Patterns

#### A. Event Sourcing + CQRS

**Pattern:**

```
Write Side (Event Sourcing):
  - Commands ‚Üí Events ‚Üí Event Store
  - State derived from events

Read Side (CQRS):
  - Events ‚Üí Projections ‚Üí Read Models
  - Optimized for queries
```

**Benefits:**

- **Fast Writes:** Append-only (no locks)
- **Fast Reads:** Pre-computed read models
- **Scalability:** Independent scaling

#### B. Event Sourcing + Snapshots

**Pattern:**

```
Event Store:
  - All events (complete history)
  - Snapshots (periodic state saves)

Loading:
  - Load snapshot (fast)
  - Replay events since snapshot (fast)
```

**Benefits:**

- **Performance:** Fast aggregate loading
- **History:** Complete event history preserved
- **Flexibility:** Can change snapshot format

#### C. Event Sourcing + Message Queue

**Pattern:**

```
Event Store:
  - Primary source of truth
  - Events also published to message queue

Consumers:
  - Read from message queue (real-time)
  - Or read from event store (replay)
```

**Benefits:**

- **Real-Time:** Consumers get events immediately
- **Replay:** Can replay from event store
- **Decoupling:** Event store + message queue

### 8. Event Versioning

**Problem:** Event schema changes over time

**Solution:** Event versioning

**Strategies:**

**1. Schema Evolution:**

```
Version 1: { "amount": 500 }
Version 2: { "amount": 500, "currency": "USD" }
Version 3: { "amount": 500, "currency": "USD", "source": "bank" }

Handler:
  if event.version == 1:
    # Handle old format
  elif event.version == 2:
    # Handle new format
```

**2. Upcasting:**

```
Transform old events to new format:
  Old Event: { "amount": 500 }
  Upcasted: { "amount": 500, "currency": "USD" }
```

**3. Multiple Event Types:**

```
Deprecate old event, create new:
  Old: AccountDepositedV1
  New: AccountDepositedV2
```

### 9. Use Cases

#### A. Financial Systems

**Requirements:**

- Complete audit trail (regulatory)
- Transaction history (7+ years)
- Reproducibility (dispute resolution)

**Solution:**

- Event Sourcing for all transactions
- Events stored for 7+ years
- Can reconstruct account at any point

#### B. Collaborative Editing

**Requirements:**

- Version history (every edit)
- Time travel (view any version)
- Conflict resolution (replay operations)

**Solution:**

- Event Sourcing for all edits
- Snapshots for fast loading
- Replay for version viewing

#### C. E-Commerce Orders

**Requirements:**

- Order history (status changes)
- Audit trail (who changed what)
- Reproducibility (debug issues)

**Solution:**

- Event Sourcing for order lifecycle
- Events: Created, Paid, Shipped, Delivered
- Can reconstruct order state at any time

### 10. Challenges and Solutions

#### A. Event Replay Performance

**Problem:** Replaying millions of events is slow

**Solutions:**

- **Snapshots:** Periodic state saves
- **Parallel Replay:** Replay events in parallel
- **Incremental Snapshots:** Save state after each event (expensive)

#### B. Event Store Growth

**Problem:** Event store grows indefinitely

**Solutions:**

- **Retention Policies:** Delete old events (if allowed)
- **Archival:** Move old events to cold storage (S3)
- **Compaction:** Keep only latest state (lose history)

#### C. Eventual Consistency

**Problem:** Read models lag behind event store

**Solutions:**

- **Synchronous Projections:** Update read model in same transaction (slower)
- **Asynchronous Projections:** Update read model eventually (faster)
- **Version Numbers:** Track read model version

---

## When to Use Event Sourcing

### ‚úÖ Use Event Sourcing When:

1. **Audit Trail Required:** Need complete history (compliance)
2. **Time Travel Needed:** Reconstruct state at any point
3. **High Write Throughput:** Append-only is faster than updates
4. **Debugging Critical:** Need to replay events to debug
5. **Event-Driven Architecture:** Natural fit for event-driven systems
6. **Complex Business Logic:** Events capture business intent

### ‚ùå Don't Use Event Sourcing When:

1. **Simple CRUD:** Standard CRUD is sufficient
2. **Read-Heavy:** Event replay is slow (use CQRS)
3. **No History Needed:** Don't need audit trail
4. **Low Complexity Tolerance:** Event sourcing adds complexity
5. **Storage Cost Sensitive:** Events require more storage

---

## Real-World Examples

### Netflix (Event Sourcing)

**Use Case:** User activity tracking

**Scale:**

- Billions of events per day
- Complete user journey history
- Replay for recommendations

### Uber (Event Sourcing)

**Use Case:** Trip lifecycle

**Scale:**

- Millions of trips per day
- Complete trip history
- Dispute resolution

### Banking Systems (Event Sourcing)

**Use Case:** Account transactions

**Scale:**

- All transactions stored
- 7+ year retention (regulatory)
- Audit trail for compliance

---

## Event Sourcing vs. Other Solutions

| Solution                | Best For                 | History  | Performance               | Complexity |
|-------------------------|--------------------------|----------|---------------------------|------------|
| **Event Sourcing**      | Audit trail, time travel | Complete | Fast writes, slow reads   | High       |
| **Traditional CRUD**    | Simple applications      | None     | Fast reads, slower writes | Low        |
| **Event Logging**       | Audit logging only       | Partial  | Medium                    | Medium     |
| **Change Data Capture** | Database replication     | Partial  | Fast                      | Medium     |

---

## Common Anti-Patterns

### ‚ùå **1. No Snapshots**

**Problem:** Replaying millions of events for each read

**Solution:** Implement snapshot strategy

```
‚ùå Bad:
Load aggregate: Replay 1M events (30 seconds)
‚Üí Unacceptable performance

‚úÖ Good:
Load aggregate: Load snapshot + replay 50 events (50ms)
‚Üí Fast performance
```

### ‚ùå **2. Updating Events**

**Problem:** Modifying events breaks immutability

**Solution:** Create new events, don't modify old ones

```
‚ùå Bad:
UPDATE events SET amount = 600 WHERE event_id = 123
‚Üí Breaks immutability, loses history

‚úÖ Good:
Event 1: Deposit $500 (original)
Event 2: Correction $100 (new event)
‚Üí History preserved, correction tracked
```

### ‚ùå **3. Storing Current State in Events**

**Problem:** Events should describe what happened, not current state

**Solution:** Store only the change, derive state

```
‚ùå Bad:
Event: { "balance": 1000 }  // Current state
‚Üí What changed? Unknown

‚úÖ Good:
Event: { "amount": 500, "type": "deposit" }  // What happened
‚Üí Clear intent, state derived
```

---

## Trade-offs Summary

| Aspect                   | What You Gain            | What You Sacrifice          |
|--------------------------|--------------------------|-----------------------------|
| **Complete History**     | Audit trail, time travel | Higher storage cost         |
| **Fast Writes**          | Append-only (no locks)   | Slow reads (replay needed)  |
| **Reproducibility**      | Replay events to debug   | Complexity (event handlers) |
| **Audit Trail**          | Built-in compliance      | Storage growth              |
| **Eventual Consistency** | Independent scaling      | Read model lag              |

---

## References

- **Event Sourcing Pattern:
  ** [https://martinfowler.com/eaaDev/EventSourcing.html](https://martinfowler.com/eaaDev/EventSourcing.html)
- **EventStore Documentation:** [https://www.eventstore.com/docs/](https://www.eventstore.com/docs/)
- **Related Chapters:**
    - [2.1.6 Data Modeling for Scale](../2.1-databases/2.1.6-data-modeling-for-scale.md) - CQRS overview
    - [2.3.2 Kafka Deep Dive](./2.3.2-kafka-deep-dive.md) - Event store implementation
    - [3.4.5 Stock Brokerage Challenge](../../../03-challenges/3.4.5-stock-brokerage/README.md) - Event sourcing in
      finance
    - [3.4.6 Collaborative Editor Challenge](../../../03-challenges/3.4.6-collaborative-editor/README.md) - Event
      sourcing for version control

---

## ‚úèÔ∏è Design Challenge

### Problem

You are designing an e-commerce order system that must:

1. **Store complete order history** (every status change, payment, shipment)
2. **Handle 100K orders per day** (high write volume)
3. **Support order queries** (find orders by user, date, status)
4. **Enable time travel** (view order at any point in time)
5. **Provide audit trail** (compliance requirement)
6. **Fast order loading** (<100ms to load order)

**Constraints:**

- Orders have complex lifecycle (Created ‚Üí Paid ‚Üí Shipped ‚Üí Delivered ‚Üí Returned)
- Need to support order disputes (reconstruct order state)
- Storage cost-sensitive (prefer efficient storage)
- Must handle order updates (status changes, cancellations)

Design an Event Sourcing strategy that:

- Stores complete order history
- Handles high write volume
- Enables fast order queries
- Supports time travel
- Provides audit trail
- Optimizes storage

### Solution

#### üß© Scenario

- **Orders:** 100K orders/day = 1.16 orders/second average, 100/second peak
- **Events per Order:** Average 10 events (Created, Paid, Shipped, etc.)
- **Events per Day:** 100K orders √ó 10 events = 1M events/day
- **Retention:** 7 years (compliance)

**Calculations:**

- **Events per Second:** 1M events/day = 11.6 events/second average, 1000/second peak
- **Storage per Event:** ~500 bytes (JSON)
- **Daily Storage:** 1M √ó 500 bytes = 500 MB/day
- **7-Year Storage:** 500 MB √ó 365 √ó 7 = ~1.3 TB

#### ‚úÖ Step 1: Event Store Choice

**Choice: Kafka (Distributed Log)**

**Why:**

- **High Throughput:** Handles 1000 events/second easily
- **Durability:** Replicated (3√ó replication)
- **Retention:** 7-year retention configurable
- **Partitioning:** Partition by order_id (maintains ordering)
- **Scalability:** Horizontal scaling

**Kafka Configuration:**

```
Topic: order-events
  - Partitions: 50 (for 100K orders/day)
  - Replication: 3
  - Retention: 7 years (2555 days)
  - Compression: gzip (reduces storage by ~70%)
```

#### ‚úÖ Step 2: Event Schema

**Event Types:**

```json
OrderCreated:
{
"event_id": "evt_123",
"order_id": "ord_456",
"event_type": "OrderCreated",
"event_data": {
"user_id": "user_789",
"items": [...],
"total_amount": 150.00,
"currency": "USD"
},
"metadata": {
"timestamp": "2024-01-15T10:30:00Z",
"user_id": "user_789",
"correlation_id": "req_abc"
},
"version": 1
}

OrderPaid: {
"event_id": "evt_124",
"order_id": "ord_456",
"event_type": "OrderPaid",
"event_data": {
"payment_id": "pay_xyz",
"amount": 150.00,
"payment_method": "credit_card"
},
"metadata": {
...
},
"version": 1
}

OrderShipped: {
"event_id": "evt_125",
"order_id": "ord_456",
"event_type": "OrderShipped",
"event_data": {
"tracking_number": "TRACK123",
"carrier": "UPS",
"shipped_at": "2024-01-16T10:00:00Z"
},
"metadata": {...},
"version": 1
}
```

#### ‚úÖ Step 3: Snapshot Strategy

**Snapshot Policy:**

```
Snapshot Frequency:
  - Save snapshot after every 10 events
  - Or save snapshot after major state change (Paid, Shipped, Delivered)

Snapshot Schema (PostgreSQL):
  CREATE TABLE order_snapshots (
      order_id VARCHAR(255) PRIMARY KEY,
      version BIGINT NOT NULL,
      state JSONB NOT NULL,
      created_at TIMESTAMP DEFAULT NOW()
  );
```

**Snapshot Content:**

```json
{
  "order_id": "ord_456",
  "version": 10,
  "state": {
    "status": "shipped",
    "user_id": "user_789",
    "items": [
      ...
    ],
    "total_amount": 150.00,
    "payment_id": "pay_xyz",
    "tracking_number": "TRACK123",
    "shipped_at": "2024-01-16T10:00:00Z"
  }
}
```

**Loading Order:**

```
1. Load snapshot (version 10) ‚Üí Fast (<10ms)
2. Load events after snapshot (version 11-15) ‚Üí Fast (<10ms)
3. Replay events (5 events) ‚Üí Fast (<10ms)
4. Total: <30ms (meets <100ms requirement)
```

#### ‚úÖ Step 4: CQRS Read Model

**Read Model (PostgreSQL):**

```
Order Table (Optimized for Queries):
  CREATE TABLE orders (
      order_id VARCHAR(255) PRIMARY KEY,
      user_id VARCHAR(255) NOT NULL,
      status VARCHAR(50) NOT NULL,
      total_amount DECIMAL(10,2),
      created_at TIMESTAMP,
      paid_at TIMESTAMP,
      shipped_at TIMESTAMP,
      INDEX idx_user_created (user_id, created_at),
      INDEX idx_status (status)
  );
```

**Projection (Event Handler):**

```
Event ‚Üí Read Model Update:
  OrderCreated ‚Üí INSERT INTO orders (...)
  OrderPaid ‚Üí UPDATE orders SET status='paid', paid_at=...
  OrderShipped ‚Üí UPDATE orders SET status='shipped', shipped_at=...
```

**Benefits:**

- **Fast Queries:** Standard SQL queries (<10ms)
- **Indexes:** Fast lookups by user, status, date
- **Familiar:** Standard database queries

#### ‚úÖ Step 5: Event Processing Pipeline

**Pipeline:**

```
1. Order Service ‚Üí Kafka: Publishes event
2. Kafka ‚Üí Event Store: Event persisted
3. Kafka ‚Üí Snapshot Service: Consumes event
4. Snapshot Service: Updates snapshot (every 10 events)
5. Kafka ‚Üí Read Model Service: Consumes event
6. Read Model Service: Updates read model (PostgreSQL)
```

**Processing:**

```
Snapshot Service:
  - Consumes events from Kafka
  - Maintains in-memory state per order
  - Saves snapshot every 10 events
  - Handles failures (replay from last snapshot)

Read Model Service:
  - Consumes events from Kafka
  - Updates PostgreSQL read model
  - Handles idempotency (event_id deduplication)
```

#### ‚úÖ Step 6: Time Travel Support

**Reconstructing Order at Point in Time:**

```
1. Load snapshot before target time (version N)
2. Load events between snapshot and target time
3. Replay events up to target time
4. Return state at target time

Example:
  Target: 2024-01-16 10:00:00
  Snapshot: 2024-01-15 12:00:00 (version 5)
  Events: 6 events between snapshot and target
  Replay: Apply 6 events to snapshot
  Result: Order state at 2024-01-16 10:00:00
```

#### ‚úÖ Step 7: Storage Optimization

**Compression:**

```
Kafka Compression: gzip
  - Reduces storage by ~70%
  - 1.3 TB ‚Üí ~400 GB (compressed)
```

**Archival:**

```
Storage Tiering:
  - Hot (Kafka): Last 30 days (fast access)
  - Warm (S3): 30 days - 1 year (slower access)
  - Cold (Glacier): 1-7 years (archive, slow access)

Benefits:
  - Reduces Kafka storage (only 30 days)
  - Lower cost (S3/Glacier cheaper)
  - Still accessible (can replay from S3)
```

#### ‚úÖ Complete Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ          Order Service (Write Side)                      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ Create Order ‚Üí OrderCreated Event                  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ Pay Order ‚Üí OrderPaid Event                        ‚îÇ ‚îÇ
‚îÇ  ‚îÇ Ship Order ‚Üí OrderShipped Event                    ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ   Kafka (Event Store)              ‚îÇ
        ‚îÇ   Topic: order-events              ‚îÇ
        ‚îÇ   Partitions: 50                   ‚îÇ
        ‚îÇ   Retention: 7 years               ‚îÇ
        ‚îÇ   Compression: gzip                ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ                 ‚îÇ                 ‚îÇ
        ‚ñº                 ‚ñº                 ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Snapshot     ‚îÇ  ‚îÇ Read Model   ‚îÇ  ‚îÇ Archive      ‚îÇ
‚îÇ Service      ‚îÇ  ‚îÇ Service      ‚îÇ  ‚îÇ Service      ‚îÇ
‚îÇ              ‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ              ‚îÇ
‚îÇ - Maintains  ‚îÇ  ‚îÇ - Updates    ‚îÇ  ‚îÇ - Moves old  ‚îÇ
‚îÇ   snapshots  ‚îÇ  ‚îÇ   PostgreSQL ‚îÇ  ‚îÇ   events to  ‚îÇ
‚îÇ - Saves      ‚îÇ  ‚îÇ   read model ‚îÇ  ‚îÇ   S3/Glacier ‚îÇ
‚îÇ   every 10   ‚îÇ  ‚îÇ - Fast       ‚îÇ  ‚îÇ - 30+ days  ‚îÇ
‚îÇ   events     ‚îÇ  ‚îÇ   queries    ‚îÇ  ‚îÇ   retention  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ                 ‚îÇ
       ‚ñº                 ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ PostgreSQL   ‚îÇ  ‚îÇ PostgreSQL   ‚îÇ
‚îÇ (Snapshots)  ‚îÇ  ‚îÇ (Read Model) ‚îÇ
‚îÇ              ‚îÇ  ‚îÇ              ‚îÇ
‚îÇ - Fast       ‚îÇ  ‚îÇ - Fast       ‚îÇ
‚îÇ   loading    ‚îÇ  ‚îÇ   queries    ‚îÇ
‚îÇ - Version    ‚îÇ  ‚îÇ - Indexes    ‚îÇ
‚îÇ   tracking   ‚îÇ  ‚îÇ   by user,   ‚îÇ
‚îÇ              ‚îÇ  ‚îÇ   status     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Request Flow:**

```
1. User ‚Üí Order Service: Create order
2. Order Service: Validates, creates OrderCreated event
3. Order Service ‚Üí Kafka: Publishes event
4. Kafka: Persists event (replicated, durable)
5. Snapshot Service ‚Üí Kafka: Consumes event
6. Snapshot Service: Updates in-memory state, saves snapshot (every 10 events)
7. Read Model Service ‚Üí Kafka: Consumes event
8. Read Model Service: Updates PostgreSQL read model
9. User ‚Üí Read Model: Queries order (fast, from PostgreSQL)
```

**Time Travel Flow:**

```
1. User ‚Üí Order Service: Get order at timestamp T
2. Order Service: Finds snapshot before T
3. Order Service: Loads events between snapshot and T
4. Order Service: Replays events to reconstruct state at T
5. Order Service ‚Üí User: Returns order state at T
```

#### ‚öñÔ∏è Trade-offs Summary

| Decision              | What We Gain                | What We Sacrifice               |
|-----------------------|-----------------------------|---------------------------------|
| **Kafka Event Store** | High throughput, durability | Additional infrastructure       |
| **Snapshots**         | Fast order loading          | Additional storage, complexity  |
| **CQRS Read Model**   | Fast queries                | Eventual consistency (1-2s lag) |
| **7-Year Retention**  | Compliance, audit trail     | High storage cost (400 GB)      |
| **Compression**       | Lower storage cost          | Slight CPU overhead             |

#### ‚úÖ Final Summary

**Event Sourcing Strategy:**

- **Event Store:** Kafka (50 partitions, 7-year retention, gzip compression)
- **Snapshots:** PostgreSQL (every 10 events, fast loading)
- **Read Model:** PostgreSQL (CQRS, optimized for queries)
- **Archival:** S3/Glacier (30+ days, cost optimization)

**Performance:**

- **Write Throughput:** 1000 events/second (handles peak)
- **Order Loading:** <30ms (snapshot + replay, meets <100ms requirement)
- **Query Performance:** <10ms (read model, indexed)
- **Storage:** ~400 GB (7 years, compressed)

**Result:**

- ‚úÖ Stores complete order history (all events)
- ‚úÖ Handles 100K orders/day (1000 events/sec peak)
- ‚úÖ Fast order queries (<10ms from read model)
- ‚úÖ Time travel support (replay to any point)
- ‚úÖ Audit trail (all events stored)
- ‚úÖ Fast order loading (<30ms with snapshots)
- ‚úÖ Storage optimized (compression, archival)

