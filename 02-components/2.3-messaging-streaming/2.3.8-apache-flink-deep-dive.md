# 2.3.8 Apache Flink Deep Dive: The True Stream Processing Engine

## Intuitive Explanation

Apache Flink is a **true stream processing** framework that processes data **event-by-event** in real-time (unlike Spark's micro-batching). Think of it as a pipeline where each event flows through immediately, with sub-millisecond latency. While Spark treats streaming as "fast batch," Flink treats batch as a special case of streaming. It's designed for ultra-low latency applications like fraud detection, real-time recommendations, and IoT analytics.

- **True Streaming:** Processes events one-by-one (not micro-batches)
- **Low Latency:** Millisecond-level latency (vs. Spark's seconds)
- **Exactly-Once:** Guarantees no data loss or duplication (even during failures)
- **Stateful Processing:** Maintains state across events (windows, aggregations)
- **Use Cases:** Real-time fraud detection, CEP (Complex Event Processing), real-time ML inference

**The Power:** Process 1 million events/sec with <10ms latency (vs. Spark's 500ms-2s).

---

## In-Depth Analysis

### 1. Architecture: Job Manager, Task Managers

```
┌────────────────────────────────────────────────┐
│           Flink Application                     │
├────────────────────────────────────────────────┤
│  ┌──────────────────────────────┐             │
│  │     Job Manager              │             │
│  │  - Job scheduling            │             │
│  │  - Checkpointing coordinator │             │
│  │  - Resource management       │             │
│  └──────────┬───────────────────┘             │
│             │                                  │
│    ┌────────┼────────┬────────┐              │
│    ▼        ▼        ▼        ▼              │
│  ┌─────┐  ┌─────┐  ┌─────┐  ┌─────┐        │
│  │ TM 1│  │ TM 2│  │ TM 3│  │ TM 4│        │
│  └─────┘  └─────┘  └─────┘  └─────┘        │
│  Task Managers (execute tasks, hold state)   │
│                                              │
│  Event-by-event processing (not micro-batch)│
└────────────────────────────────────────────────┘
```

**Key Components:**

| Component | Role |
|-----------|------|
| **Job Manager** | Coordinates job execution, manages checkpoints |
| **Task Managers** | Execute dataflow tasks (map, filter, aggregate) |
| **State Backend** | Stores operator state (RocksDB, in-memory) |
| **Checkpoints** | Periodic snapshots for fault tolerance |

---

### 2. DataStream API: Core Abstraction

**DataStream:** Unbounded stream of events.

```java
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.datastream.DataStream;

StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

// Read from Kafka
DataStream<String> stream = env.addSource(
    new FlinkKafkaConsumer<>("topic", new SimpleStringSchema(), properties)
);

// Transformations (event-by-event)
DataStream<WordCount> counts = stream
    .flatMap((String line, Collector<String> out) -> {
        for (String word : line.split(" ")) {
            out.collect(word);
        }
    })
    .map(word -> new WordCount(word, 1))
    .keyBy(wc -> wc.word)
    .sum("count");

// Sink to Kafka
counts.addSink(new FlinkKafkaProducer<>("output-topic", schema, properties));

env.execute("WordCount");
```

**Key Operations:**

| Operation | What It Does | Example |
|-----------|--------------|---------|
| `map()` | Transform each event | `.map(x -> x * 2)` |
| `filter()` | Keep matching events | `.filter(x -> x > 10)` |
| `flatMap()` | One-to-many transformation | `.flatMap((line, out) -> out.collect(line.split(" ")))` |
| `keyBy()` | Partition by key (for stateful ops) | `.keyBy(event -> event.userId)` |
| `window()` | Group events into time windows | `.window(TumblingEventTimeWindows.of(Time.minutes(5)))` |
| `reduce()` | Aggregate events | `.reduce((a, b) -> a + b)` |

---

### 3. Event Time vs. Processing Time

**Event Time:** When the event actually occurred (embedded in data).  
**Processing Time:** When Flink processes the event.

```
Event Time:     [10:00] [10:01] [10:02] [10:03] [10:04]
                   │       │       │       │       │
                   │       │       │       │       │
                   ▼       ▼       ▼       ▼       ▼
Processing Time: [10:05] [10:06] [10:05] [10:07] [10:08]
                          ↑ Out of order!
```

**Why Event Time Matters:**

- Events arrive out of order (network delays, clock skew)
- Event time ensures correct results (e.g., hourly aggregations based on when events happened, not when received)

**Configuration:**

```java
env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);

// Extract timestamp from event
stream.assignTimestampsAndWatermarks(
    WatermarkStrategy.<Event>forBoundedOutOfOrderness(Duration.ofSeconds(5))
        .withTimestampAssigner((event, timestamp) -> event.getTimestamp())
);
```

**Watermarks:** Signal that "all events up to time T have arrived."

---

### 4. Windowing: Group Events by Time

**Window Types:**

| Type | Behavior | Use Case |
|------|----------|----------|
| **Tumbling** | Fixed-size, non-overlapping | Hourly aggregations |
| **Sliding** | Fixed-size, overlapping | Moving averages |
| **Session** | Gap-based (timeout) | User sessions |

**Example: Tumbling Window**

```java
stream
    .keyBy(event -> event.userId)
    .window(TumblingEventTimeWindows.of(Time.minutes(5)))  // 5-minute tumbling windows
    .sum("value");

// Window 1: [00:00, 00:05)
// Window 2: [00:05, 00:10)
// Window 3: [00:10, 00:15)
```

**Example: Sliding Window**

```java
stream
    .keyBy(event -> event.userId)
    .window(SlidingEventTimeWindows.of(Time.minutes(10), Time.minutes(5)))
    .sum("value");

// Window 1: [00:00, 00:10)
// Window 2: [00:05, 00:15) ← Overlaps with Window 1
// Window 3: [00:10, 00:20)
```

**Example: Session Window**

```java
stream
    .keyBy(event -> event.userId)
    .window(EventTimeSessionWindows.withGap(Time.minutes(10)))  // 10-minute gap
    .sum("value");

// Events at: 00:00, 00:05, 00:07 → Session 1 (ends at 00:17)
// Events at: 00:25, 00:30 → Session 2 (ends at 00:40)
```

---

### 5. Stateful Processing

**State:** Data maintained across events (e.g., running count, session data).

**State Types:**

| Type | Scope | Use Case |
|------|-------|----------|
| **ValueState** | Single value per key | User balance, last login time |
| **ListState** | List of values per key | Shopping cart items |
| **MapState** | Key-value map per key | User preferences |
| **ReducingState** | Aggregated value per key | Running sum |

**Example: ValueState**

```java
class CountMapper extends RichFlatMapFunction<Event, Tuple2<String, Integer>> {
    private ValueState<Integer> countState;
    
    @Override
    public void open(Configuration config) {
        ValueStateDescriptor<Integer> descriptor = 
            new ValueStateDescriptor<>("count", Integer.class, 0);
        countState = getRuntimeContext().getState(descriptor);
    }
    
    @Override
    public void flatMap(Event event, Collector<Tuple2<String, Integer>> out) throws Exception {
        int currentCount = countState.value();
        currentCount++;
        countState.update(currentCount);
        
        out.collect(new Tuple2<>(event.userId, currentCount));
    }
}
```

**State Backend:**

| Backend | Storage | Performance | Use Case |
|---------|---------|-------------|----------|
| **MemoryStateBackend** | JVM heap | ⚡ Fastest | Small state (<GB) |
| **FsStateBackend** | Disk (HDFS/S3) | Medium | Medium state (GB) |
| **RocksDBStateBackend** | Embedded RocksDB | Slower | Large state (TB) |

---

### 6. Fault Tolerance: Exactly-Once Semantics

**Chandy-Lamport Algorithm (Distributed Snapshots):**

```
┌──────────────────────────────────────────────┐
│        Flink Checkpointing                   │
├──────────────────────────────────────────────┤
│  1. Job Manager triggers checkpoint          │
│  2. Source operators inject barriers         │
│  3. Barriers flow through pipeline           │
│  4. Each operator snapshots state when       │
│     barrier arrives                          │
│  5. All operators confirm → checkpoint done  │
└──────────────────────────────────────────────┘

Timeline:
T0: Checkpoint 1 starts → Barrier injected
T1: Operator A snapshots state (barrier passes through A)
T2: Operator B snapshots state (barrier passes through B)
T3: Operator C snapshots state (barrier passes through C)
T4: Checkpoint 1 complete → State saved to durable storage

If failure at T5:
  → Restore from Checkpoint 1
  → Replay events from Kafka (offset stored in checkpoint)
  → Exactly-once guarantee maintained
```

**Configuration:**

```java
env.enableCheckpointing(5000);  // Checkpoint every 5 seconds
env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);
env.getCheckpointConfig().setMinPauseBetweenCheckpoints(1000);
env.getCheckpointConfig().setCheckpointTimeout(60000);
```

---

### 7. Flink SQL & Table API

**SQL on Streams:**

```java
import org.apache.flink.table.api.*;

StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);

// Register Kafka source as table
tableEnv.executeSql(
    "CREATE TABLE transactions (" +
    "  user_id STRING," +
    "  amount DOUBLE," +
    "  transaction_time TIMESTAMP(3)," +
    "  WATERMARK FOR transaction_time AS transaction_time - INTERVAL '5' SECOND" +
    ") WITH (" +
    "  'connector' = 'kafka'," +
    "  'topic' = 'transactions'," +
    "  'properties.bootstrap.servers' = 'localhost:9092'" +
    ")"
);

// Run SQL query (streaming)
Table result = tableEnv.sqlQuery(
    "SELECT user_id, COUNT(*) as txn_count, SUM(amount) as total_amount " +
    "FROM transactions " +
    "GROUP BY user_id, TUMBLE(transaction_time, INTERVAL '1' HOUR)"
);

// Convert back to DataStream
DataStream<Row> resultStream = tableEnv.toAppendStream(result, Row.class);
```

---

### 8. CEP (Complex Event Processing)

**Pattern Matching on Streams:**

```java
import org.apache.flink.cep.*;
import org.apache.flink.cep.pattern.Pattern;

// Define pattern: 3 failed logins within 5 seconds
Pattern<LoginEvent, ?> pattern = Pattern
    .<LoginEvent>begin("first")
        .where(event -> event.status.equals("failed"))
    .next("second")
        .where(event -> event.status.equals("failed"))
    .next("third")
        .where(event -> event.status.equals("failed"))
    .within(Time.seconds(5));

// Apply pattern to stream
PatternStream<LoginEvent> patternStream = CEP.pattern(loginStream.keyBy(LoginEvent::getUserId), pattern);

// Select matches
DataStream<Alert> alerts = patternStream.select((PatternSelectFunction<LoginEvent, Alert>) pattern -> {
    LoginEvent first = pattern.get("first").get(0);
    return new Alert(first.getUserId(), "3 failed logins in 5 seconds");
});
```

**Use Cases:**
- Fraud detection (suspicious patterns)
- Network intrusion detection
- IoT anomaly detection

---

### 9. When to Use Apache Flink

#### **✅ Use Flink When:**

1. **Ultra-low latency** — <100ms latency required (real-time dashboards, fraud detection)
2. **True event-by-event processing** — Can't tolerate micro-batch delays
3. **Complex event processing (CEP)** — Pattern matching on streams
4. **Stateful stream processing** — Need to maintain state across events
5. **Event time processing** — Out-of-order events with watermarks
6. **Exactly-once semantics** — No data loss or duplication
7. **Streaming-first** — Stream processing is primary workload (not batch)

#### **❌ Don't Use Flink When:**

1. **Batch-only workload** — Use Spark (better batch support)
2. **Machine learning training** — Use Spark MLlib (Flink's ML library is limited)
3. **Simple ETL** — Use Spark (simpler API, better ecosystem)
4. **Interactive queries** — Use Spark SQL or Presto
5. **Small data** — Overhead not worth it (<1GB/day)

---

### 10. Real-World Examples

| Company | Use Case | Why Flink? |
|---------|----------|------------|
| **Alibaba** | Real-time recommendations | Ultra-low latency (<50ms) |
| **Uber** | Real-time fraud detection | CEP for suspicious patterns |
| **Netflix** | Real-time anomaly detection | Stateful processing on billions of events |
| **Lyft** | Real-time pricing | Sub-second latency for surge pricing |
| **ING Bank** | Fraud detection | Exactly-once guarantees for financial data |

---

### 11. Flink vs. Spark Streaming

| Feature | Flink | Spark Streaming |
|---------|-------|-----------------|
| **Processing model** | ✅ True streaming (event-by-event) | ⚠️ Micro-batch (500ms-2s batches) |
| **Latency** | ✅ Milliseconds (<10ms) | ⚠️ Seconds (500ms-2s) |
| **State management** | ✅ Built-in stateful operators | ⚠️ Manual (checkpointing) |
| **Event time** | ✅ First-class support (watermarks) | ⚠️ Limited support |
| **Exactly-once** | ✅ Native (Chandy-Lamport) | ✅ Yes (via Kafka offsets) |
| **CEP** | ✅ Built-in pattern matching | ❌ No |
| **ML Library** | ⚠️ FlinkML (limited) | ✅ MLlib (mature) |
| **Batch processing** | ⚠️ Supported (but not primary) | ✅ Excellent |
| **Use case** | Ultra-low latency streaming | Unified batch + streaming + ML |

---

### 12. Common Anti-Patterns

#### ❌ **1. Using Processing Time Instead of Event Time**

**Problem:**

```java
// Bad: Processing time (incorrect for out-of-order events)
stream.window(TumblingProcessingTimeWindows.of(Time.minutes(5)))
```

**Solution:**

```java
// Good: Event time (correct results)
stream
    .assignTimestampsAndWatermarks(...)
    .window(TumblingEventTimeWindows.of(Time.minutes(5)))
```

#### ❌ **2. Not Configuring Checkpointing**

**Problem:** No fault tolerance (data loss on failure).

**Solution:**

```java
env.enableCheckpointing(5000);  // Checkpoint every 5 seconds
```

#### ❌ **3. Using Wrong State Backend**

**Problem:** Using MemoryStateBackend for large state (OOM).

**Solution:**

```java
// Use RocksDB for large state
env.setStateBackend(new RocksDBStateBackend("hdfs://checkpoints"));
```

---

### 13. Trade-offs Summary

| What You Gain | What You Sacrifice |
|---------------|-------------------|
| ✅ True streaming (event-by-event) | ❌ More complex than Spark (steeper learning curve) |
| ✅ Ultra-low latency (<10ms) | ❌ Smaller ecosystem (fewer integrations) |
| ✅ Built-in stateful processing | ❌ Limited ML library (vs. Spark MLlib) |
| ✅ First-class event time support | ❌ Batch processing not as mature as Spark |
| ✅ Exactly-once semantics | ❌ Higher operational complexity |

---

### 14. References

- **Apache Flink Documentation:** [https://flink.apache.org/](https://flink.apache.org/)
- **Stream Processing with Apache Flink (Book):** By Fabian Hueske & Vasiliki Kalavri
- **Flink Forward Conference:** [https://www.flink-forward.org/](https://www.flink-forward.org/)
- **Related Chapters:**
  - [2.3.7 Apache Spark Deep Dive](./2.3.7-apache-spark-deep-dive.md) — Spark vs. Flink
  - [2.3.2 Kafka Deep Dive](./2.3.2-kafka-deep-dive.md) — Kafka + Flink integration
  - [2.3.5 Batch vs Stream Processing](./2.3.5-batch-vs-stream-processing.md) — When to use streaming

---

## ✏️ Design Challenge

### Problem

You're building a **real-time ride-hailing surge pricing system** (like Uber). Requirements:

1. **Latency:** Calculate surge multiplier within **100ms** of receiving demand/supply data
2. **Data sources:**
   - Ride requests (Kafka): user_id, location, timestamp
   - Driver availability (Kafka): driver_id, location, available (boolean)
3. **Logic:**
   - Calculate demand (ride requests) and supply (available drivers) per geographic zone (1km × 1km grid)
   - Surge multiplier = demand / supply (range: 1.0x to 5.0x)
   - Update surge multiplier every 30 seconds (sliding window)
4. **Scale:** 100,000 events/sec (peak), 1 million drivers worldwide
5. **Consistency:** Exactly-once semantics (no double-charging users)

**Question:** Would you use Flink or Spark? How would you design the windowing logic? How would you ensure <100ms latency while maintaining exactly-once guarantees?

### Solution

#### 🧩 Scenario

- **System:** Real-time surge pricing
- **Latency requirement:** <100ms
- **Event rate:** 100K events/sec (50K ride requests + 50K driver updates)
- **Logic:** Demand/supply ratio per geographic zone (1km grid)
- **Window:** 30-second sliding window
- **Scale:** 1M drivers, 10M users

#### ✅ Goal

- Calculate surge multiplier in <100ms
- Update every 30 seconds (sliding window)
- Exactly-once semantics (no double-charging)
- Handle 100K events/sec without backpressure
- Partition by geographic zone for parallelism

#### ⚙️ Solution: Apache Flink (True Streaming Required)

**Why Flink over Spark?**

| Requirement | Flink | Spark Streaming |
|-------------|-------|-----------------|
| **Latency** | ✅ <10ms event processing | ❌ 500ms-2s micro-batch |
| **<100ms end-to-end** | ✅ Achievable | ❌ Impossible (micro-batch overhead) |
| **Stateful processing** | ✅ Built-in window state | ⚠️ Manual checkpointing |
| **Event-by-event** | ✅ True streaming | ❌ Batches |
| **Exactly-once** | ✅ Native (Chandy-Lamport) | ✅ Yes (but with higher latency) |

**Recommendation:** **Flink** — Only way to meet <100ms latency.

**Implementation:**

```java
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.windowing.time.Time;
import org.apache.flink.streaming.api.windowing.assigners.SlidingEventTimeWindows;

StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

// Enable checkpointing (exactly-once)
env.enableCheckpointing(5000);
env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);

// Read ride requests from Kafka
DataStream<RideRequest> rideRequests = env
    .addSource(new FlinkKafkaConsumer<>("ride-requests", schema, properties))
    .assignTimestampsAndWatermarks(
        WatermarkStrategy.<RideRequest>forBoundedOutOfOrderness(Duration.ofSeconds(5))
            .withTimestampAssigner((event, timestamp) -> event.getTimestamp())
    );

// Read driver availability from Kafka
DataStream<DriverUpdate> driverUpdates = env
    .addSource(new FlinkKafkaConsumer<>("driver-updates", schema, properties))
    .assignTimestampsAndWatermarks(
        WatermarkStrategy.<DriverUpdate>forBoundedOutOfOrderness(Duration.ofSeconds(5))
            .withTimestampAssigner((event, timestamp) -> event.getTimestamp())
    );

// Convert to geographic zone (1km grid)
DataStream<DemandEvent> demand = rideRequests
    .map(req -> new DemandEvent(
        getZoneId(req.getLatitude(), req.getLongitude()),  // e.g., "zone_123_456"
        1,  // 1 ride request
        req.getTimestamp()
    ));

DataStream<SupplyEvent> supply = driverUpdates
    .filter(driver -> driver.isAvailable())  // Only available drivers
    .map(driver -> new SupplyEvent(
        getZoneId(driver.getLatitude(), driver.getLongitude()),
        1,  // 1 available driver
        driver.getTimestamp()
    ));

// Union demand and supply streams
DataStream<ZoneEvent> events = demand.map(d -> new ZoneEvent(d.zoneId, d.count, 0, d.timestamp))
    .union(supply.map(s -> new ZoneEvent(s.zoneId, 0, s.count, s.timestamp)));

// Sliding window: 30-second window, update every 5 seconds
DataStream<SurgeMultiplier> surgeMultipliers = events
    .keyBy(event -> event.zoneId)
    .window(SlidingEventTimeWindows.of(Time.seconds(30), Time.seconds(5)))
    .aggregate(new AggregateFunction<ZoneEvent, ZoneStats, SurgeMultiplier>() {
        @Override
        public ZoneStats createAccumulator() {
            return new ZoneStats(0, 0);
        }
        
        @Override
        public ZoneStats add(ZoneEvent event, ZoneStats acc) {
            acc.demandCount += event.demandCount;
            acc.supplyCount += event.supplyCount;
            return acc;
        }
        
        @Override
        public SurgeMultiplier getResult(ZoneStats acc) {
            double surge = acc.supplyCount == 0 ? 5.0 : 
                           Math.min(5.0, Math.max(1.0, (double) acc.demandCount / acc.supplyCount));
            return new SurgeMultiplier(acc.zoneId, surge, System.currentTimeMillis());
        }
        
        @Override
        public ZoneStats merge(ZoneStats a, ZoneStats b) {
            return new ZoneStats(a.demandCount + b.demandCount, a.supplyCount + b.supplyCount);
        }
    });

// Write to output Kafka (for ride service to consume)
surgeMultipliers.addSink(
    new FlinkKafkaProducer<>("surge-multipliers", schema, properties)
);

env.execute("SurgePricingFlink");
```

**Key Design Decisions:**

1. **Geographic Zoning (1km × 1km):**
   ```java
   String getZoneId(double lat, double lon) {
       int latZone = (int) (lat * 100);  // 1km ≈ 0.01 degrees
       int lonZone = (int) (lon * 100);
       return "zone_" + latZone + "_" + lonZone;
   }
   ```

2. **Sliding Window (30s window, 5s slide):**
   - Updates surge every 5 seconds (not too frequent, but responsive)
   - 30-second window captures recent demand/supply trends

3. **Keyed by Zone:**
   - Parallel processing per zone (1000s of zones = 1000s of parallel tasks)
   - Each zone's state isolated (no cross-zone coordination needed)

#### ⚠️ Latency Breakdown

```
Event arrives → Kafka (10ms) → Flink ingestion (5ms) → 
Window aggregation (10ms) → Kafka output (10ms) → 
Ride service consumes (10ms) → Total: 45ms ✅

<100ms requirement met!
```

**Optimization Tips:**

1. **Parallelism:**
   ```java
   env.setParallelism(100);  // 100 parallel tasks
   ```

2. **Kafka Partitioning:**
   - Partition Kafka by zone_id (co-locate data for same zone)
   - Reduces network shuffling

3. **State Backend:**
   ```java
   env.setStateBackend(new RocksDBStateBackend("hdfs://checkpoints"));
   ```
   - RocksDB for large state (1M zones × window state)

#### 🧠 Handling Scale (100K events/sec)

**Challenge:** 100K events/sec = high throughput. How to avoid backpressure?

**Solution:**

1. **Partition Kafka Topic:**
   - 100 partitions (1K events/sec per partition)
   - Each Flink task consumes 1 partition

2. **Flink Parallelism:**
   - 100 parallel tasks (matches Kafka partitions)
   - Each task handles 1K events/sec (easily manageable)

3. **Checkpointing:**
   - Async checkpointing (doesn't block processing)
   - Incremental checkpoints (RocksDB)

#### ✅ Final Answer

| Aspect | Decision | Reason |
|--------|----------|--------|
| **Framework** | **Apache Flink** | Only way to achieve <100ms latency (true streaming) |
| **Window** | Sliding (30s window, 5s slide) | Updates every 5s, captures recent trends |
| **Partitioning** | Key by zone_id (geographic hash) | Parallel processing per zone |
| **Latency** | ~45ms (event → output) | Meets <100ms requirement ✅ |
| **Throughput** | 100K events/sec | 100 Kafka partitions × 1K events/sec each |
| **Exactly-Once** | Flink checkpointing + Kafka offsets | No double-charging users |
| **State Backend** | RocksDB | Large state (1M zones × window data) |
| **Trade-off** | More complex than Spark | Gain: Sub-100ms latency (impossible with Spark) |

**Performance Metrics:**
- **End-to-end latency:** 45ms (p99)
- **Throughput:** 100K events/sec
- **State size:** ~10GB (1M zones × 10KB state/zone)
- **Checkpoint interval:** 5 seconds
- **Recovery time:** <30 seconds (from last checkpoint)

**Why NOT Spark:**
- ❌ **Micro-batch latency:** 500ms-2s (cannot meet <100ms)
- ❌ **Batch processing overhead:** Even with 100ms trigger, batching adds latency
- ✅ **Flink is the only option** for <100ms real-time requirements

**When to Reconsider:**
- If latency requirement relaxed to >1 second → Spark Streaming viable (simpler ops)
- If need ML model training → Hybrid (Spark for training, Flink for scoring)
- If batch analytics needed → Hybrid (Flink for streaming, Spark for batch)

