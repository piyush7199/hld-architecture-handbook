# 2.3.8 Apache Flink Deep Dive: The True Stream Processing Engine

## Intuitive Explanation

Apache Flink is a **true stream processing** framework that processes data **event-by-event** in real-time (unlike
Spark's micro-batching). Think of it as a pipeline where each event flows through immediately, with sub-millisecond
latency. While Spark treats streaming as "fast batch," Flink treats batch as a special case of streaming. It's designed
for ultra-low latency applications like fraud detection, real-time recommendations, and IoT analytics.

- **True Streaming:** Processes events one-by-one (not micro-batches)
- **Low Latency:** Millisecond-level latency (vs. Spark's seconds)
- **Exactly-Once:** Guarantees no data loss or duplication (even during failures)
- **Stateful Processing:** Maintains state across events (windows, aggregations)
- **Use Cases:** Real-time fraud detection, CEP (Complex Event Processing), real-time ML inference

**The Power:** Process 1 million events/sec with <10ms latency (vs. Spark's 500ms-2s).

---

## In-Depth Analysis

### 1. Architecture: Job Manager, Task Managers

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           Flink Application                     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê             ‚îÇ
‚îÇ  ‚îÇ     Job Manager              ‚îÇ             ‚îÇ
‚îÇ  ‚îÇ  - Job scheduling            ‚îÇ             ‚îÇ
‚îÇ  ‚îÇ  - Checkpointing coordinator ‚îÇ             ‚îÇ
‚îÇ  ‚îÇ  - Resource management       ‚îÇ             ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò             ‚îÇ
‚îÇ             ‚îÇ                                  ‚îÇ
‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îÇ
‚îÇ    ‚ñº        ‚ñº        ‚ñº        ‚ñº              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ
‚îÇ  ‚îÇ TM 1‚îÇ  ‚îÇ TM 2‚îÇ  ‚îÇ TM 3‚îÇ  ‚îÇ TM 4‚îÇ        ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ
‚îÇ  Task Managers (execute tasks, hold state)   ‚îÇ
‚îÇ                                              ‚îÇ
‚îÇ  Event-by-event processing (not micro-batch)‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Key Components:**

| Component         | Role                                            |
|-------------------|-------------------------------------------------|
| **Job Manager**   | Coordinates job execution, manages checkpoints  |
| **Task Managers** | Execute dataflow tasks (map, filter, aggregate) |
| **State Backend** | Stores operator state (RocksDB, in-memory)      |
| **Checkpoints**   | Periodic snapshots for fault tolerance          |

---

### 2. DataStream API: Core Abstraction

**DataStream:** Unbounded stream of events.

```java
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.datastream.DataStream;

StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

// Read from Kafka
DataStream<String> stream = env.addSource(
        new FlinkKafkaConsumer<>("topic", new SimpleStringSchema(), properties)
);

// Transformations (event-by-event)
DataStream<WordCount> counts = stream
        .flatMap((String line, Collector<String> out) -> {
            for (String word : line.split(" ")) {
                out.collect(word);
            }
        })
        .map(word -> new WordCount(word, 1))
        .keyBy(wc -> wc.word)
        .sum("count");

// Sink to Kafka
counts.

addSink(new FlinkKafkaProducer<>("output-topic", schema, properties));

        env.

execute("WordCount");
```

**Key Operations:**

| Operation   | What It Does                        | Example                                                 |
|-------------|-------------------------------------|---------------------------------------------------------|
| `map()`     | Transform each event                | `.map(x -> x * 2)`                                      |
| `filter()`  | Keep matching events                | `.filter(x -> x > 10)`                                  |
| `flatMap()` | One-to-many transformation          | `.flatMap((line, out) -> out.collect(line.split(" ")))` |
| `keyBy()`   | Partition by key (for stateful ops) | `.keyBy(event -> event.userId)`                         |
| `window()`  | Group events into time windows      | `.window(TumblingEventTimeWindows.of(Time.minutes(5)))` |
| `reduce()`  | Aggregate events                    | `.reduce((a, b) -> a + b)`                              |

---

### 3. Event Time vs. Processing Time

**Event Time:** When the event actually occurred (embedded in data).  
**Processing Time:** When Flink processes the event.

```
Event Time:     [10:00] [10:01] [10:02] [10:03] [10:04]
                   ‚îÇ       ‚îÇ       ‚îÇ       ‚îÇ       ‚îÇ
                   ‚îÇ       ‚îÇ       ‚îÇ       ‚îÇ       ‚îÇ
                   ‚ñº       ‚ñº       ‚ñº       ‚ñº       ‚ñº
Processing Time: [10:05] [10:06] [10:05] [10:07] [10:08]
                          ‚Üë Out of order!
```

**Why Event Time Matters:**

- Events arrive out of order (network delays, clock skew)
- Event time ensures correct results (e.g., hourly aggregations based on when events happened, not when received)

**Configuration:**

```java
env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);

// Extract timestamp from event
stream.

assignTimestampsAndWatermarks(
        WatermarkStrategy .<Event>forBoundedOutOfOrderness(Duration.ofSeconds(5))
        .

withTimestampAssigner((event, timestamp) ->event.

getTimestamp())
        );
```

**Watermarks:** Signal that "all events up to time T have arrived."

---

### 4. Windowing: Group Events by Time

**Window Types:**

| Type         | Behavior                    | Use Case            |
|--------------|-----------------------------|---------------------|
| **Tumbling** | Fixed-size, non-overlapping | Hourly aggregations |
| **Sliding**  | Fixed-size, overlapping     | Moving averages     |
| **Session**  | Gap-based (timeout)         | User sessions       |

**Example: Tumbling Window**

```java
stream
        .keyBy(event ->event.userId)
        .

window(TumblingEventTimeWindows.of(Time.minutes(5)))  // 5-minute tumbling windows
        .

sum("value");

// Window 1: [00:00, 00:05)
// Window 2: [00:05, 00:10)
// Window 3: [00:10, 00:15)
```

**Example: Sliding Window**

```java
stream
        .keyBy(event ->event.userId)
        .

window(SlidingEventTimeWindows.of(Time.minutes(10),Time.

minutes(5)))
        .

sum("value");

// Window 1: [00:00, 00:10)
// Window 2: [00:05, 00:15) ‚Üê Overlaps with Window 1
// Window 3: [00:10, 00:20)
```

**Example: Session Window**

```java
stream
        .keyBy(event ->event.userId)
        .

window(EventTimeSessionWindows.withGap(Time.minutes(10)))  // 10-minute gap
        .

sum("value");

// Events at: 00:00, 00:05, 00:07 ‚Üí Session 1 (ends at 00:17)
// Events at: 00:25, 00:30 ‚Üí Session 2 (ends at 00:40)
```

---

### 5. Stateful Processing

**State:** Data maintained across events (e.g., running count, session data).

**State Types:**

| Type              | Scope                    | Use Case                      |
|-------------------|--------------------------|-------------------------------|
| **ValueState**    | Single value per key     | User balance, last login time |
| **ListState**     | List of values per key   | Shopping cart items           |
| **MapState**      | Key-value map per key    | User preferences              |
| **ReducingState** | Aggregated value per key | Running sum                   |

**Example: ValueState**

```java
class CountMapper extends RichFlatMapFunction<Event, Tuple2<String, Integer>> {
    private ValueState<Integer> countState;

    @Override
    public void open(Configuration config) {
        ValueStateDescriptor<Integer> descriptor =
                new ValueStateDescriptor<>("count", Integer.class, 0);
        countState = getRuntimeContext().getState(descriptor);
    }

    @Override
    public void flatMap(Event event, Collector<Tuple2<String, Integer>> out) throws Exception {
        int currentCount = countState.value();
        currentCount++;
        countState.update(currentCount);

        out.collect(new Tuple2<>(event.userId, currentCount));
    }
}
```

**State Backend:**

| Backend                 | Storage          | Performance | Use Case          |
|-------------------------|------------------|-------------|-------------------|
| **MemoryStateBackend**  | JVM heap         | ‚ö° Fastest   | Small state (<GB) |
| **FsStateBackend**      | Disk (HDFS/S3)   | Medium      | Medium state (GB) |
| **RocksDBStateBackend** | Embedded RocksDB | Slower      | Large state (TB)  |

---

### 6. Fault Tolerance: Exactly-Once Semantics

**Chandy-Lamport Algorithm (Distributed Snapshots):**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ        Flink Checkpointing                   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  1. Job Manager triggers checkpoint          ‚îÇ
‚îÇ  2. Source operators inject barriers         ‚îÇ
‚îÇ  3. Barriers flow through pipeline           ‚îÇ
‚îÇ  4. Each operator snapshots state when       ‚îÇ
‚îÇ     barrier arrives                          ‚îÇ
‚îÇ  5. All operators confirm ‚Üí checkpoint done  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Timeline:
T0: Checkpoint 1 starts ‚Üí Barrier injected
T1: Operator A snapshots state (barrier passes through A)
T2: Operator B snapshots state (barrier passes through B)
T3: Operator C snapshots state (barrier passes through C)
T4: Checkpoint 1 complete ‚Üí State saved to durable storage

If failure at T5:
  ‚Üí Restore from Checkpoint 1
  ‚Üí Replay events from Kafka (offset stored in checkpoint)
  ‚Üí Exactly-once guarantee maintained
```

**Configuration:**

```java
env.enableCheckpointing(5000);  // Checkpoint every 5 seconds
env.

getCheckpointConfig().

setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);
env.

getCheckpointConfig().

setMinPauseBetweenCheckpoints(1000);
env.

getCheckpointConfig().

setCheckpointTimeout(60000);
```

---

### 7. Flink SQL & Table API

**SQL on Streams:**

```java
import org.apache.flink.table.api.*;

StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);

// Register Kafka source as table
tableEnv.

executeSql(
    "CREATE TABLE transactions ("+
            "  user_id STRING,"+
            "  amount DOUBLE,"+
            "  transaction_time TIMESTAMP(3),"+
            "  WATERMARK FOR transaction_time AS transaction_time - INTERVAL '5' SECOND"+
            ") WITH ("+
            "  'connector' = 'kafka',"+
            "  'topic' = 'transactions',"+
            "  'properties.bootstrap.servers' = 'localhost:9092'"+
            ")"
);

// Run SQL query (streaming)
Table result = tableEnv.sqlQuery(
        "SELECT user_id, COUNT(*) as txn_count, SUM(amount) as total_amount " +
                "FROM transactions " +
                "GROUP BY user_id, TUMBLE(transaction_time, INTERVAL '1' HOUR)"
);

// Convert back to DataStream
DataStream<Row> resultStream = tableEnv.toAppendStream(result, Row.class);
```

---

### 8. CEP (Complex Event Processing)

**Pattern Matching on Streams:**

```java
import org.apache.flink.cep.*;
import org.apache.flink.cep.pattern.Pattern;

// Define pattern: 3 failed logins within 5 seconds
Pattern<LoginEvent, ?> pattern = Pattern
        .<LoginEvent>begin("first")
        .where(event -> event.status.equals("failed"))
        .next("second")
        .where(event -> event.status.equals("failed"))
        .next("third")
        .where(event -> event.status.equals("failed"))
        .within(Time.seconds(5));

        // Apply pattern to stream
        PatternStream<LoginEvent> patternStream = CEP.pattern(loginStream.keyBy(LoginEvent::getUserId), pattern);

        // Select matches
        DataStream<Alert> alerts = patternStream.select((PatternSelectFunction<LoginEvent, Alert>) pattern -> {
            LoginEvent first = pattern.get("first").get(0);
            return new Alert(first.getUserId(), "3 failed logins in 5 seconds");
        });
```

**Use Cases:**

- Fraud detection (suspicious patterns)
- Network intrusion detection
- IoT anomaly detection

---

### 9. When to Use Apache Flink

#### **‚úÖ Use Flink When:**

1. **Ultra-low latency** ‚Äî <100ms latency required (real-time dashboards, fraud detection)
2. **True event-by-event processing** ‚Äî Can't tolerate micro-batch delays
3. **Complex event processing (CEP)** ‚Äî Pattern matching on streams
4. **Stateful stream processing** ‚Äî Need to maintain state across events
5. **Event time processing** ‚Äî Out-of-order events with watermarks
6. **Exactly-once semantics** ‚Äî No data loss or duplication
7. **Streaming-first** ‚Äî Stream processing is primary workload (not batch)

#### **‚ùå Don't Use Flink When:**

1. **Batch-only workload** ‚Äî Use Spark (better batch support)
2. **Machine learning training** ‚Äî Use Spark MLlib (Flink's ML library is limited)
3. **Simple ETL** ‚Äî Use Spark (simpler API, better ecosystem)
4. **Interactive queries** ‚Äî Use Spark SQL or Presto
5. **Small data** ‚Äî Overhead not worth it (<1GB/day)

---

### 10. Real-World Examples

| Company      | Use Case                    | Why Flink?                                 |
|--------------|-----------------------------|--------------------------------------------|
| **Alibaba**  | Real-time recommendations   | Ultra-low latency (<50ms)                  |
| **Uber**     | Real-time fraud detection   | CEP for suspicious patterns                |
| **Netflix**  | Real-time anomaly detection | Stateful processing on billions of events  |
| **Lyft**     | Real-time pricing           | Sub-second latency for surge pricing       |
| **ING Bank** | Fraud detection             | Exactly-once guarantees for financial data |

---

### 11. Flink vs. Spark Streaming

| Feature              | Flink                              | Spark Streaming                   |
|----------------------|------------------------------------|-----------------------------------|
| **Processing model** | ‚úÖ True streaming (event-by-event)  | ‚ö†Ô∏è Micro-batch (500ms-2s batches) |
| **Latency**          | ‚úÖ Milliseconds (<10ms)             | ‚ö†Ô∏è Seconds (500ms-2s)             |
| **State management** | ‚úÖ Built-in stateful operators      | ‚ö†Ô∏è Manual (checkpointing)         |
| **Event time**       | ‚úÖ First-class support (watermarks) | ‚ö†Ô∏è Limited support                |
| **Exactly-once**     | ‚úÖ Native (Chandy-Lamport)          | ‚úÖ Yes (via Kafka offsets)         |
| **CEP**              | ‚úÖ Built-in pattern matching        | ‚ùå No                              |
| **ML Library**       | ‚ö†Ô∏è FlinkML (limited)               | ‚úÖ MLlib (mature)                  |
| **Batch processing** | ‚ö†Ô∏è Supported (but not primary)     | ‚úÖ Excellent                       |
| **Use case**         | Ultra-low latency streaming        | Unified batch + streaming + ML    |

---

### 12. Common Anti-Patterns

#### ‚ùå **1. Using Processing Time Instead of Event Time**

**Problem:**

```java
// Bad: Processing time (incorrect for out-of-order events)
stream.window(TumblingProcessingTimeWindows.of(Time.minutes(5)))
```

**Solution:**

```java
// Good: Event time (correct results)
stream
        .assignTimestampsAndWatermarks(...)
    .

window(TumblingEventTimeWindows.of(Time.minutes(5)))
```

#### ‚ùå **2. Not Configuring Checkpointing**

**Problem:** No fault tolerance (data loss on failure).

**Solution:**

```java
env.enableCheckpointing(5000);  // Checkpoint every 5 seconds
```

#### ‚ùå **3. Using Wrong State Backend**

**Problem:** Using MemoryStateBackend for large state (OOM).

**Solution:**

```java
// Use RocksDB for large state
env.setStateBackend(new RocksDBStateBackend("hdfs://checkpoints"));
```

---

### 13. Trade-offs Summary

| What You Gain                     | What You Sacrifice                                 |
|-----------------------------------|----------------------------------------------------|
| ‚úÖ True streaming (event-by-event) | ‚ùå More complex than Spark (steeper learning curve) |
| ‚úÖ Ultra-low latency (<10ms)       | ‚ùå Smaller ecosystem (fewer integrations)           |
| ‚úÖ Built-in stateful processing    | ‚ùå Limited ML library (vs. Spark MLlib)             |
| ‚úÖ First-class event time support  | ‚ùå Batch processing not as mature as Spark          |
| ‚úÖ Exactly-once semantics          | ‚ùå Higher operational complexity                    |

---

### 14. References

- **Apache Flink Documentation:** [https://flink.apache.org/](https://flink.apache.org/)
- **Stream Processing with Apache Flink (Book):** By Fabian Hueske & Vasiliki Kalavri
- **Flink Forward Conference:** [https://www.flink-forward.org/](https://www.flink-forward.org/)
- **Related Chapters:**
    - [2.3.7 Apache Spark Deep Dive](./2.3.7-apache-spark-deep-dive.md) ‚Äî Spark vs. Flink
    - [2.3.2 Kafka Deep Dive](./2.3.2-kafka-deep-dive.md) ‚Äî Kafka + Flink integration
    - [2.3.5 Batch vs Stream Processing](./2.3.5-batch-vs-stream-processing.md) ‚Äî When to use streaming

---

## ‚úèÔ∏è Design Challenge

### Problem

You're building a **real-time ride-hailing surge pricing system** (like Uber). Requirements:

1. **Latency:** Calculate surge multiplier within **100ms** of receiving demand/supply data
2. **Data sources:**
    - Ride requests (Kafka): user_id, location, timestamp
    - Driver availability (Kafka): driver_id, location, available (boolean)
3. **Logic:**
    - Calculate demand (ride requests) and supply (available drivers) per geographic zone (1km √ó 1km grid)
    - Surge multiplier = demand / supply (range: 1.0x to 5.0x)
    - Update surge multiplier every 30 seconds (sliding window)
4. **Scale:** 100,000 events/sec (peak), 1 million drivers worldwide
5. **Consistency:** Exactly-once semantics (no double-charging users)

**Question:** Would you use Flink or Spark? How would you design the windowing logic? How would you ensure <100ms
latency while maintaining exactly-once guarantees?

### Solution

#### üß© Scenario

- **System:** Real-time surge pricing
- **Latency requirement:** <100ms
- **Event rate:** 100K events/sec (50K ride requests + 50K driver updates)
- **Logic:** Demand/supply ratio per geographic zone (1km grid)
- **Window:** 30-second sliding window
- **Scale:** 1M drivers, 10M users

#### ‚úÖ Goal

- Calculate surge multiplier in <100ms
- Update every 30 seconds (sliding window)
- Exactly-once semantics (no double-charging)
- Handle 100K events/sec without backpressure
- Partition by geographic zone for parallelism

#### ‚öôÔ∏è Solution: Apache Flink (True Streaming Required)

**Why Flink over Spark?**

| Requirement             | Flink                     | Spark Streaming                     |
|-------------------------|---------------------------|-------------------------------------|
| **Latency**             | ‚úÖ <10ms event processing  | ‚ùå 500ms-2s micro-batch              |
| **<100ms end-to-end**   | ‚úÖ Achievable              | ‚ùå Impossible (micro-batch overhead) |
| **Stateful processing** | ‚úÖ Built-in window state   | ‚ö†Ô∏è Manual checkpointing             |
| **Event-by-event**      | ‚úÖ True streaming          | ‚ùå Batches                           |
| **Exactly-once**        | ‚úÖ Native (Chandy-Lamport) | ‚úÖ Yes (but with higher latency)     |

**Recommendation:** **Flink** ‚Äî Only way to meet <100ms latency.

**Implementation:**

```java
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.windowing.time.Time;
import org.apache.flink.streaming.api.windowing.assigners.SlidingEventTimeWindows;

StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

// Enable checkpointing (exactly-once)
env.

enableCheckpointing(5000);
env.

getCheckpointConfig().

setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);

// Read ride requests from Kafka
DataStream<RideRequest> rideRequests = env
        .addSource(new FlinkKafkaConsumer<>("ride-requests", schema, properties))
        .assignTimestampsAndWatermarks(
                WatermarkStrategy.<RideRequest>forBoundedOutOfOrderness(Duration.ofSeconds(5))
                        .withTimestampAssigner((event, timestamp) -> event.getTimestamp())
        );

// Read driver availability from Kafka
DataStream<DriverUpdate> driverUpdates = env
        .addSource(new FlinkKafkaConsumer<>("driver-updates", schema, properties))
        .assignTimestampsAndWatermarks(
                WatermarkStrategy.<DriverUpdate>forBoundedOutOfOrderness(Duration.ofSeconds(5))
                        .withTimestampAssigner((event, timestamp) -> event.getTimestamp())
        );

// Convert to geographic zone (1km grid)
DataStream<DemandEvent> demand = rideRequests
        .map(req -> new DemandEvent(
                getZoneId(req.getLatitude(), req.getLongitude()),  // e.g., "zone_123_456"
                1,  // 1 ride request
                req.getTimestamp()
        ));

DataStream<SupplyEvent> supply = driverUpdates
        .filter(driver -> driver.isAvailable())  // Only available drivers
        .map(driver -> new SupplyEvent(
                getZoneId(driver.getLatitude(), driver.getLongitude()),
                1,  // 1 available driver
                driver.getTimestamp()
        ));

// Union demand and supply streams
DataStream<ZoneEvent> events = demand.map(d -> new ZoneEvent(d.zoneId, d.count, 0, d.timestamp))
        .union(supply.map(s -> new ZoneEvent(s.zoneId, 0, s.count, s.timestamp)));

// Sliding window: 30-second window, update every 5 seconds
DataStream<SurgeMultiplier> surgeMultipliers = events
        .keyBy(event -> event.zoneId)
        .window(SlidingEventTimeWindows.of(Time.seconds(30), Time.seconds(5)))
        .aggregate(new AggregateFunction<ZoneEvent, ZoneStats, SurgeMultiplier>() {
            @Override
            public ZoneStats createAccumulator() {
                return new ZoneStats(0, 0);
            }

            @Override
            public ZoneStats add(ZoneEvent event, ZoneStats acc) {
                acc.demandCount += event.demandCount;
                acc.supplyCount += event.supplyCount;
                return acc;
            }

            @Override
            public SurgeMultiplier getResult(ZoneStats acc) {
                double surge = acc.supplyCount == 0 ? 5.0 :
                        Math.min(5.0, Math.max(1.0, (double) acc.demandCount / acc.supplyCount));
                return new SurgeMultiplier(acc.zoneId, surge, System.currentTimeMillis());
            }

            @Override
            public ZoneStats merge(ZoneStats a, ZoneStats b) {
                return new ZoneStats(a.demandCount + b.demandCount, a.supplyCount + b.supplyCount);
            }
        });

// Write to output Kafka (for ride service to consume)
surgeMultipliers.

addSink(
    new FlinkKafkaProducer<>("surge-multipliers", schema, properties)
);

        env.

execute("SurgePricingFlink");
```

**Key Design Decisions:**

1. **Geographic Zoning (1km √ó 1km):**
   ```java
   String getZoneId(double lat, double lon) {
       int latZone = (int) (lat * 100);  // 1km ‚âà 0.01 degrees
       int lonZone = (int) (lon * 100);
       return "zone_" + latZone + "_" + lonZone;
   }
   ```

2. **Sliding Window (30s window, 5s slide):**
    - Updates surge every 5 seconds (not too frequent, but responsive)
    - 30-second window captures recent demand/supply trends

3. **Keyed by Zone:**
    - Parallel processing per zone (1000s of zones = 1000s of parallel tasks)
    - Each zone's state isolated (no cross-zone coordination needed)

#### ‚ö†Ô∏è Latency Breakdown

```
Event arrives ‚Üí Kafka (10ms) ‚Üí Flink ingestion (5ms) ‚Üí 
Window aggregation (10ms) ‚Üí Kafka output (10ms) ‚Üí 
Ride service consumes (10ms) ‚Üí Total: 45ms ‚úÖ

<100ms requirement met!
```

**Optimization Tips:**

1. **Parallelism:**
   ```java
   env.setParallelism(100);  // 100 parallel tasks
   ```

2. **Kafka Partitioning:**
    - Partition Kafka by zone_id (co-locate data for same zone)
    - Reduces network shuffling

3. **State Backend:**
   ```java
   env.setStateBackend(new RocksDBStateBackend("hdfs://checkpoints"));
   ```
    - RocksDB for large state (1M zones √ó window state)

#### üß† Handling Scale (100K events/sec)

**Challenge:** 100K events/sec = high throughput. How to avoid backpressure?

**Solution:**

1. **Partition Kafka Topic:**
    - 100 partitions (1K events/sec per partition)
    - Each Flink task consumes 1 partition

2. **Flink Parallelism:**
    - 100 parallel tasks (matches Kafka partitions)
    - Each task handles 1K events/sec (easily manageable)

3. **Checkpointing:**
    - Async checkpointing (doesn't block processing)
    - Incremental checkpoints (RocksDB)

#### ‚úÖ Final Answer

| Aspect            | Decision                            | Reason                                              |
|-------------------|-------------------------------------|-----------------------------------------------------|
| **Framework**     | **Apache Flink**                    | Only way to achieve <100ms latency (true streaming) |
| **Window**        | Sliding (30s window, 5s slide)      | Updates every 5s, captures recent trends            |
| **Partitioning**  | Key by zone_id (geographic hash)    | Parallel processing per zone                        |
| **Latency**       | ~45ms (event ‚Üí output)              | Meets <100ms requirement ‚úÖ                          |
| **Throughput**    | 100K events/sec                     | 100 Kafka partitions √ó 1K events/sec each           |
| **Exactly-Once**  | Flink checkpointing + Kafka offsets | No double-charging users                            |
| **State Backend** | RocksDB                             | Large state (1M zones √ó window data)                |
| **Trade-off**     | More complex than Spark             | Gain: Sub-100ms latency (impossible with Spark)     |

**Performance Metrics:**

- **End-to-end latency:** 45ms (p99)
- **Throughput:** 100K events/sec
- **State size:** ~10GB (1M zones √ó 10KB state/zone)
- **Checkpoint interval:** 5 seconds
- **Recovery time:** <30 seconds (from last checkpoint)

**Why NOT Spark:**

- ‚ùå **Micro-batch latency:** 500ms-2s (cannot meet <100ms)
- ‚ùå **Batch processing overhead:** Even with 100ms trigger, batching adds latency
- ‚úÖ **Flink is the only option** for <100ms real-time requirements

**When to Reconsider:**

- If latency requirement relaxed to >1 second ‚Üí Spark Streaming viable (simpler ops)
- If need ML model training ‚Üí Hybrid (Spark for training, Flink for scoring)
- If batch analytics needed ‚Üí Hybrid (Flink for streaming, Spark for batch)

