# 2.3.7 Apache Spark Deep Dive: The Unified Analytics Engine

## Intuitive Explanation

Apache Spark is a **unified analytics engine** for large-scale data processing. Unlike traditional MapReduce (which is
disk-based and slow), Spark processes data **in-memory** and is **100x faster**. Think of it as a powerful framework
that lets you process terabytes of data across hundreds of machines with simple code, whether you're doing batch
processing, stream processing, machine learning, or SQL queries.

- **In-Memory Computing:** Caches data in RAM for iterative algorithms (vs. MapReduce's disk I/O)
- **Unified API:** Same code for batch, streaming, SQL, and ML
- **Fault Tolerant:** Automatically recovers from failures (lineage-based recovery)
- **Scalable:** Process petabytes of data on thousands of nodes
- **Use Cases:** ETL pipelines, log analysis, machine learning training, real-time analytics

**The Power:** Process 10TB of data in minutes (vs. hours in Hadoop MapReduce).

---

## In-Depth Analysis

### 1. Architecture: Driver, Executors, Cluster Manager

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ            Spark Application                     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê               ‚îÇ
‚îÇ  ‚îÇ     Driver Program           ‚îÇ               ‚îÇ
‚îÇ  ‚îÇ  - SparkContext              ‚îÇ               ‚îÇ
‚îÇ  ‚îÇ  - DAG Scheduler             ‚îÇ               ‚îÇ
‚îÇ  ‚îÇ  - Task Scheduler            ‚îÇ               ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚îÇ
‚îÇ             ‚îÇ                                    ‚îÇ
‚îÇ             ‚ñº                                    ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê               ‚îÇ
‚îÇ  ‚îÇ   Cluster Manager             ‚îÇ               ‚îÇ
‚îÇ  ‚îÇ  (YARN / Mesos / K8s)        ‚îÇ               ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚îÇ
‚îÇ             ‚îÇ                                    ‚îÇ
‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ
‚îÇ    ‚ñº        ‚ñº        ‚ñº        ‚ñº                ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îÇ
‚îÇ  ‚îÇExec‚îÇ  ‚îÇExec‚îÇ  ‚îÇExec‚îÇ  ‚îÇExec‚îÇ              ‚îÇ
‚îÇ  ‚îÇ 1  ‚îÇ  ‚îÇ 2  ‚îÇ  ‚îÇ 3  ‚îÇ  ‚îÇ 4  ‚îÇ              ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ
‚îÇ  (Worker nodes with cached data)               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Key Components:**

| Component           | Role                                                 | Analogy                           |
|---------------------|------------------------------------------------------|-----------------------------------|
| **Driver**          | Orchestrates the job, maintains metadata             | Project manager (plans tasks)     |
| **Executors**       | Execute tasks, cache data in memory                  | Workers (do actual work)          |
| **Cluster Manager** | Allocates resources (CPU, memory)                    | HR department (assigns resources) |
| **SparkContext**    | Entry point to Spark, coordinates driver & executors | Main communication channel        |

---

### 2. Core Abstraction: RDD (Resilient Distributed Dataset)

**RDD:** Immutable, distributed collection of objects that can be processed in parallel.

```python
from pyspark import SparkContext

sc = SparkContext("local", "WordCount")

# Create RDD from file
lines = sc.textFile("hdfs://path/to/file.txt")  # RDD[String]

# Transformations (lazy evaluation)
words = lines.flatMap(lambda line: line.split(" "))  # RDD[String]
word_counts = words.map(lambda word: (word, 1))      # RDD[(String, Int)]

# Action (triggers computation)
result = word_counts.reduceByKey(lambda a, b: a + b).collect()

# Result: [("hello", 5), ("world", 3), ...]
```

**RDD Properties:**

| Property        | Explanation                                               |
|-----------------|-----------------------------------------------------------|
| **Immutable**   | Once created, cannot be modified (create new RDD instead) |
| **Distributed** | Automatically partitioned across cluster                  |
| **Resilient**   | Fault-tolerant via lineage (recomputes lost partitions)   |
| **Lazy**        | Transformations not executed until action is called       |

**Transformations (Lazy):**

| Transformation   | What It Does                     | Example                            |
|------------------|----------------------------------|------------------------------------|
| `map(f)`         | Apply function to each element   | `rdd.map(lambda x: x * 2)`         |
| `filter(f)`      | Keep elements matching predicate | `rdd.filter(lambda x: x > 10)`     |
| `flatMap(f)`     | Map + flatten (one-to-many)      | `rdd.flatMap(lambda x: x.split())` |
| `reduceByKey(f)` | Aggregate by key                 | `rdd.reduceByKey(lambda a,b: a+b)` |
| `groupByKey()`   | Group values by key              | `rdd.groupByKey()`                 |
| `join()`         | Join two RDDs by key             | `rdd1.join(rdd2)`                  |

**Actions (Trigger Execution):**

| Action             | What It Does                  | Example                          |
|--------------------|-------------------------------|----------------------------------|
| `collect()`        | Return all elements to driver | `rdd.collect()`                  |
| `count()`          | Count number of elements      | `rdd.count()`                    |
| `take(n)`          | Return first n elements       | `rdd.take(10)`                   |
| `saveAsTextFile()` | Write to HDFS/S3              | `rdd.saveAsTextFile("s3://...")` |
| `reduce(f)`        | Aggregate all elements        | `rdd.reduce(lambda a,b: a+b)`    |

---

### 3. DataFrame API (High-Level Abstraction)

**DataFrames:** RDDs with schema (like SQL tables).

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("DataFrameExample").getOrCreate()

# Create DataFrame from file
df = spark.read.json("hdfs://path/to/users.json")

# Schema
df.printSchema()
# root
#  |-- name: string
#  |-- age: integer
#  |-- city: string

# SQL-like operations
result = df.filter(df.age > 25) \
           .groupBy("city") \
           .count() \
           .orderBy("count", ascending=False)

result.show()
```

**DataFrame vs. RDD:**

| Feature          | RDD                     | DataFrame                      |
|------------------|-------------------------|--------------------------------|
| **API**          | Low-level (map, filter) | High-level (SQL-like)          |
| **Optimization** | Manual                  | Catalyst optimizer (automatic) |
| **Performance**  | Good                    | Better (optimized query plans) |
| **Schema**       | No schema               | Typed schema                   |
| **Use case**     | Complex custom logic    | SQL-like queries, ETL          |

---

### 4. Spark SQL

**Run SQL queries on DataFrames:**

```python
# Register DataFrame as temp table
df.createOrReplaceTempView("users")

# Run SQL query
result = spark.sql("""
    SELECT city, COUNT(*) as user_count
    FROM users
    WHERE age > 25
    GROUP BY city
    ORDER BY user_count DESC
""")

result.show()
```

**Benefits:**

- ‚úÖ Familiar SQL syntax (easy for analysts)
- ‚úÖ Catalyst optimizer (automatic query optimization)
- ‚úÖ Interoperability (mix SQL and DataFrame code)

---

### 5. Structured Streaming (Real-Time Processing)

**Process streaming data with DataFrame API:**

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("StreamingWordCount").getOrCreate()

# Read streaming data from Kafka
lines = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "logs") \
    .load() \
    .selectExpr("CAST(value AS STRING)")

# Process stream (same as batch!)
word_counts = lines.select(explode(split(col("value"), " ")).alias("word")) \
                   .groupBy("word") \
                   .count()

# Write to console (for demo) or Kafka/HDFS
query = word_counts.writeStream \
    .outputMode("complete") \
    .format("console") \
    .start()

query.awaitTermination()
```

**Streaming Modes:**

| Mode         | Behavior                    | Use Case                  |
|--------------|-----------------------------|---------------------------|
| **Append**   | Only new rows written       | ETL, event logging        |
| **Complete** | Entire result table written | Aggregations (count, sum) |
| **Update**   | Only updated rows written   | Deduplication, joins      |

---

### 6. MLlib (Machine Learning)

**Built-in machine learning library:**

```python
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.feature import VectorAssembler

# Load data
training = spark.read.csv("data/training.csv", header=True, inferSchema=True)

# Feature engineering
assembler = VectorAssembler(inputCols=["age", "income"], outputCol="features")
training_data = assembler.transform(training)

# Train model
lr = LogisticRegression(labelCol="label", featuresCol="features")
model = lr.fit(training_data)

# Predict
test = spark.read.csv("data/test.csv", header=True, inferSchema=True)
test_data = assembler.transform(test)
predictions = model.transform(test_data)

predictions.select("features", "label", "prediction").show()
```

**MLlib Algorithms:**

| Category                | Algorithms                                              |
|-------------------------|---------------------------------------------------------|
| **Classification**      | Logistic Regression, Decision Trees, Random Forest, GBT |
| **Regression**          | Linear Regression, Decision Trees, Random Forest        |
| **Clustering**          | K-Means, Bisecting K-Means, GMM                         |
| **Recommendation**      | ALS (Collaborative Filtering)                           |
| **Feature Engineering** | VectorAssembler, StandardScaler, PCA                    |

---

### 7. Performance Optimization

#### **7.1 Caching/Persistence**

```python
# Cache in memory (for iterative algorithms)
df.cache()  # or df.persist(StorageLevel.MEMORY_AND_DISK)

# Use cached DataFrame multiple times
df.filter("age > 25").count()  # First execution: reads from source + caches
df.filter("age > 30").count()  # Second execution: uses cache (fast!)

# Unpersist when done
df.unpersist()
```

**Storage Levels:**

| Level             | Memory | Disk | Serialized | Use Case                                             |
|-------------------|--------|------|------------|------------------------------------------------------|
| `MEMORY_ONLY`     | ‚úÖ      | ‚ùå    | ‚ùå          | Fast iterative algorithms (if data fits in RAM)      |
| `MEMORY_AND_DISK` | ‚úÖ      | ‚úÖ    | ‚ùå          | Default (spills to disk if RAM full)                 |
| `MEMORY_ONLY_SER` | ‚úÖ      | ‚ùå    | ‚úÖ          | Save memory (slower due to serialization)            |
| `DISK_ONLY`       | ‚ùå      | ‚úÖ    | ‚ùå          | Data doesn't fit in RAM, still faster than recompute |

#### **7.2 Partitioning**

```python
# Repartition (shuffle data across cluster)
df = df.repartition(100)  # 100 partitions

# Coalesce (reduce partitions without shuffle)
df = df.coalesce(10)  # Reduce to 10 partitions (no shuffle)

# Partition by column (for joins)
df = df.repartition("user_id")  # All rows with same user_id in same partition
```

**Partitioning Strategy:**

- **Too few partitions:** Underutilized cluster (some executors idle)
- **Too many partitions:** Overhead (scheduling, coordination)
- **Rule of thumb:** 2-4 partitions per CPU core

#### **7.3 Broadcast Variables**

```python
# Broadcast small dataset to all executors (avoid shuffle)
small_dict = {"user1": "Alice", "user2": "Bob"}
broadcast_dict = sc.broadcast(small_dict)

# Use in transformations
def lookup_name(user_id):
    return broadcast_dict.value.get(user_id, "Unknown")

df = df.withColumn("name", lookup_name(col("user_id")))
```

**When to Use:** Broadcast small datasets (<200MB) to avoid expensive shuffles.

---

### 8. Fault Tolerance: Lineage-Based Recovery

**How Spark recovers from failures:**

```
RDD1 (input)
  ‚îÇ
  ‚îú‚îÄ map() ‚îÄ> RDD2
  ‚îÇ
  ‚îú‚îÄ filter() ‚îÄ> RDD3
  ‚îÇ
  ‚îî‚îÄ reduceByKey() ‚îÄ> RDD4 (result)

If Executor 2 fails while computing RDD3:
  1. Spark detects failure
  2. Replays transformations from RDD1 ‚Üí RDD3 on another executor
  3. No data loss (because lineage is tracked)
```

**Benefits:**

- ‚úÖ **No checkpointing overhead** (unlike Hadoop, which writes to disk)
- ‚úÖ **Fast recovery** (only recompute lost partitions)
- ‚úÖ **Automatic** (no manual intervention)

**Trade-off:** Long lineage chains can slow recovery (use `checkpoint()` for very long chains).

---

### 9. When to Use Apache Spark

#### **‚úÖ Use Spark When:**

1. **Batch processing** ‚Äî ETL pipelines, log analysis, data transformation
2. **Iterative algorithms** ‚Äî Machine learning (K-Means, PageRank, etc.)
3. **Interactive queries** ‚Äî Ad-hoc SQL queries on large datasets
4. **Streaming** ‚Äî Near real-time processing (Structured Streaming)
5. **Unified workloads** ‚Äî Need batch + streaming + ML in one framework
6. **Large datasets** ‚Äî Terabytes to petabytes (100GB+)
7. **In-memory processing** ‚Äî Data fits in cluster RAM (huge speedup)

#### **‚ùå Don't Use Spark When:**

1. **Small data** ‚Äî <10GB (overkill, use Pandas/SQL)
2. **Ultra-low latency** ‚Äî <100ms (use Flink or real-time databases)
3. **Simple queries** ‚Äî Use SQL database (PostgreSQL, BigQuery)
4. **Transactional workloads** ‚Äî OLTP (use PostgreSQL/MySQL)
5. **Small cluster** ‚Äî <3 nodes (overhead not worth it)

---

### 10. Real-World Examples

| Company     | Use Case                                       | Why Spark?                                  |
|-------------|------------------------------------------------|---------------------------------------------|
| **Netflix** | Movie recommendation (collaborative filtering) | MLlib for training on 100TB+ data           |
| **Uber**    | ETL pipelines (trip data processing)           | Process 100TB+ daily in batch + stream      |
| **Airbnb**  | Pricing optimization                           | Machine learning on historical booking data |
| **NASA**    | Log analysis (Mars Rover)                      | Process terabytes of sensor logs            |
| **eBay**    | Real-time fraud detection                      | Structured Streaming + MLlib                |

---

### 11. Spark vs. Other Frameworks

| Feature             | Spark                        | Hadoop MapReduce             | Flink                             |
|---------------------|------------------------------|------------------------------|-----------------------------------|
| **Speed**           | ‚ö° 100x faster (in-memory)    | ‚ùå Slow (disk-based)          | ‚ö° Faster (for streaming)          |
| **API**             | High-level (DataFrame, SQL)  | Low-level (Map/Reduce)       | High-level (DataStream)           |
| **Streaming**       | Micro-batch (near real-time) | ‚ùå Not designed for streaming | ‚úÖ True streaming (event-by-event) |
| **ML Library**      | ‚úÖ MLlib                      | ‚ùå No                         | ‚ö†Ô∏è FlinkML (limited)              |
| **Fault Tolerance** | Lineage-based (fast)         | Checkpoint-based (slow)      | Checkpoint-based                  |
| **Use Case**        | Batch + Streaming + ML       | Legacy batch processing      | Real-time streaming               |

---

### 12. Common Anti-Patterns

#### ‚ùå **1. Collecting Large Datasets to Driver**

**Problem:**

```python
# Bad: Collects 1TB to driver (out of memory)
all_data = df.collect()
```

**Solution:**

```python
# Good: Process in distributed manner
df.write.parquet("s3://output/")
```

#### ‚ùå **2. Not Caching Reused DataFrames**

**Problem:**

```python
# Bad: Recomputes df three times
df.filter("age > 25").count()
df.filter("age > 30").count()
df.filter("age > 35").count()
```

**Solution:**

```python
# Good: Cache once, reuse
df.cache()
df.filter("age > 25").count()
df.filter("age > 30").count()
df.unpersist()
```

#### ‚ùå **3. Using groupByKey Instead of reduceByKey**

**Problem:**

```python
# Bad: Shuffles all data (slow)
rdd.groupByKey().mapValues(sum)
```

**Solution:**

```python
# Good: Reduces before shuffle (fast)
rdd.reduceByKey(lambda a, b: a + b)
```

---

### 13. Trade-offs Summary

| What You Gain                             | What You Sacrifice                                 |
|-------------------------------------------|----------------------------------------------------|
| ‚úÖ 100x faster than MapReduce (in-memory)  | ‚ùå Higher memory requirements                       |
| ‚úÖ Unified API (batch, streaming, ML, SQL) | ‚ùå Not true real-time (micro-batch, ~100ms latency) |
| ‚úÖ Fault tolerant (lineage-based recovery) | ‚ùå Long lineage chains can slow recovery            |
| ‚úÖ Easy to use (high-level APIs)           | ‚ùå Steep learning curve (for optimization)          |
| ‚úÖ Scales to petabytes                     | ‚ùå Overkill for small data (<10GB)                  |

---

### 14. References

- **Apache Spark Documentation:** [https://spark.apache.org/docs/](https://spark.apache.org/docs/)
- **Spark: The Definitive Guide (Book):** By Bill Chambers & Matei Zaharia
- **Databricks Blog:** [https://databricks.com/blog](https://databricks.com/blog)
- **Related Chapters:**
    - [2.3.5 Batch vs Stream Processing](./2.3.5-batch-vs-stream-processing.md) ‚Äî When to use batch vs. streaming
    - [2.3.2 Kafka Deep Dive](./2.3.2-kafka-deep-dive.md) ‚Äî Kafka + Spark integration
    - [2.3.8 Apache Flink Deep Dive](./2.3.8-apache-flink-deep-dive.md) ‚Äî Spark vs. Flink

---

## ‚úèÔ∏è Design Challenge

### Problem

You're building a **real-time fraud detection system** for a payment processor. Requirements:

1. **Data volume:** 10,000 transactions/sec = 864M transactions/day
2. **Processing:**
    - Batch: Daily model training on historical data (1TB/day)
    - Streaming: Real-time fraud scoring (<5 second latency)
3. **Features:**
    - Transaction velocity (transactions per user per hour)
    - Geographic anomaly (transaction from unusual location)
    - Merchant risk score (based on historical fraud rate)
4. **ML Model:** Train daily on last 30 days of data, deploy to streaming pipeline
5. **Scale:** 100 million users, 1 billion transactions/month

**Question:** How would you design this system using Spark? Would you use batch, streaming, or both? How would you train
and deploy the ML model? What about latency requirements?

### Solution

#### üß© Scenario

- **System:** Real-time fraud detection
- **Write load:** 10,000 transactions/sec
- **Batch workload:** Train ML model daily on 1TB data (last 30 days)
- **Streaming workload:** Score transactions in real-time (<5 sec latency)
- **Data:** user_id, transaction_id, amount, merchant, location, timestamp
- **ML Model:** Logistic regression or gradient boosted trees

#### ‚úÖ Goal

- Train accurate fraud model daily (batch processing)
- Score transactions in real-time (<5 sec)
- Handle 10K transactions/sec without dropping data
- Automatically deploy updated model to streaming pipeline
- Minimize false positives (don't block legitimate transactions)

#### ‚öôÔ∏è Solution: Hybrid Spark Architecture (Batch + Streaming)

**Architecture:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           Fraud Detection System (Spark)             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                      ‚îÇ
‚îÇ  1. BATCH PIPELINE (Daily Model Training)           ‚îÇ
‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê               ‚îÇ
‚îÇ     ‚îÇ  Spark Batch Job             ‚îÇ               ‚îÇ
‚îÇ     ‚îÇ  - Read last 30 days (S3)    ‚îÇ               ‚îÇ
‚îÇ     ‚îÇ  - Feature engineering        ‚îÇ               ‚îÇ
‚îÇ     ‚îÇ  - Train ML model (MLlib)     ‚îÇ               ‚îÇ
‚îÇ     ‚îÇ  - Save model to S3           ‚îÇ               ‚îÇ
‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚îÇ
‚îÇ                ‚îÇ                                     ‚îÇ
‚îÇ                ‚ñº                                     ‚îÇ
‚îÇ  2. MODEL DEPLOYMENT                                ‚îÇ
‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê               ‚îÇ
‚îÇ     ‚îÇ  Model Registry (S3/MLflow)  ‚îÇ               ‚îÇ
‚îÇ     ‚îÇ  - Store model versions      ‚îÇ               ‚îÇ
‚îÇ     ‚îÇ  - Track performance          ‚îÇ               ‚îÇ
‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚îÇ
‚îÇ                ‚îÇ                                     ‚îÇ
‚îÇ                ‚ñº                                     ‚îÇ
‚îÇ  3. STREAMING PIPELINE (Real-Time Scoring)         ‚îÇ
‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê               ‚îÇ
‚îÇ     ‚îÇ  Structured Streaming         ‚îÇ               ‚îÇ
‚îÇ     ‚îÇ  - Read from Kafka            ‚îÇ               ‚îÇ
‚îÇ     ‚îÇ  - Feature engineering        ‚îÇ               ‚îÇ
‚îÇ     ‚îÇ  - Score with ML model        ‚îÇ               ‚îÇ
‚îÇ     ‚îÇ  - Write to output Kafka      ‚îÇ               ‚îÇ
‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Implementation:**

**1. Batch Pipeline (Daily Model Training):**

```python
from pyspark.sql import SparkSession
from pyspark.ml.classification import GBTClassifier
from pyspark.ml.feature import VectorAssembler

# Initialize Spark
spark = SparkSession.builder \
    .appName("FraudModelTraining") \
    .config("spark.executor.memory", "16g") \
    .getOrCreate()

# Read historical data (last 30 days from S3)
df = spark.read.parquet("s3://fraud-data/transactions/")
df = df.filter(col("transaction_date") >= current_date() - 30)

# Feature engineering
df = df.withColumn("velocity", 
    # Count transactions per user in last hour (window function)
    count("transaction_id").over(
        Window.partitionBy("user_id")
              .orderBy("timestamp")
              .rangeBetween(-3600, 0)
    )
)

df = df.withColumn("geo_anomaly",
    # Distance from user's usual location
    udf_calculate_distance(col("location"), col("user_home_location"))
)

# Prepare features
feature_cols = ["amount", "velocity", "geo_anomaly", "merchant_risk_score"]
assembler = VectorAssembler(inputCols=feature_cols, outputCol="features")
training_data = assembler.transform(df)

# Train model
gbt = GBTClassifier(labelCol="is_fraud", featuresCol="features", maxIter=100)
model = gbt.fit(training_data)

# Evaluate model
predictions = model.transform(test_data)
accuracy = predictions.filter(col("is_fraud") == col("prediction")).count() / predictions.count()
print(f"Model accuracy: {accuracy}")

# Save model
model.write().overwrite().save("s3://fraud-models/gbt_model_2024-01-15")
```

**2. Streaming Pipeline (Real-Time Scoring):**

```python
from pyspark.sql import SparkSession
from pyspark.ml.classification import GBTClassificationModel

# Initialize Spark Streaming
spark = SparkSession.builder \
    .appName("FraudDetectionStreaming") \
    .getOrCreate()

# Load trained model
model = GBTClassificationModel.load("s3://fraud-models/gbt_model_2024-01-15")

# Read streaming data from Kafka
transactions = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "transactions") \
    .option("startingOffsets", "latest") \
    .load()

# Parse JSON
from pyspark.sql.functions import from_json, col
schema = StructType([
    StructField("transaction_id", StringType()),
    StructField("user_id", StringType()),
    StructField("amount", DoubleType()),
    StructField("merchant", StringType()),
    StructField("location", StringType()),
    StructField("timestamp", TimestampType())
])

parsed = transactions.select(
    from_json(col("value").cast("string"), schema).alias("data")
).select("data.*")

# Feature engineering (same as batch)
features = parsed.withColumn("velocity", ...)  # Real-time velocity calculation
features = features.withColumn("geo_anomaly", ...)

# Assemble features
feature_cols = ["amount", "velocity", "geo_anomaly", "merchant_risk_score"]
assembler = VectorAssembler(inputCols=feature_cols, outputCol="features")
features_assembled = assembler.transform(features)

# Score with ML model
predictions = model.transform(features_assembled)

# Write to output Kafka (fraud alerts)
fraud_alerts = predictions.filter(col("prediction") == 1.0)

query = fraud_alerts \
    .selectExpr("transaction_id", "user_id", "amount", "probability") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("topic", "fraud-alerts") \
    .option("checkpointLocation", "s3://checkpoints/fraud-detection") \
    .start()

query.awaitTermination()
```

#### ‚ö†Ô∏è Handling Latency Requirements (<5 seconds)

**Problem:** Spark Structured Streaming uses micro-batches (default: 500ms-2s latency). Is this fast enough?

**Analysis:**

```
Transaction ‚Üí Kafka ‚Üí Spark Streaming ‚Üí Fraud Score ‚Üí Alert
   (0ms)      (10ms)       (2000ms)         (100ms)    (10ms)

Total: ~2.1 seconds ‚úÖ (meets <5 second requirement)
```

**Optimization Tips:**

1. **Reduce micro-batch interval:**
   ```python
   .trigger(processingTime='500 milliseconds')  # Default: 2 seconds
   ```

2. **Partition Kafka topic:**
    - More partitions = more parallelism
    - Rule: partitions ‚â• number of Spark executors

3. **Optimize feature engineering:**
    - Pre-compute merchant risk scores (batch job)
    - Use broadcast variables for lookups

#### üß† Model Deployment Strategy

**Challenge:** How to update the model daily without downtime?

**Solution: Blue-Green Deployment**

```python
# Daily batch job
def train_and_deploy_model():
    # Train new model
    new_model = train_model(date=today)
    
    # Save with date suffix
    model_path = f"s3://fraud-models/gbt_model_{today}"
    new_model.write().save(model_path)
    
    # Update "latest" pointer (atomic operation)
    with open("s3://fraud-models/LATEST", "w") as f:
        f.write(model_path)

# Streaming job (reads latest model every 5 minutes)
def get_latest_model():
    with open("s3://fraud-models/LATEST") as f:
        model_path = f.read()
    return GBTClassificationModel.load(model_path)

# Reload model periodically
while True:
    model = get_latest_model()
    # Use model for 5 minutes
    time.sleep(300)
```

#### ‚úÖ Final Answer

| Aspect                  | Decision                                     | Reason                                              |
|-------------------------|----------------------------------------------|-----------------------------------------------------|
| **Architecture**        | **Hybrid (Spark Batch + Streaming)**         | Batch for training, streaming for real-time scoring |
| **Batch Job**           | Daily training on 1TB (last 30 days)         | MLlib GBT model, runs overnight                     |
| **Streaming**           | Structured Streaming (Kafka ‚Üí Spark ‚Üí Kafka) | Real-time scoring with <2s latency                  |
| **Latency**             | ~2 seconds (micro-batch)                     | Meets <5 second requirement ‚úÖ                       |
| **Model Deployment**    | Blue-green (daily updates)                   | Zero downtime, reload every 5 minutes               |
| **Throughput**          | 10K transactions/sec                         | Kafka partitioning + Spark parallelism              |
| **Feature Engineering** | Velocity, geo anomaly, merchant risk         | Same logic in batch and streaming                   |
| **Trade-off**           | Micro-batch (not true streaming)             | Gain: Easier development, unified API               |

**Performance Metrics:**

- **Batch training:** 2-3 hours (1TB data, 100 nodes)
- **Streaming latency:** 2 seconds (p99)
- **Throughput:** 10K+ transactions/sec
- **Model accuracy:** 95%+ (with daily retraining)

**Why Spark (Not Flink)?**

- ‚úÖ **Unified API:** Same code for batch training and streaming scoring
- ‚úÖ **MLlib:** Built-in ML library (Flink's ML library is limited)
- ‚úÖ **Micro-batch is enough:** <2s latency meets requirements
- ‚ùå **Not true streaming:** If needed <100ms, use Flink

**When to Reconsider:**

- If latency requirement <500ms ‚Üí Use Flink (true streaming)
- If model training takes >8 hours ‚Üí Use distributed ML frameworks (Horovod, Ray)
- If need <100ms latency ‚Üí Use pre-computed features + lightweight scoring (no Spark)

