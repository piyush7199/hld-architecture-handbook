# 2.3.7 Apache Spark Deep Dive: The Unified Analytics Engine

## Intuitive Explanation

Apache Spark is a **unified analytics engine** for large-scale data processing. Unlike traditional MapReduce (which is disk-based and slow), Spark processes data **in-memory** and is **100x faster**. Think of it as a powerful framework that lets you process terabytes of data across hundreds of machines with simple code, whether you're doing batch processing, stream processing, machine learning, or SQL queries.

- **In-Memory Computing:** Caches data in RAM for iterative algorithms (vs. MapReduce's disk I/O)
- **Unified API:** Same code for batch, streaming, SQL, and ML
- **Fault Tolerant:** Automatically recovers from failures (lineage-based recovery)
- **Scalable:** Process petabytes of data on thousands of nodes
- **Use Cases:** ETL pipelines, log analysis, machine learning training, real-time analytics

**The Power:** Process 10TB of data in minutes (vs. hours in Hadoop MapReduce).

---

## In-Depth Analysis

### 1. Architecture: Driver, Executors, Cluster Manager

```
┌─────────────────────────────────────────────────┐
│            Spark Application                     │
├─────────────────────────────────────────────────┤
│  ┌──────────────────────────────┐               │
│  │     Driver Program           │               │
│  │  - SparkContext              │               │
│  │  - DAG Scheduler             │               │
│  │  - Task Scheduler            │               │
│  └──────────┬───────────────────┘               │
│             │                                    │
│             ▼                                    │
│  ┌──────────────────────────────┐               │
│  │   Cluster Manager             │               │
│  │  (YARN / Mesos / K8s)        │               │
│  └──────────┬───────────────────┘               │
│             │                                    │
│    ┌────────┼────────┬────────┐                │
│    ▼        ▼        ▼        ▼                │
│  ┌────┐  ┌────┐  ┌────┐  ┌────┐              │
│  │Exec│  │Exec│  │Exec│  │Exec│              │
│  │ 1  │  │ 2  │  │ 3  │  │ 4  │              │
│  └────┘  └────┘  └────┘  └────┘              │
│  (Worker nodes with cached data)               │
└─────────────────────────────────────────────────┘
```

**Key Components:**

| Component | Role | Analogy |
|-----------|------|---------|
| **Driver** | Orchestrates the job, maintains metadata | Project manager (plans tasks) |
| **Executors** | Execute tasks, cache data in memory | Workers (do actual work) |
| **Cluster Manager** | Allocates resources (CPU, memory) | HR department (assigns resources) |
| **SparkContext** | Entry point to Spark, coordinates driver & executors | Main communication channel |

---

### 2. Core Abstraction: RDD (Resilient Distributed Dataset)

**RDD:** Immutable, distributed collection of objects that can be processed in parallel.

```python
from pyspark import SparkContext

sc = SparkContext("local", "WordCount")

# Create RDD from file
lines = sc.textFile("hdfs://path/to/file.txt")  # RDD[String]

# Transformations (lazy evaluation)
words = lines.flatMap(lambda line: line.split(" "))  # RDD[String]
word_counts = words.map(lambda word: (word, 1))      # RDD[(String, Int)]

# Action (triggers computation)
result = word_counts.reduceByKey(lambda a, b: a + b).collect()

# Result: [("hello", 5), ("world", 3), ...]
```

**RDD Properties:**

| Property | Explanation |
|----------|-------------|
| **Immutable** | Once created, cannot be modified (create new RDD instead) |
| **Distributed** | Automatically partitioned across cluster |
| **Resilient** | Fault-tolerant via lineage (recomputes lost partitions) |
| **Lazy** | Transformations not executed until action is called |

**Transformations (Lazy):**

| Transformation | What It Does | Example |
|----------------|--------------|---------|
| `map(f)` | Apply function to each element | `rdd.map(lambda x: x * 2)` |
| `filter(f)` | Keep elements matching predicate | `rdd.filter(lambda x: x > 10)` |
| `flatMap(f)` | Map + flatten (one-to-many) | `rdd.flatMap(lambda x: x.split())` |
| `reduceByKey(f)` | Aggregate by key | `rdd.reduceByKey(lambda a,b: a+b)` |
| `groupByKey()` | Group values by key | `rdd.groupByKey()` |
| `join()` | Join two RDDs by key | `rdd1.join(rdd2)` |

**Actions (Trigger Execution):**

| Action | What It Does | Example |
|--------|--------------|---------|
| `collect()` | Return all elements to driver | `rdd.collect()` |
| `count()` | Count number of elements | `rdd.count()` |
| `take(n)` | Return first n elements | `rdd.take(10)` |
| `saveAsTextFile()` | Write to HDFS/S3 | `rdd.saveAsTextFile("s3://...")` |
| `reduce(f)` | Aggregate all elements | `rdd.reduce(lambda a,b: a+b)` |

---

### 3. DataFrame API (High-Level Abstraction)

**DataFrames:** RDDs with schema (like SQL tables).

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("DataFrameExample").getOrCreate()

# Create DataFrame from file
df = spark.read.json("hdfs://path/to/users.json")

# Schema
df.printSchema()
# root
#  |-- name: string
#  |-- age: integer
#  |-- city: string

# SQL-like operations
result = df.filter(df.age > 25) \
           .groupBy("city") \
           .count() \
           .orderBy("count", ascending=False)

result.show()
```

**DataFrame vs. RDD:**

| Feature | RDD | DataFrame |
|---------|-----|-----------|
| **API** | Low-level (map, filter) | High-level (SQL-like) |
| **Optimization** | Manual | Catalyst optimizer (automatic) |
| **Performance** | Good | Better (optimized query plans) |
| **Schema** | No schema | Typed schema |
| **Use case** | Complex custom logic | SQL-like queries, ETL |

---

### 4. Spark SQL

**Run SQL queries on DataFrames:**

```python
# Register DataFrame as temp table
df.createOrReplaceTempView("users")

# Run SQL query
result = spark.sql("""
    SELECT city, COUNT(*) as user_count
    FROM users
    WHERE age > 25
    GROUP BY city
    ORDER BY user_count DESC
""")

result.show()
```

**Benefits:**

- ✅ Familiar SQL syntax (easy for analysts)
- ✅ Catalyst optimizer (automatic query optimization)
- ✅ Interoperability (mix SQL and DataFrame code)

---

### 5. Structured Streaming (Real-Time Processing)

**Process streaming data with DataFrame API:**

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("StreamingWordCount").getOrCreate()

# Read streaming data from Kafka
lines = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "logs") \
    .load() \
    .selectExpr("CAST(value AS STRING)")

# Process stream (same as batch!)
word_counts = lines.select(explode(split(col("value"), " ")).alias("word")) \
                   .groupBy("word") \
                   .count()

# Write to console (for demo) or Kafka/HDFS
query = word_counts.writeStream \
    .outputMode("complete") \
    .format("console") \
    .start()

query.awaitTermination()
```

**Streaming Modes:**

| Mode | Behavior | Use Case |
|------|----------|----------|
| **Append** | Only new rows written | ETL, event logging |
| **Complete** | Entire result table written | Aggregations (count, sum) |
| **Update** | Only updated rows written | Deduplication, joins |

---

### 6. MLlib (Machine Learning)

**Built-in machine learning library:**

```python
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.feature import VectorAssembler

# Load data
training = spark.read.csv("data/training.csv", header=True, inferSchema=True)

# Feature engineering
assembler = VectorAssembler(inputCols=["age", "income"], outputCol="features")
training_data = assembler.transform(training)

# Train model
lr = LogisticRegression(labelCol="label", featuresCol="features")
model = lr.fit(training_data)

# Predict
test = spark.read.csv("data/test.csv", header=True, inferSchema=True)
test_data = assembler.transform(test)
predictions = model.transform(test_data)

predictions.select("features", "label", "prediction").show()
```

**MLlib Algorithms:**

| Category | Algorithms |
|----------|------------|
| **Classification** | Logistic Regression, Decision Trees, Random Forest, GBT |
| **Regression** | Linear Regression, Decision Trees, Random Forest |
| **Clustering** | K-Means, Bisecting K-Means, GMM |
| **Recommendation** | ALS (Collaborative Filtering) |
| **Feature Engineering** | VectorAssembler, StandardScaler, PCA |

---

### 7. Performance Optimization

#### **7.1 Caching/Persistence**

```python
# Cache in memory (for iterative algorithms)
df.cache()  # or df.persist(StorageLevel.MEMORY_AND_DISK)

# Use cached DataFrame multiple times
df.filter("age > 25").count()  # First execution: reads from source + caches
df.filter("age > 30").count()  # Second execution: uses cache (fast!)

# Unpersist when done
df.unpersist()
```

**Storage Levels:**

| Level | Memory | Disk | Serialized | Use Case |
|-------|--------|------|------------|----------|
| `MEMORY_ONLY` | ✅ | ❌ | ❌ | Fast iterative algorithms (if data fits in RAM) |
| `MEMORY_AND_DISK` | ✅ | ✅ | ❌ | Default (spills to disk if RAM full) |
| `MEMORY_ONLY_SER` | ✅ | ❌ | ✅ | Save memory (slower due to serialization) |
| `DISK_ONLY` | ❌ | ✅ | ❌ | Data doesn't fit in RAM, still faster than recompute |

#### **7.2 Partitioning**

```python
# Repartition (shuffle data across cluster)
df = df.repartition(100)  # 100 partitions

# Coalesce (reduce partitions without shuffle)
df = df.coalesce(10)  # Reduce to 10 partitions (no shuffle)

# Partition by column (for joins)
df = df.repartition("user_id")  # All rows with same user_id in same partition
```

**Partitioning Strategy:**

- **Too few partitions:** Underutilized cluster (some executors idle)
- **Too many partitions:** Overhead (scheduling, coordination)
- **Rule of thumb:** 2-4 partitions per CPU core

#### **7.3 Broadcast Variables**

```python
# Broadcast small dataset to all executors (avoid shuffle)
small_dict = {"user1": "Alice", "user2": "Bob"}
broadcast_dict = sc.broadcast(small_dict)

# Use in transformations
def lookup_name(user_id):
    return broadcast_dict.value.get(user_id, "Unknown")

df = df.withColumn("name", lookup_name(col("user_id")))
```

**When to Use:** Broadcast small datasets (<200MB) to avoid expensive shuffles.

---

### 8. Fault Tolerance: Lineage-Based Recovery

**How Spark recovers from failures:**

```
RDD1 (input)
  │
  ├─ map() ─> RDD2
  │
  ├─ filter() ─> RDD3
  │
  └─ reduceByKey() ─> RDD4 (result)

If Executor 2 fails while computing RDD3:
  1. Spark detects failure
  2. Replays transformations from RDD1 → RDD3 on another executor
  3. No data loss (because lineage is tracked)
```

**Benefits:**

- ✅ **No checkpointing overhead** (unlike Hadoop, which writes to disk)
- ✅ **Fast recovery** (only recompute lost partitions)
- ✅ **Automatic** (no manual intervention)

**Trade-off:** Long lineage chains can slow recovery (use `checkpoint()` for very long chains).

---

### 9. When to Use Apache Spark

#### **✅ Use Spark When:**

1. **Batch processing** — ETL pipelines, log analysis, data transformation
2. **Iterative algorithms** — Machine learning (K-Means, PageRank, etc.)
3. **Interactive queries** — Ad-hoc SQL queries on large datasets
4. **Streaming** — Near real-time processing (Structured Streaming)
5. **Unified workloads** — Need batch + streaming + ML in one framework
6. **Large datasets** — Terabytes to petabytes (100GB+)
7. **In-memory processing** — Data fits in cluster RAM (huge speedup)

#### **❌ Don't Use Spark When:**

1. **Small data** — <10GB (overkill, use Pandas/SQL)
2. **Ultra-low latency** — <100ms (use Flink or real-time databases)
3. **Simple queries** — Use SQL database (PostgreSQL, BigQuery)
4. **Transactional workloads** — OLTP (use PostgreSQL/MySQL)
5. **Small cluster** — <3 nodes (overhead not worth it)

---

### 10. Real-World Examples

| Company | Use Case | Why Spark? |
|---------|----------|------------|
| **Netflix** | Movie recommendation (collaborative filtering) | MLlib for training on 100TB+ data |
| **Uber** | ETL pipelines (trip data processing) | Process 100TB+ daily in batch + stream |
| **Airbnb** | Pricing optimization | Machine learning on historical booking data |
| **NASA** | Log analysis (Mars Rover) | Process terabytes of sensor logs |
| **eBay** | Real-time fraud detection | Structured Streaming + MLlib |

---

### 11. Spark vs. Other Frameworks

| Feature | Spark | Hadoop MapReduce | Flink |
|---------|-------|------------------|-------|
| **Speed** | ⚡ 100x faster (in-memory) | ❌ Slow (disk-based) | ⚡ Faster (for streaming) |
| **API** | High-level (DataFrame, SQL) | Low-level (Map/Reduce) | High-level (DataStream) |
| **Streaming** | Micro-batch (near real-time) | ❌ Not designed for streaming | ✅ True streaming (event-by-event) |
| **ML Library** | ✅ MLlib | ❌ No | ⚠️ FlinkML (limited) |
| **Fault Tolerance** | Lineage-based (fast) | Checkpoint-based (slow) | Checkpoint-based |
| **Use Case** | Batch + Streaming + ML | Legacy batch processing | Real-time streaming |

---

### 12. Common Anti-Patterns

#### ❌ **1. Collecting Large Datasets to Driver**

**Problem:**

```python
# Bad: Collects 1TB to driver (out of memory)
all_data = df.collect()
```

**Solution:**

```python
# Good: Process in distributed manner
df.write.parquet("s3://output/")
```

#### ❌ **2. Not Caching Reused DataFrames**

**Problem:**

```python
# Bad: Recomputes df three times
df.filter("age > 25").count()
df.filter("age > 30").count()
df.filter("age > 35").count()
```

**Solution:**

```python
# Good: Cache once, reuse
df.cache()
df.filter("age > 25").count()
df.filter("age > 30").count()
df.unpersist()
```

#### ❌ **3. Using groupByKey Instead of reduceByKey**

**Problem:**

```python
# Bad: Shuffles all data (slow)
rdd.groupByKey().mapValues(sum)
```

**Solution:**

```python
# Good: Reduces before shuffle (fast)
rdd.reduceByKey(lambda a, b: a + b)
```

---

### 13. Trade-offs Summary

| What You Gain | What You Sacrifice |
|---------------|-------------------|
| ✅ 100x faster than MapReduce (in-memory) | ❌ Higher memory requirements |
| ✅ Unified API (batch, streaming, ML, SQL) | ❌ Not true real-time (micro-batch, ~100ms latency) |
| ✅ Fault tolerant (lineage-based recovery) | ❌ Long lineage chains can slow recovery |
| ✅ Easy to use (high-level APIs) | ❌ Steep learning curve (for optimization) |
| ✅ Scales to petabytes | ❌ Overkill for small data (<10GB) |

---

### 14. References

- **Apache Spark Documentation:** [https://spark.apache.org/docs/](https://spark.apache.org/docs/)
- **Spark: The Definitive Guide (Book):** By Bill Chambers & Matei Zaharia
- **Databricks Blog:** [https://databricks.com/blog](https://databricks.com/blog)
- **Related Chapters:**
  - [2.3.5 Batch vs Stream Processing](./2.3.5-batch-vs-stream-processing.md) — When to use batch vs. streaming
  - [2.3.2 Kafka Deep Dive](./2.3.2-kafka-deep-dive.md) — Kafka + Spark integration
  - [2.3.8 Apache Flink Deep Dive](./2.3.8-apache-flink-deep-dive.md) — Spark vs. Flink

---

## ✏️ Design Challenge

### Problem

You're building a **real-time fraud detection system** for a payment processor. Requirements:

1. **Data volume:** 10,000 transactions/sec = 864M transactions/day
2. **Processing:**
   - Batch: Daily model training on historical data (1TB/day)
   - Streaming: Real-time fraud scoring (<5 second latency)
3. **Features:**
   - Transaction velocity (transactions per user per hour)
   - Geographic anomaly (transaction from unusual location)
   - Merchant risk score (based on historical fraud rate)
4. **ML Model:** Train daily on last 30 days of data, deploy to streaming pipeline
5. **Scale:** 100 million users, 1 billion transactions/month

**Question:** How would you design this system using Spark? Would you use batch, streaming, or both? How would you train and deploy the ML model? What about latency requirements?

### Solution

#### 🧩 Scenario

- **System:** Real-time fraud detection
- **Write load:** 10,000 transactions/sec
- **Batch workload:** Train ML model daily on 1TB data (last 30 days)
- **Streaming workload:** Score transactions in real-time (<5 sec latency)
- **Data:** user_id, transaction_id, amount, merchant, location, timestamp
- **ML Model:** Logistic regression or gradient boosted trees

#### ✅ Goal

- Train accurate fraud model daily (batch processing)
- Score transactions in real-time (<5 sec)
- Handle 10K transactions/sec without dropping data
- Automatically deploy updated model to streaming pipeline
- Minimize false positives (don't block legitimate transactions)

#### ⚙️ Solution: Hybrid Spark Architecture (Batch + Streaming)

**Architecture:**

```
┌─────────────────────────────────────────────────────┐
│           Fraud Detection System (Spark)             │
├─────────────────────────────────────────────────────┤
│                                                      │
│  1. BATCH PIPELINE (Daily Model Training)           │
│     ┌──────────────────────────────┐               │
│     │  Spark Batch Job             │               │
│     │  - Read last 30 days (S3)    │               │
│     │  - Feature engineering        │               │
│     │  - Train ML model (MLlib)     │               │
│     │  - Save model to S3           │               │
│     └──────────┬───────────────────┘               │
│                │                                     │
│                ▼                                     │
│  2. MODEL DEPLOYMENT                                │
│     ┌──────────────────────────────┐               │
│     │  Model Registry (S3/MLflow)  │               │
│     │  - Store model versions      │               │
│     │  - Track performance          │               │
│     └──────────┬───────────────────┘               │
│                │                                     │
│                ▼                                     │
│  3. STREAMING PIPELINE (Real-Time Scoring)         │
│     ┌──────────────────────────────┐               │
│     │  Structured Streaming         │               │
│     │  - Read from Kafka            │               │
│     │  - Feature engineering        │               │
│     │  - Score with ML model        │               │
│     │  - Write to output Kafka      │               │
│     └──────────────────────────────┘               │
└─────────────────────────────────────────────────────┘
```

**Implementation:**

**1. Batch Pipeline (Daily Model Training):**

```python
from pyspark.sql import SparkSession
from pyspark.ml.classification import GBTClassifier
from pyspark.ml.feature import VectorAssembler

# Initialize Spark
spark = SparkSession.builder \
    .appName("FraudModelTraining") \
    .config("spark.executor.memory", "16g") \
    .getOrCreate()

# Read historical data (last 30 days from S3)
df = spark.read.parquet("s3://fraud-data/transactions/")
df = df.filter(col("transaction_date") >= current_date() - 30)

# Feature engineering
df = df.withColumn("velocity", 
    # Count transactions per user in last hour (window function)
    count("transaction_id").over(
        Window.partitionBy("user_id")
              .orderBy("timestamp")
              .rangeBetween(-3600, 0)
    )
)

df = df.withColumn("geo_anomaly",
    # Distance from user's usual location
    udf_calculate_distance(col("location"), col("user_home_location"))
)

# Prepare features
feature_cols = ["amount", "velocity", "geo_anomaly", "merchant_risk_score"]
assembler = VectorAssembler(inputCols=feature_cols, outputCol="features")
training_data = assembler.transform(df)

# Train model
gbt = GBTClassifier(labelCol="is_fraud", featuresCol="features", maxIter=100)
model = gbt.fit(training_data)

# Evaluate model
predictions = model.transform(test_data)
accuracy = predictions.filter(col("is_fraud") == col("prediction")).count() / predictions.count()
print(f"Model accuracy: {accuracy}")

# Save model
model.write().overwrite().save("s3://fraud-models/gbt_model_2024-01-15")
```

**2. Streaming Pipeline (Real-Time Scoring):**

```python
from pyspark.sql import SparkSession
from pyspark.ml.classification import GBTClassificationModel

# Initialize Spark Streaming
spark = SparkSession.builder \
    .appName("FraudDetectionStreaming") \
    .getOrCreate()

# Load trained model
model = GBTClassificationModel.load("s3://fraud-models/gbt_model_2024-01-15")

# Read streaming data from Kafka
transactions = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "transactions") \
    .option("startingOffsets", "latest") \
    .load()

# Parse JSON
from pyspark.sql.functions import from_json, col
schema = StructType([
    StructField("transaction_id", StringType()),
    StructField("user_id", StringType()),
    StructField("amount", DoubleType()),
    StructField("merchant", StringType()),
    StructField("location", StringType()),
    StructField("timestamp", TimestampType())
])

parsed = transactions.select(
    from_json(col("value").cast("string"), schema).alias("data")
).select("data.*")

# Feature engineering (same as batch)
features = parsed.withColumn("velocity", ...)  # Real-time velocity calculation
features = features.withColumn("geo_anomaly", ...)

# Assemble features
feature_cols = ["amount", "velocity", "geo_anomaly", "merchant_risk_score"]
assembler = VectorAssembler(inputCols=feature_cols, outputCol="features")
features_assembled = assembler.transform(features)

# Score with ML model
predictions = model.transform(features_assembled)

# Write to output Kafka (fraud alerts)
fraud_alerts = predictions.filter(col("prediction") == 1.0)

query = fraud_alerts \
    .selectExpr("transaction_id", "user_id", "amount", "probability") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("topic", "fraud-alerts") \
    .option("checkpointLocation", "s3://checkpoints/fraud-detection") \
    .start()

query.awaitTermination()
```

#### ⚠️ Handling Latency Requirements (<5 seconds)

**Problem:** Spark Structured Streaming uses micro-batches (default: 500ms-2s latency). Is this fast enough?

**Analysis:**

```
Transaction → Kafka → Spark Streaming → Fraud Score → Alert
   (0ms)      (10ms)       (2000ms)         (100ms)    (10ms)

Total: ~2.1 seconds ✅ (meets <5 second requirement)
```

**Optimization Tips:**

1. **Reduce micro-batch interval:**
   ```python
   .trigger(processingTime='500 milliseconds')  # Default: 2 seconds
   ```

2. **Partition Kafka topic:**
   - More partitions = more parallelism
   - Rule: partitions ≥ number of Spark executors

3. **Optimize feature engineering:**
   - Pre-compute merchant risk scores (batch job)
   - Use broadcast variables for lookups

#### 🧠 Model Deployment Strategy

**Challenge:** How to update the model daily without downtime?

**Solution: Blue-Green Deployment**

```python
# Daily batch job
def train_and_deploy_model():
    # Train new model
    new_model = train_model(date=today)
    
    # Save with date suffix
    model_path = f"s3://fraud-models/gbt_model_{today}"
    new_model.write().save(model_path)
    
    # Update "latest" pointer (atomic operation)
    with open("s3://fraud-models/LATEST", "w") as f:
        f.write(model_path)

# Streaming job (reads latest model every 5 minutes)
def get_latest_model():
    with open("s3://fraud-models/LATEST") as f:
        model_path = f.read()
    return GBTClassificationModel.load(model_path)

# Reload model periodically
while True:
    model = get_latest_model()
    # Use model for 5 minutes
    time.sleep(300)
```

#### ✅ Final Answer

| Aspect | Decision | Reason |
|--------|----------|--------|
| **Architecture** | **Hybrid (Spark Batch + Streaming)** | Batch for training, streaming for real-time scoring |
| **Batch Job** | Daily training on 1TB (last 30 days) | MLlib GBT model, runs overnight |
| **Streaming** | Structured Streaming (Kafka → Spark → Kafka) | Real-time scoring with <2s latency |
| **Latency** | ~2 seconds (micro-batch) | Meets <5 second requirement ✅ |
| **Model Deployment** | Blue-green (daily updates) | Zero downtime, reload every 5 minutes |
| **Throughput** | 10K transactions/sec | Kafka partitioning + Spark parallelism |
| **Feature Engineering** | Velocity, geo anomaly, merchant risk | Same logic in batch and streaming |
| **Trade-off** | Micro-batch (not true streaming) | Gain: Easier development, unified API |

**Performance Metrics:**
- **Batch training:** 2-3 hours (1TB data, 100 nodes)
- **Streaming latency:** 2 seconds (p99)
- **Throughput:** 10K+ transactions/sec
- **Model accuracy:** 95%+ (with daily retraining)

**Why Spark (Not Flink)?**
- ✅ **Unified API:** Same code for batch training and streaming scoring
- ✅ **MLlib:** Built-in ML library (Flink's ML library is limited)
- ✅ **Micro-batch is enough:** <2s latency meets requirements
- ❌ **Not true streaming:** If needed <100ms, use Flink

**When to Reconsider:**
- If latency requirement <500ms → Use Flink (true streaming)
- If model training takes >8 hours → Use distributed ML frameworks (Horovod, Ray)
- If need <100ms latency → Use pre-computed features + lightweight scoring (no Spark)

