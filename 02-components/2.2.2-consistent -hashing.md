# 2.2.2 Consistent Hashing: Minimizing Data Chaos

## Intuitive Explanation

Imagine you have 10,000 keys (user data, images, etc.) stored across 10 cache servers (nodes). If you add or remove just
one server, the simple formula we usually use (like `key % total_servers`) breaks. Every single key has to be remapped
and moved, causing massive downtime.

**Consistent Hashing** is an algorithm that fixes this. It ensures that when a server is added or removed, only a small,
localized fraction of the keys need to be remapped and moved.

- **The Ring:** All cache servers (nodes) and all data keys are mapped onto a conceptual ring (a circular hash space).
- **The Rule:** A data key is stored on the first server encountered when moving clockwise around the ring from the
  key's position.

---

## In-Depth Analysis

### 1. The Hashing Ring Mechanism

- **Nodes on the Ring:** Each server (Node) is hashed onto the ring using a cryptographic hash function (
  like $\text{MD5}$
  or $\text{SHA}-1$). This maps the server's name or $\text{IP}$ address to a specific point on the ring, usually a $32$
  -bit or $64$-bit integer space.
- **Keys on the Ring:** Each data item (Key) is also hashed using the same function to map it to a position on the ring.
- **Assignment:** To find where a Key should be stored, you start at the Key's position and move clockwise until you hit
  the
  first Node. That node stores the data.

### 2. The Core Advantage: Minimizing Remapping

- **Node Removal (Failure):** If Node $A$ fails, only the keys that were previously assigned to $A$ need to be
  reassigned. These keys move clockwise to the next available node, Node $B$. No other keys on the rest of the ring are
  affected.
- **Node Addition (Scaling):** If a new Node $C$ is added, it inserts itself into the ring. It only takes responsibility
  for the keys located between itself and the previous node (Node $A$). Only keys between $A$ and $C$ need to be moved.

This is a massive improvement over modulo hashing, where adding one node requires moving $\frac{N-1}{N}$ keys (almost
all of them). Consistent Hashing limits data movement to $\frac{1}{N}$ (where $N$ is the number of nodes).

### 3. The Problem of Uneven Distribution (Hotspots)

If the number of physical nodes is small, the servers might not be distributed evenly around the ring, leading to two
problems:

- **Uneven Key Distribution:** One node might end up storing a disproportionately large number of keys.
- **Uneven Load Distribution (Hotspot):** One node might handle many more requests than others.

### 4. Solution: Virtual Nodes (VNodes)

To solve uneven distribution, we introduce Virtual Nodes ($\text{VNodes}$).

- Each physical server maps to many ($100$ to $200$) virtual nodes scattered across the ring.
- The system uses the $VNode$ positions to determine storage, but the physical server identity is carried along with
  the $VNode$.
- **Benefit:** This smooths out the distribution of keys and load significantly. If one physical server fails,
  its $100$ $\text{VNodes}$ are removed, and their load is evenly distributed among all remaining physical servers
  across the ring.

### Key Concepts / Tradeoffs

| Feature       | Description                                                                     | Trade-off / Scalability Issue                                                                                         |
|---------------|---------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------|
| Data Locality | Ensures data resides close to the application service that needs it most.       | In a multi-region deployment, the system must decide between consistency (latest data) and locality (nearest server). |
| VNodes        | Solves the problem of uneven key and load distribution across physical servers. | Adds complexity to the lookup table; the client must track $N \times V$ nodes instead of just $N$ physical nodes.     |
| Complexity    | The mechanism is more complex than simple modulo hashing.                       | Implementation complexity (requires careful management of the hash function and $VNode$ mapping) is higher.           |

---

## ‚úèÔ∏è Design Challenge

### Problem

You are running a global $\text{CDN}$ network with $100$ edge caches. In a high-traffic scenario, one edge cache server
suddenly fails. Using the concept of Consistent Hashing, explain the primary benefit this gives you compared to if you
were using a standard key % 100 hashing strategy. What role do $\text{VNodes}$ play in mitigating the immediate load
spike on the remaining $99$ servers?

### Solution

#### üß© Scenario Summary

- **System:** Global CDN with 100 edge caches (each stores cached objects like images, videos, etc.)
- **Challenge:** One cache server fails under high traffic.
- **Goal:** Minimize cache misses and rebalancing overhead.

#### ‚úÖ Step 1: Standard Hashing Problem

In a naive hash-based load distribution:

```
server = hash(key) % 100
```

- If any one of the 100 servers goes down ‚Üí the modulus changes (100 ‚Üí 99).
- This causes almost all keys to map to different servers.
- Result:
    - Massive cache invalidation
    - Cache warm-up storms (clients fetch everything from origin again)
    - Load spike on remaining servers

#### ‚úÖ Step 2: Consistent Hashing ‚Äì The Better Approach

In **Consistent Hashing:**

- Servers are placed on a hash ring (0‚Äì2¬≥¬≤).
- Each key is also hashed and placed on the ring.
- A key is assigned to the first server clockwise from its hash position.

When one server fails:

- Only keys that were mapped to that server need to be remapped.
- All other keys continue to map to the same servers.

üîπ Result:

- Minimal re-distribution (~1/n of keys)
- 99% of keys remain stable
- Drastically reduced cache churn and origin load

#### ‚úÖ Step 3: Role of Virtual Nodes (VNodes)

**Without VNodes:**

- Each physical server has a single position on the hash ring.
- When one fails, its range (segment of the ring) gets entirely transferred to a few neighbors ‚Üí load imbalance.

**With VNodes:**

- Each physical server is represented by multiple virtual nodes (e.g., 100 VNodes per server).
- VNodes from all servers are interleaved across the ring.

**üß† Example:**

- 100 servers √ó 100 VNodes = 10,000 logical positions on the ring.
- If 1 server (100 VNodes) fails, its keys are spread evenly across the remaining 9,900 VNodes.

**üîπ Result:**

- Even redistribution of load across all surviving servers.
- No single server is overwhelmed.

#### ‚úÖ Step 4: Why This Works in CDN Context

| Problem                        | Solution via Consistent Hashing + VNodes  |
|--------------------------------|-------------------------------------------|
| Server failure ‚Üí key remapping | Only small fraction of keys remap         |
| Hotspot risk (uneven key load) | VNodes smooth out load across servers     |
| Cache invalidation storm       | 99% of cached content remains valid       |
| Sudden origin fetch surge      | Reduced dramatically; origin stays stable |

#### ‚öôÔ∏è Step 5: Visual Summary

##### üî∏ Naive Hash (key % 100)

```
Server 42 fails ‚Üí All hashes remap ‚Üí Cache miss storm!
```

##### üî∏ Consistent Hashing

```
Server 42 fails ‚Üí Only its segment‚Äôs keys move ‚Üí Stable system
```

##### üî∏ With VNodes

```
Server 42‚Äôs 100 segments redistributed evenly ‚Üí No hotspots
```

##### ‚úÖ Final Summary

| Aspect                       | Naive `key % N`      | Consistent Hashing + VNodes     |
|------------------------------|----------------------|---------------------------------|
| **Key Remapping on Failure** | ~100% of keys        | ~1/N keys                       |
| **Cache Stability**          | Massive invalidation | 99% cache retained              |
| **Load Balance**             | Uneven               | Even (via VNodes)               |
| **Recovery Load**            | High                 | Minimal                         |
| **Best Use Case**            | Small systems        | Large distributed CDNs / caches |
